# Real-Time Fraud Detection Data Lakehouse

Há»‡ thá»‘ng Data Lakehouse phÃ¡t hiá»‡n gian láº­n tháº» tÃ­n dá»¥ng trong thá»i gian thá»±c sá»­ dá»¥ng **Sparkov Credit Card Transactions Dataset** vá»›i kiáº¿n trÃºc Medallion (Bronze-Silver-Gold).

![Architecture Diagram](docs/architecture.png)

## Tá»•ng quan

Dá»± Ã¡n xÃ¢y dá»±ng pipeline xá»­ lÃ½ dá»¯ liá»‡u end-to-end:

1. **Thu tháº­p dá»¯ liá»‡u**: PostgreSQL â†’ Debezium CDC â†’ Kafka (real-time streaming)
2. **Xá»­ lÃ½ dá»¯ liá»‡u**: Apache Spark vá»›i Delta Lake (Bronze/Silver/Gold layers)
3. **Feature Engineering**: 15 features tá»« dá»¯ liá»‡u Ä‘á»‹a lÃ½, nhÃ¢n kháº©u há»c, giao dá»‹ch
4. **Machine Learning**: Random Forest & Logistic Regression (99%+ accuracy)
5. **Model Serving**: FastAPI cho prediction real-time

## Tech Stack

| Component         | Technology               | MÃ´ táº£                          |
| ----------------- | ------------------------ | ------------------------------ |
| **Source DB**     | PostgreSQL 14            | OLTP database vá»›i CDC enabled  |
| **CDC**           | Debezium 2.5             | Change Data Capture connector  |
| **Streaming**     | Apache Kafka             | Message broker                 |
| **Processing**    | Apache Spark 3.4.1       | Stream & batch processing      |
| **Storage**       | Delta Lake 2.4.0 + MinIO | ACID transactions, time travel |
| **Metastore**     | Hive Metastore           | Table metadata management      |
| **ML**            | Scikit-learn, MLflow     | Model training & registry      |
| **API**           | FastAPI                  | Real-time prediction service   |
| **Orchestration** | Apache Airflow           | Workflow scheduling            |

## Cáº¥u trÃºc thÆ° má»¥c

```
real-time-fraud-detection-lakehouse/
â”œâ”€â”€ airflow/dags/              # Airflow DAGs (model retraining, reports)
â”œâ”€â”€ config/                    # Service configurations
â”‚   â”œâ”€â”€ metastore/             # Hive metastore config
â”‚   â”œâ”€â”€ spark/                 # Spark defaults
â”‚   â””â”€â”€ trino/                 # Trino settings
â”œâ”€â”€ data/                      # Sparkov dataset (CSV files)
â”œâ”€â”€ database/                  # PostgreSQL initialization
â”‚   â””â”€â”€ init_postgres.sql      # Schema setup (22 columns)
â”œâ”€â”€ deployment/                # Infrastructure automation
â”‚   â”œâ”€â”€ debezium/              # CDC configuration scripts
â”‚   â””â”€â”€ minio/                 # MinIO bucket setup
â”œâ”€â”€ docs/                      # Documentation
â”‚   â””â”€â”€ PROJECT_SPECIFICATION.md
â”œâ”€â”€ notebooks/                 # Jupyter notebooks (EDA, experiments)
â”œâ”€â”€ services/                  # Microservices
â”‚   â”œâ”€â”€ data-producer/         # PostgreSQL data simulator
â”‚   â”œâ”€â”€ fraud-detection-api/   # FastAPI prediction service
â”‚   â””â”€â”€ mlflow/                # MLflow tracking server
â”œâ”€â”€ spark/                     # Custom Spark with ML libraries
â”‚   â”œâ”€â”€ app/                   # PySpark jobs
â”‚   â”‚   â”œâ”€â”€ streaming_job.py   # Bronze layer (CDC â†’ Delta Lake)
â”‚   â”‚   â”œâ”€â”€ silver_layer_job.py # Feature engineering (15 features)
â”‚   â”‚   â”œâ”€â”€ gold_layer_dimfact_job.py # Star Schema (dimensions/facts)
â”‚   â”‚   â””â”€â”€ ml_training_job.py # Model training pipeline
â”‚   â””â”€â”€ Dockerfile             # Spark + MLflow + ML libraries
â”œâ”€â”€ sql/                       # SQL views for Gold layer
â”‚   â””â”€â”€ gold_layer_views.sql   # Materialized views for dashboards
â”œâ”€â”€ docker-compose.yml         # 11 services orchestration
â””â”€â”€ README.md
```

## Dataset

**Sparkov Credit Card Transactions Fraud Detection Dataset** ([Kaggle](https://www.kaggle.com/datasets/kartik2112/fraud-detection))

- `data/fraudTrain.csv` - 1,296,675 transactions (01/2019 - 12/2020)
- `data/fraudTest.csv` - 555,719 transactions
- **22 columns**: Geographic (lat/long), demographic (age, gender, job), transaction (amount, merchant, category)

### Schema chÃ­nh

| Column                    | Type     | Description                  |
| ------------------------- | -------- | ---------------------------- |
| `trans_date_trans_time`   | DateTime | Thá»i gian giao dá»‹ch          |
| `cc_num`                  | Long     | Sá»‘ tháº» tÃ­n dá»¥ng              |
| `merchant`                | String   | TÃªn cá»­a hÃ ng                 |
| `category`                | String   | Danh má»¥c (grocery, gas, ...) |
| `amt`                     | Double   | Sá»‘ tiá»n giao dá»‹ch            |
| `gender`                  | String   | Giá»›i tÃ­nh (M/F)              |
| `lat`, `long`             | Double   | Vá»‹ trÃ­ khÃ¡ch hÃ ng            |
| `merch_lat`, `merch_long` | Double   | Vá»‹ trÃ­ cá»­a hÃ ng              |
| `is_fraud`                | Integer  | NhÃ£n gian láº­n (0/1)          |

### Feature Engineering (15 features)

**Geographic**: `distance_km` (Haversine), `is_distant_transaction`
**Demographic**: `age`, `gender_encoded`
**Time**: `hour`, `day_of_week`, `is_weekend`, `is_late_night`, `hour_sin`, `hour_cos`
**Amount**: `log_amount`, `amount_bin`, `is_zero_amount`, `is_high_amount`

## HÆ°á»›ng dáº«n cháº¡y

### 1. YÃªu cáº§u há»‡ thá»‘ng

- Docker & Docker Compose
- Python 3.9+
- 8GB RAM, 20GB disk space

---

### 2. Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng

#### CÃ¡ch 1: Tá»± Ä‘á»™ng (Khuyáº¿n nghá»‹)

```bash
# Clone repository
git clone https://github.com/bin-bard/real-time-fraud-detection-lakehouse.git
cd real-time-fraud-detection-lakehouse

# Khá»Ÿi Ä‘á»™ng toÃ n bá»™ há»‡ thá»‘ng (MinIO buckets, Debezium CDC connector, data-producer Ä‘á»u tá»± Ä‘á»™ng)
docker-compose up -d
```

> **LÆ°u Ã½:** KhÃ´ng cáº§n cháº¡y thÃªm báº¥t ká»³ lá»‡nh setup nÃ o khÃ¡c.

#### CÃ¡ch 2: Thá»§ cÃ´ng (tÃ¹y chá»‰nh tá»«ng bÆ°á»›c)

```bash
# Clone repository
git clone https://github.com/bin-bard/real-time-fraud-detection-lakehouse.git
cd real-time-fraud-detection-lakehouse

# Khá»Ÿi Ä‘á»™ng cÃ¡c service chÃ­nh (khÃ´ng tá»± Ä‘á»™ng táº¡o bucket, khÃ´ng tá»± Ä‘á»™ng cháº¡y data-producer)
docker-compose up -d --scale minio-setup=0 --scale data-producer=0

# Táº¡o MinIO buckets thá»§ cÃ´ng
docker-compose run --rm minio-setup

# Setup Debezium CDC (PowerShell)
.\deployment\debezium\setup_debezium.ps1

# Khá»Ÿi Ä‘á»™ng data-producer thá»§ cÃ´ng (náº¿u muá»‘n)
docker-compose up -d data-producer
```

#### Rebuild MLflow service (náº¿u gáº·p lá»—i)

```powershell
# Rebuild MLflow vá»›i cáº¥u trÃºc má»›i
docker-compose build mlflow
docker-compose up -d mlflow

# Kiá»ƒm tra MLflow logs
docker logs mlflow -f
```

---

### 3. Cháº¡y Data Pipeline (Streaming Architecture)

Há»‡ thá»‘ng sá»­ dá»¥ng **kiáº¿n trÃºc streaming liÃªn tá»¥c** - khi dá»¯ liá»‡u vÃ o Bronze thÃ¬ tá»± Ä‘á»™ng Ä‘Æ°á»£c xá»­ lÃ½ qua Silver vÃ  Gold ngay láº­p tá»©c.

#### âš¡ **CÃ¡ch 1: Tá»± Ä‘á»™ng 100% (Khuyáº¿n nghá»‹)**

```bash
# Chá»‰ cáº§n 1 lá»‡nh duy nháº¥t - Táº¥t cáº£ tá»± Ä‘á»™ng!
docker-compose up -d
```

âœ… **3 streaming jobs sáº½ tá»± Ä‘á»™ng khá»Ÿi Ä‘á»™ng:**
- `bronze-streaming`: Kafka â†’ Bronze Delta Lake
- `silver-streaming`: Bronze â†’ Silver (15 features)
- `gold-streaming`: Silver â†’ Gold (Star Schema)

**Kiá»ƒm tra logs:**
```bash
# Xem táº¥t cáº£ streaming jobs
docker-compose logs -f bronze-streaming silver-streaming gold-streaming

# Hoáº·c tá»«ng job riÃªng láº»
docker logs -f bronze-streaming
docker logs -f silver-streaming
docker logs -f gold-streaming
```

---

#### ğŸš€ **CÃ¡ch 2: Script PowerShell (Má»Ÿ 3 terminals riÃªng)**

```powershell
# Cháº¡y script tá»± Ä‘á»™ng (má»Ÿ 3 cá»­a sá»• riÃªng cho má»—i job)
.\scripts\start-streaming-pipeline.ps1
```

**Æ¯u Ä‘iá»ƒm:** Dá»… debug, cÃ³ thá»ƒ Ctrl+C tá»«ng job riÃªng

---

#### ğŸ”§ **CÃ¡ch 3: Thá»§ cÃ´ng (Chá»‰ khi cáº§n debug chi tiáº¿t)**

**BÆ°á»›c 1: Bronze Layer (CDC ingestion)**

Má»Ÿ terminal Ä‘áº§u tiÃªn vÃ  cháº¡y Bronze streaming job:

```bash
docker exec spark-master /opt/spark/bin/spark-submit \
  --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  --conf 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension' \
  --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog' \
  /app/streaming_job.py
```

**Káº¿t quáº£ mong Ä‘á»£i:**
```
âœ… Spark Session with Delta Lake created successfully.
Bronze layer streaming started. Writing to MinIO...
Writing batch 0 to Bronze layer...
Batch 0 written to Bronze successfully.
```

**KhÃ´ng táº¯t terminal nÃ y** - Ä‘á»ƒ job cháº¡y liÃªn tá»¥c. Chá» tháº¥y Ã­t nháº¥t 5 batches thÃ nh cÃ´ng trÆ°á»›c khi cháº¡y Silver job.

**BÆ°á»›c 2: Silver Layer (Feature engineering) - Streaming Mode**

Má»Ÿ terminal má»›i vÃ  cháº¡y:

```bash
docker exec -it spark-master bash -c "/opt/spark/bin/spark-submit \
  --packages io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  --conf 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension' \
  --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog' \
  /app/silver_layer_job.py"
```

**Káº¿t quáº£:**
- Job sáº½ cháº¡y liÃªn tá»¥c, tá»± Ä‘á»™ng xá»­ lÃ½ dá»¯ liá»‡u má»›i tá»« Bronze
- Checkpoint Ä‘Æ°á»£c lÆ°u táº¡i `s3a://lakehouse/checkpoints/bronze_to_silver`
- Trigger má»—i 30 giÃ¢y Ä‘á»ƒ xá»­ lÃ½ micro-batch
- **KhÃ´ng táº¯t terminal nÃ y** - Ä‘á»ƒ job cháº¡y liÃªn tá»¥c

**BÆ°á»›c 3: Gold Layer (Dimensional Model - Star Schema) - Streaming Mode**

Má»Ÿ terminal má»›i khÃ¡c vÃ  cháº¡y:

```bash
docker exec -it spark-master bash -c "/opt/spark/bin/spark-submit \
  --packages io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  --conf 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension' \
  --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog' \
  /app/gold_layer_dimfact_job.py"
```

**Káº¿t quáº£:**
- Job sáº½ cháº¡y liÃªn tá»¥c, tá»± Ä‘á»™ng xá»­ lÃ½ dá»¯ liá»‡u má»›i tá»« Silver
- Táº¡o 5 streaming tables song song:
  - `dim_customer` - Dimension table (khÃ¡ch hÃ ng)
  - `dim_merchant` - Dimension table (cá»­a hÃ ng)
  - `dim_time` - Dimension table (thá»i gian)
  - `dim_location` - Dimension table (Ä‘á»‹a Ä‘iá»ƒm)
  - `fact_transactions` - Fact table (giao dá»‹ch vá»›i metrics)
- Checkpoint Ä‘Æ°á»£c lÆ°u táº¡i `s3a://lakehouse/checkpoints/silver_to_gold/*`
- Trigger má»—i 30 giÃ¢y Ä‘á»ƒ xá»­ lÃ½ micro-batch
- **KhÃ´ng táº¯t terminal nÃ y** - Ä‘á»ƒ job cháº¡y liÃªn tá»¥c

---

**Luá»“ng xá»­ lÃ½ hoÃ n chá»‰nh (End-to-End):**

```
PostgreSQL INSERT â†’ Debezium CDC â†’ Kafka 
  â†“ (Bronze Streaming - Auto)
Bronze Layer (Delta Lake)
  â†“ (Silver Streaming - 30s trigger)
Silver Layer + Feature Engineering (15 features)
  â†“ (Gold Streaming - 30s trigger)
Gold Layer (Star Schema: 4 Dims + 1 Fact)
```

**Æ¯u Ä‘iá»ƒm kiáº¿n trÃºc streaming:**
- âœ… **Near Real-time**: Äá»™ trá»… ~30-60 giÃ¢y tá»« INSERT Ä‘áº¿n Gold
- âœ… **Tá»± Ä‘á»™ng**: KhÃ´ng cáº§n trigger thá»§ cÃ´ng
- âœ… **Scalable**: Xá»­ lÃ½ Ä‘Æ°á»£c millions records/day
- âœ… **Fault-tolerant**: Checkpoint Ä‘áº£m báº£o exactly-once processing

---

**BÆ°á»›c 3b (Optional): Táº¡o SQL Views cho Dashboard**

Truy cáº­p Trino vÃ  cháº¡y file `sql/gold_layer_views.sql` Ä‘á»ƒ táº¡o 9 views tá»‘i Æ°u:

```bash
# Access Trino CLI (náº¿u cÃ³)
docker exec -it trino trino --catalog lakehouse --schema gold

# Hoáº·c sá»­ dá»¥ng Metabase/DBeaver Ä‘á»ƒ cháº¡y tá»«ng view trong file:
# - daily_summary
# - hourly_summary
# - state_summary
# - category_summary
# - amount_summary
# - latest_metrics
# - fraud_patterns
# - merchant_analysis
# - time_period_analysis
```

**BÆ°á»›c 3c: Truy cáº­p Metabase Ä‘á»ƒ trá»±c quan hÃ³a dá»¯ liá»‡u**

- Truy cáº­p Metabase táº¡i: http://localhost:3000
- Láº§n Ä‘áº§u Ä‘Äƒng nháº­p: táº¡o tÃ i khoáº£n admin
- Káº¿t ná»‘i Trino/Presto (host: `trino`, port: `8082`, catalog: `lakehouse`, schema: `gold`)
- Query tá»« dimensional model:

  ```sql
  -- Dashboard metrics (sá»­ dá»¥ng views)
  SELECT * FROM lakehouse.gold.daily_summary;
  SELECT * FROM lakehouse.gold.latest_metrics;

  -- Ad-hoc analysis (sá»­ dá»¥ng dim/fact)
  SELECT f.*, c.first_name, m.merchant
  FROM lakehouse.gold.fact_transactions f
  JOIN lakehouse.gold.dim_customer c ON f.customer_key = c.customer_key
  JOIN lakehouse.gold.dim_merchant m ON f.merchant_key = m.merchant_key
  WHERE f.is_fraud = '1'
  ORDER BY f.transaction_amount DESC
  LIMIT 10;
  ```

> **Lá»£i Ã­ch Star Schema:**
>
> - Dashboard queries nhanh (pre-joined dimensions)
> - Chatbot linh hoáº¡t (ad-hoc drill-down)
> - Metabase auto-refresh (1-60 phÃºt) cho monitoring gáº§n real-time

**BÆ°á»›c 4: ML Training**

```bash
docker exec -it spark-master bash -c "/opt/spark/bin/spark-submit \
  --packages io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.4 \
  --conf 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension' \
  --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog' \
  /app/ml_training_job.py"
```

---

### 4. Truy cáº­p Services

| Service             | URL                   | Username / Password                             | Ghi chÃº                                           |
| ------------------- | --------------------- | ----------------------------------------------- | ------------------------------------------------- |
| Spark Master UI     | http://localhost:8080 | KhÃ´ng cáº§n                                       | Monitoring Spark jobs                             |
| MinIO Console       | http://localhost:9001 | `minio` / `minio123`                            | Quáº£n lÃ½ buckets vÃ  files (Data Lake)              |
| MLflow UI           | http://localhost:5000 | KhÃ´ng cáº§n                                       | ML model tracking & registry                      |
| Kafka UI            | http://localhost:9002 | KhÃ´ng cáº§n                                       | Xem topics, messages, consumer groups             |
| Trino UI            | http://localhost:8085 | KhÃ´ng cáº§n                                       | Query engine monitoring                           |
| Metabase            | http://localhost:3000 | TÃ¹y chá»n (vÃ­ dá»¥:`admin@admin.com` / `admin123`) | BI Dashboard, tá»± táº¡o tÃ i khoáº£n admin láº§n Ä‘áº§u      |
| Fraud Detection API | http://localhost:8000 | KhÃ´ng cáº§n                                       | Real-time prediction endpoint                     |
| Kafka Broker        | localhost:9092        | KhÃ´ng cáº§n                                       | Kafka bootstrap server                            |
| PostgreSQL (Source) | localhost:5432        | `postgres` / `postgres`                         | Database `frauddb`                                |
| Metabase DB         | Internal              | `postgres` / `postgres`                         | Database `metabase` (khÃ´ng cáº§n truy cáº­p thá»§ cÃ´ng) |
| Hive Metastore DB   | Internal (9083)       | `hive` / `hive`                                 | Postgres cho Hive (khÃ´ng expose ra ngoÃ i)         |

> **LÆ°u Ã½ quan trá»ng:**
>
> - **MinIO, PostgreSQL:** Credentials cá»‘ Ä‘á»‹nh trong `docker-compose.yml` (cÃ³ thá»ƒ Ä‘á»•i trÆ°á»›c khi khá»Ÿi Ä‘á»™ng).
> - **Metabase:** Táº¡o tÃ i khoáº£n admin khi truy cáº­p láº§n Ä‘áº§u, email/password tÃ¹y chá»n (vÃ­ dá»¥: `admin@admin.com` / `admin123`).
> - **Spark UI, Kafka UI, Trino UI:** KhÃ´ng yÃªu cáº§u Ä‘Äƒng nháº­p.
> - **Airflow, MLflow:** ChÆ°a Ä‘Æ°á»£c khá»Ÿi Ä‘á»™ng trong docker-compose hiá»‡n táº¡i (cÃ³ thá»ƒ thÃªm sau).

### 5. Kiá»ƒm tra Pipeline

**Check Bronze layer data:**

```bash
# MinIO console: http://localhost:9001
# Navigate to: lakehouse/bronze/transactions/
```

**Check Kafka messages:**

```bash
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic postgres.public.transactions \
  --from-beginning --max-messages 5
```

### XÃ¡c minh CDC (INSERT/UPDATE/DELETE) vÃ  giÃ¡ trá»‹ trÆ°á»ng `amt`

**1. Láº¥y trans_num thá»±c táº¿ Ä‘á»ƒ test:**

```sql
SELECT trans_num FROM transactions LIMIT 5;
```

**2. Thá»±c hiá»‡n cÃ¡c thao tÃ¡c trÃªn PostgreSQL:**

```sql
-- UPDATE
UPDATE transactions SET amt = amt + 1 WHERE trans_num = '<trans_num thá»±c táº¿>';
-- DELETE
DELETE FROM transactions WHERE trans_num = '<trans_num thá»±c táº¿>';
```

**3. Kiá»ƒm tra message CDC trÃªn Kafka:**

```bash
docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic postgres.public.transactions --from-beginning --max-messages 100000 --timeout-ms 3000 2>$null | Select-String -Pattern "<trans_num thá»±c táº¿>"
```

Káº¿t quáº£:

- `"op":"c"` = insert, `"op":"u"` = update, `"op":"d"` = delete.
- TrÆ°á»ng `amt` sáº½ á»Ÿ dáº¡ng mÃ£ hÃ³a Base64 (vÃ­ dá»¥: "amt":"Ark=").

**4. Decode giÃ¡ trá»‹ amt (PowerShell):**

```powershell
[System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String("Ark="))
```

Viá»‡c decode nÃ y chá»‰ Ä‘á»ƒ xem giÃ¡ trá»‹ thá»±c, khÃ´ng áº£nh hÆ°á»Ÿng pipeline.

**5. Xem Kafka messages qua UI:**

Truy cáº­p Kafka UI táº¡i http://localhost:9002 Ä‘á»ƒ xem topics, messages, consumer groups qua giao diá»‡n web thÃ¢n thiá»‡n.

**Monitor Spark jobs:**

```bash
docker logs -f spark-master
# Or visit Spark UI: http://localhost:8080
```

### 6. Lakehouse Structure

```
s3a://lakehouse/
â”œâ”€â”€ bronze/transactions/          # Raw CDC data from PostgreSQL
â”œâ”€â”€ silver/transactions/          # 15 engineered features
â”œâ”€â”€ gold/                         # Business aggregations
â”‚   â”œâ”€â”€ daily_summary/
â”‚   â”œâ”€â”€ hourly_summary/
â”‚   â”œâ”€â”€ state_summary/
â”‚   â””â”€â”€ category_summary/
â”œâ”€â”€ checkpoints/                  # Spark streaming checkpoints
â””â”€â”€ models/                       # ML models & artifacts
```

### 7. Model Performance

| Model               | AUC    | Accuracy | Fraud Detection Rate |
| ------------------- | ------ | -------- | -------------------- |
| Random Forest       | 99.99% | 99.76%   | **83.33%** â­        |
| Logistic Regression | 99.93% | 99.53%   | 66.67%               |

### 8. Troubleshooting

**Reset há»‡ thá»‘ng:**

```bash
docker-compose down -v
docker-compose up -d --build
```

**Check logs:**

```bash
docker logs data-producer
docker logs spark-master
docker logs kafka
```

**MLflow connection issues:**

```bash
docker-compose restart mlflow
docker logs mlflow
```

---

**Architecture:**

**Data Flow:**

```
CSV â†’ PostgreSQL â†’ Debezium CDC â†’ Kafka â†’ Spark Streaming (Bronze)
                                            â†“ (30s micro-batch)
                                          Silver (15 features)
                                            â†“ (30s micro-batch)
                                          Gold (Star Schema)
```

**Streaming Pipeline (3 táº§ng liÃªn tá»¥c):**

```
Bronze Layer (Raw CDC)
  â”œâ”€â”€ Input: Kafka CDC events
  â”œâ”€â”€ Processing: Filter tombstones, parse Debezium format
  â”œâ”€â”€ Output: Delta Lake (append-only)
  â””â”€â”€ Checkpoint: s3a://lakehouse/checkpoints/kafka_to_bronze

Silver Layer (Feature Engineering)
  â”œâ”€â”€ Input: Bronze Delta Lake (streaming read)
  â”œâ”€â”€ Processing: Data quality + 15 features
  â”œâ”€â”€ Output: Delta Lake (partitioned by year/month/day)
  â””â”€â”€ Checkpoint: s3a://lakehouse/checkpoints/bronze_to_silver

Gold Layer (Dimensional Model)
  â”œâ”€â”€ Input: Silver Delta Lake (streaming read)
  â”œâ”€â”€ Processing: Star Schema (4 Dims + 1 Fact)
  â”œâ”€â”€ Output: 5 Delta Lake tables
  â””â”€â”€ Checkpoint: s3a://lakehouse/checkpoints/silver_to_gold/*
```

**Services (11 containers):**

- postgres, debezium, kafka, zookeeper
- minio, spark-master, spark-worker
- data-producer, mlflow, mlflow-db, metastore-db

**Key Features:**

- âœ… Real-time CDC with Debezium
- âœ… **End-to-end streaming pipeline (Bronze â†’ Silver â†’ Gold)**
- âœ… **Near real-time processing (~30-60s latency)**
- âœ… ACID transactions with Delta Lake
- âœ… Exactly-once processing with checkpoints
- âœ… 15 engineered features (geographic, demographic, temporal, amount)
- âœ… 99%+ accuracy fraud detection
- âœ… Star Schema for analytics (Medallion architecture)

---

## Chi tiáº¿t ká»¹ thuáº­t

Xem file `docs/PROJECT_SPECIFICATION.md` Ä‘á»ƒ hiá»ƒu rÃµ:

- Kiáº¿n trÃºc há»‡ thá»‘ng chi tiáº¿t
- YÃªu cáº§u nghiá»‡p vá»¥
- Data flow vÃ  processing layers
- ML pipeline specifications
