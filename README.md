# Real-Time Fraud Detection Data Lakehouse

Há»‡ thá»‘ng Data Lakehouse phÃ¡t hiá»‡n gian láº­n tháº» tÃ­n dá»¥ng trong thá»i gian thá»±c sá»­ dá»¥ng **Sparkov Credit Card Transactions Dataset** vá»›i kiáº¿n trÃºc Medallion (Bronze-Silver-Gold).

![Architecture Diagram](docs/architecture.png)

## Tá»•ng quan

Dá»± Ã¡n xÃ¢y dá»±ng pipeline xá»­ lÃ½ dá»¯ liá»‡u end-to-end:

1. **Thu tháº­p dá»¯ liá»‡u**: PostgreSQL â†’ Debezium CDC â†’ Kafka (real-time streaming)
2. **Xá»­ lÃ½ dá»¯ liá»‡u**: Apache Spark vá»›i Delta Lake (Bronze/Silver/Gold layers)
3. **Feature Engineering**: 15 features tá»« dá»¯ liá»‡u Ä‘á»‹a lÃ½, nhÃ¢n kháº©u há»c, giao dá»‹ch
4. **Machine Learning**: Random Forest & Logistic Regression (99%+ accuracy)
5. **Model Serving**: FastAPI cho prediction real-time

## Tech Stack

| Component         | Technology               | MÃ´ táº£                                       |
| ----------------- | ------------------------ | ------------------------------------------- |
| **Source DB**     | PostgreSQL 14            | OLTP database vá»›i CDC enabled               |
| **CDC**           | Debezium 2.5             | Change Data Capture connector               |
| **Streaming**     | Apache Kafka             | Message broker                              |
| **Processing**    | Apache Spark 3.4.1       | Stream & batch processing                   |
| **Storage**       | Delta Lake 2.4.0 + MinIO | ACID transactions, time travel              |
| **Metastore**     | Hive Metastore 3.1.3     | Metadata cache for Delta catalog (optional) |
| **Query Engine**  | Trino (Delta connector)  | Distributed SQL query engine                |
| **Visualization** | Metabase                 | BI dashboards & analytics                   |
| **ML**            | Scikit-learn, MLflow     | Model training & registry                   |
| **API**           | FastAPI                  | Real-time prediction service                |
| **Orchestration** | Apache Airflow           | Workflow scheduling                         |

## Cáº¥u trÃºc thÆ° má»¥c

```
real-time-fraud-detection-lakehouse/
â”œâ”€â”€ airflow/dags/              # Airflow DAGs (model retraining, reports)
â”œâ”€â”€ config/                    # Service configurations
â”‚   â”œâ”€â”€ metastore/             # Hive metastore config
â”‚   â”œâ”€â”€ spark/                 # Spark defaults
â”‚   â””â”€â”€ trino/                 # Trino settings
â”œâ”€â”€ data/                      # Sparkov dataset (CSV files)
â”œâ”€â”€ database/                  # PostgreSQL initialization
â”‚   â””â”€â”€ init_postgres.sql      # Schema setup (22 columns)
â”œâ”€â”€ deployment/                # Infrastructure automation
â”‚   â”œâ”€â”€ debezium/              # CDC configuration scripts
â”‚   â””â”€â”€ minio/                 # MinIO bucket setup
â”œâ”€â”€ docs/                      # Documentation
â”‚   â””â”€â”€ PROJECT_SPECIFICATION.md
â”œâ”€â”€ notebooks/                 # Jupyter notebooks (EDA, experiments)
â”œâ”€â”€ services/                  # Microservices
â”‚   â”œâ”€â”€ data-producer/         # PostgreSQL data simulator
â”‚   â”œâ”€â”€ fraud-detection-api/   # FastAPI prediction service
â”‚   â””â”€â”€ mlflow/                # MLflow tracking server
â”œâ”€â”€ spark/                     # Custom Spark with ML libraries
â”‚   â”œâ”€â”€ app/                   # PySpark jobs
â”‚   â”‚   â”œâ”€â”€ streaming_job.py   # Bronze: Kafka CDC â†’ Delta Lake (continuous)
â”‚   â”‚   â”œâ”€â”€ silver_job.py      # Silver: Feature engineering (batch every 5 min)
â”‚   â”‚   â”œâ”€â”€ gold_job.py        # Gold: Star Schema (batch every 5 min)
â”‚   â”‚   â”œâ”€â”€ ml_training_job.py # Model training pipeline
â”‚   â”‚   â”œâ”€â”€ run_silver.sh      # Shell wrapper for silver batch
â”‚   â”‚   â””â”€â”€ run_gold.sh        # Shell wrapper for gold batch
â”‚   â””â”€â”€ Dockerfile             # Spark + MLflow + ML libraries
â”œâ”€â”€ sql/                       # SQL views for Gold layer
â”‚   â””â”€â”€ gold_layer_views.sql   # Materialized views for dashboards
â”œâ”€â”€ docker-compose.yml         # 11 services orchestration
â””â”€â”€ README.md
```

## Dataset

**Sparkov Credit Card Transactions Fraud Detection Dataset** ([Kaggle](https://www.kaggle.com/datasets/kartik2112/fraud-detection))

- `data/fraudTrain.csv` - 1,296,675 transactions (01/2019 - 12/2020)
- `data/fraudTest.csv` - 555,719 transactions
- **22 columns**: Geographic (lat/long), demographic (age, gender, job), transaction (amount, merchant, category)

### Schema chÃ­nh

| Column                    | Type     | Description                  |
| ------------------------- | -------- | ---------------------------- |
| `trans_date_trans_time`   | DateTime | Thá»i gian giao dá»‹ch          |
| `cc_num`                  | Long     | Sá»‘ tháº» tÃ­n dá»¥ng              |
| `merchant`                | String   | TÃªn cá»­a hÃ ng                 |
| `category`                | String   | Danh má»¥c (grocery, gas, ...) |
| `amt`                     | Double   | Sá»‘ tiá»n giao dá»‹ch            |
| `gender`                  | String   | Giá»›i tÃ­nh (M/F)              |
| `lat`, `long`             | Double   | Vá»‹ trÃ­ khÃ¡ch hÃ ng            |
| `merch_lat`, `merch_long` | Double   | Vá»‹ trÃ­ cá»­a hÃ ng              |
| `is_fraud`                | Integer  | NhÃ£n gian láº­n (0/1)          |

### Feature Engineering (40 features)

**Geographic** (2): `distance_km` (Haversine), `is_distant_transaction`  
**Demographic** (2): `age`, `gender_encoded`  
**Time** (6): `hour`, `day_of_week`, `is_weekend`, `is_late_night`, `hour_sin`, `hour_cos`  
**Amount** (4): `log_amount`, `amount_bin`, `is_zero_amount`, `is_high_amount`  
**Original** (26): All columns from Bronze layer preserved

## Kiáº¿n trÃºc há»‡ thá»‘ng

### Medallion Architecture (Hybrid: Streaming + Batch)

Há»‡ thá»‘ng sá»­ dá»¥ng **kiáº¿n trÃºc lai** Ä‘á»ƒ tá»‘i Æ°u CPU vÃ  latency:

```
PostgreSQL (Source)
    â†“ Debezium CDC
Kafka (postgres.public.transactions)
    â†“ Bronze Streaming (Continuous, ~195% CPU)
Bronze Delta Lake (s3a://lakehouse/bronze/)
    â†“ Silver Batch (Every 5 minutes, 0% CPU during sleep)
Silver Delta Lake (s3a://lakehouse/silver/)
    â†“ Gold Batch (Every 5 minutes, 0% CPU during sleep)
Gold Delta Lake (s3a://lakehouse/gold/) - 5 tables
    â†“
    â”œâ”€â†’ Hive Metastore (Metadata cache: SHOW TABLES nhanh)
    â””â”€â†’ Trino Delta Catalog (Query data: Ä‘á»c tá»« _delta_log/ + S3)
Query Layer (Metabase/DBeaver via Delta catalog)
```

**Lá»£i Ã­ch:**

- âœ… **Bronze Layer**: Real-time CDC capture tá»« Kafka (streaming liÃªn tá»¥c)
- âœ… **Silver Layer**: Feature engineering má»—i 5 phÃºt (batch) - giáº£m 60% CPU
- âœ… **Gold Layer**: Star schema má»—i 5 phÃºt (batch) - data sáºµn sÃ ng cho analytics
- âœ… **Latency**: 5-10 phÃºt tá»« source Ä‘áº¿n Gold (cháº¥p nháº­n Ä‘Æ°á»£c cho fraud detection analytics)
- âœ… **Resource**: Bronze ~195% CPU, Silver/Gold 0% CPU khi sleep

### Delta Lake Integration

Delta Lake tá»± quáº£n lÃ½ metadata qua `_delta_log/`, **Hive Metastore chá»‰ lÃ  metadata cache** (optional):

- âœ… **ACID transactions**: Delta Lake Ä‘áº£m báº£o consistency
- âœ… **Time travel**: Lá»‹ch sá»­ thay Ä‘á»•i trong `_delta_log/`
- âœ… **Schema evolution**: Tá»± Ä‘á»™ng vá»›i `overwriteSchema=true`
- âœ… **Query engine**: Trino Delta connector Ä‘á»c trá»±c tiáº¿p tá»« S3 + `_delta_log/`
- ğŸ”„ **Hive Metastore**: Cache metadata Ä‘á»ƒ `SHOW TABLES` nhanh hÆ¡n (~100ms vs ~1-2s)

**LÆ°u Ã½ quan trá»ng:**

- ğŸ“Š **Query data**: DÃ¹ng Delta catalog (`delta.bronze.*`, `delta.silver.*`, `delta.gold.*`)
- ğŸ“‹ **List tables**: CÃ³ thá»ƒ dÃ¹ng Hive catalog (`hive.*`) nhÆ°ng KHÃ”NG query Ä‘Æ°á»£c
- âš¡ **Performance**: Hive cache giÃºp discovery operations nhanh hÆ¡n 10-20 láº§n

## HÆ°á»›ng dáº«n cháº¡y

### 1. YÃªu cáº§u há»‡ thá»‘ng

**Pháº§n cá»©ng tá»‘i thiá»ƒu:**

- **CPU**: 6 cores (khuyáº¿n nghá»‹ 8+ cores)
- **RAM**: 10GB (khuyáº¿n nghá»‹ 16GB)
- **Disk**: 30GB trá»‘ng

**Pháº§n má»m:**

- Docker Desktop 4.0+ (Windows/Mac) hoáº·c Docker Engine 20.10+ (Linux)
- Docker Compose 2.0+
- PowerShell 5.1+ (Windows) hoáº·c Bash (Linux/Mac)

**Cáº¥u hÃ¬nh Docker (Windows WSL2):**

Táº¡o file `C:\Users\<YourUsername>\.wslconfig`:

```ini
[wsl2]
memory=10GB
processors=6
swap=4GB
```

Sau Ä‘Ã³ restart WSL2:

```powershell
wsl --shutdown
```

---

### 2. Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng láº§n Ä‘áº§u

```bash
# Clone repository
git clone https://github.com/bin-bard/real-time-fraud-detection-lakehouse.git
cd real-time-fraud-detection-lakehouse

# Khá»Ÿi Ä‘á»™ng toÃ n bá»™ há»‡ thá»‘ng
docker compose up -d --build
```

**â³ Thá»i gian khá»Ÿi Ä‘á»™ng:** ~5-10 phÃºt (táº£i images + khá»Ÿi táº¡o services)

> **LÆ°u Ã½ quan trá»ng:**
>
> - Táº¥t cáº£ services tá»± Ä‘á»™ng start
> - Bronze streaming cháº¡y liÃªn tá»¥c (real-time CDC)
> - Silver/Gold jobs cháº¡y batch má»—i 5 phÃºt qua Airflow DAG
> - Data producer tá»± Ä‘á»™ng insert dá»¯ liá»‡u vÃ o PostgreSQL

---

### 3. Kiá»ƒm tra há»‡ thá»‘ng Ä‘Ã£ khá»Ÿi Ä‘á»™ng

#### Verify táº¥t cáº£ containers Ä‘ang cháº¡y

```bash
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
```

**Output mong Ä‘á»£i:** 15+ containers vá»›i status `Up`

#### Kiá»ƒm tra logs Pipeline

````bash
# Bronze streaming (real-time CDC)
docker logs -f bronze-streaming

# Airflow scheduler (quáº£n lÃ½ batch jobs)
#### Verify Pipeline hoáº¡t Ä‘á»™ng

**1. Bronze Layer (Real-time CDC):**

```bash
docker logs bronze-streaming --tail 20
````

Log thÃ nh cÃ´ng:

```
25/12/04 14:10:46 INFO ProcessingTimeMicroBatchStream: Batch 5 processing started
25/12/04 14:10:47 INFO WriteToDataSourceV2: Writing 142 records to Bronze layer...
25/12/04 14:10:48 INFO WriteToDataSourceV2: âœ… Batch 5 written successfully
```

**2. Airflow DAG (Batch ETL):**

- Truy cáº­p Airflow UI: http://localhost:8081 (`admin`/`admin`)
- Check DAG `lakehouse_pipeline_taskflow` (cháº¡y má»—i 5 phÃºt)
- Xem task logs Ä‘á»ƒ verify Silver/Gold processing

**3. Data trong MinIO:**

- Truy cáº­p MinIO Console: http://localhost:9001 (`minio`/`minio123`)
- Navigate: `lakehouse/bronze/transactions/` â†’ verify cÃ³ Parquet files
- Navigate: `lakehouse/silver/transactions/` â†’ verify cÃ³ data (sau ~5 phÃºt)
- Navigate: `lakehouse/gold/` â†’ verify cÃ³ 5 folders (dim*\*, fact*\*)

**4. Trino Query (verify end-to-end):**

```bash
docker exec -it trino trino --server localhost:8081
```

````sql
-- âš ï¸ IMPORTANT: Query via DELTA catalog (not HIVE)
-- Hive catalog can list tables but cannot query Delta format

-- Kiá»ƒm tra sá»‘ lÆ°á»£ng records
SELECT COUNT(*) FROM delta.bronze.transactions;
SELECT COUNT(*) FROM delta.silver.transactions;
SELECT COUNT(*) FROM delta.gold.fact_transactions;

-- Sample data
SELECT * FROM delta.gold.fact_transactions LIMIT 5;

-- Exit
quit;
```/ 7.76GiB
gold-job           0.00%     2.555MiB / 7.76GiB
````

âœ… **Bronze**: ~195% CPU (streaming liÃªn tá»¥c)  
âœ… **Silver**: 0% CPU (Ä‘ang sleep 5 phÃºt)  
âœ… **Gold**: 0% CPU (Ä‘ang sleep 5 phÃºt)

---

### 4. Cáº¥u trÃºc Spark Jobs & Data Flow

#### Kiáº¿n trÃºc xá»­ lÃ½ dá»¯ liá»‡u

```
PostgreSQL (Source DB)
    â†“ Debezium CDC (Change Data Capture)
Kafka Topic: postgres.public.transactions
    â†“ Bronze Streaming Job (Continuous, ~195% CPU)
Bronze Delta Lake (s3a://lakehouse/bronze/)
    â†“ Silver Batch Job (Every 5 min via Airflow)
Silver Delta Lake (40 features, s3a://lakehouse/silver/)
    â†“ Gold Batch Job (Every 5 min via Airflow)
Gold Delta Lake (5 tables, s3a://lakehouse/gold/)
    â†“ Hive Metastore Registration (Auto via Airflow)
    â†“ Trino Query Engine (hive catalog)
Metabase Dashboard / Analytics
```

#### Bronze Layer (`streaming_job.py`)

**Mode:** Structured Streaming (continuous)  
**Input:** Kafka topic `postgres.public.transactions`  
**Processing:**

- Parse Debezium CDC format (`$.after.*`)
- Extract 22 fields tá»« JSON payload
- Add metadata: `ingestion_time`, partition keys (`year`, `month`, `day`)

**Output:** Delta Lake `s3a://lakehouse/bronze/transactions`  
**Trigger:** Continuous (processAvailableNow)  
**Checkpoint:** `s3a://lakehouse/checkpoints/kafka_to_bronze`

**Container:** `bronze-streaming` (auto-start)

```bash
# Xem logs
docker logs -f bronze-streaming
```

#### Silver Layer (`silver_job.py`)

**Mode:** Batch (triggered by Airflow every 5 minutes)  
**Input:** Bronze Delta Lake  
**Processing:**

- **Incremental:** Chá»‰ xá»­ lÃ½ records má»›i (watermark-based)
- Type casting (String â†’ Double/Long/Date)
- Data quality checks (fillna, outliers)
- **Feature Engineering:** 40 features
  - Geographic: `distance_km`, `is_distant_transaction`
  - Demographic: `age`, `gender_encoded`
  - Time: `hour`, `day_of_week`, `is_weekend`, `is_late_night`, `hour_sin`, `hour_cos`
  - Amount: `log_amount`, `amount_bin`, `is_zero_amount`, `is_high_amount`

**Output:** Delta Lake `s3a://lakehouse/silver/transactions`  
**Partitioning:** By `year`, `month`, `day`  
**Schema Evolution:** `overwriteSchema=true` (support ancient dates vá»›i LEGACY mode)

**Trigger:** Airflow DAG `lakehouse_pipeline_taskflow` task `run_silver_transformation`

```bash
# Xem logs trong Airflow UI
http://localhost:8081 â†’ lakehouse_pipeline_taskflow â†’ run_silver_transformation â†’ Logs
```

#### Gold Layer (`gold_job.py`)

**Mode:** Batch (triggered by Airflow every 5 minutes)  
**Input:** Silver Delta Lake  
**Processing:**

- **Star Schema transformation**
- Hash-based surrogate keys (MD5)
- **Incremental:** Chá»‰ xá»­ lÃ½ records má»›i tá»« Silver

**Output:** 5 Delta tables

1. **`dim_customer`** - Customer dimension
   - `customer_key` (PK), `cc_num`, `first`, `last`, `gender`, `dob`, `job`
2. **`dim_merchant`** - Merchant dimension
   - `merchant_key` (PK), `merchant`, `category`, `merch_lat`, `merch_long`
3. **`dim_time`** - Time dimension
   - `time_key` (PK), `trans_date`, `hour`, `day_of_week`, `is_weekend`, `is_late_night`
4. **`dim_location`** - Location dimension
   - `location_key` (PK), `city`, `state`, `zip`, `lat`, `long`, `city_pop`
5. **`fact_transactions`** - Fact table
   - Foreign keys: `customer_key`, `merchant_key`, `time_key`, `location_key`
   - Measures: `amt`, `distance_km`, `age`, `is_fraud`
   - Degenerate dimensions: `trans_num`, `trans_timestamp`

**Trigger:** Airflow DAG `lakehouse_pipeline_taskflow` task `run_gold_transformation`

```bash
# Xem logs trong Airflow UI
http://localhost:8081 â†’ lakehouse_pipeline_taskflow â†’ run_gold_transformation â†’ Logs
```

#### Hive Metastore Registration (Optional Metadata Cache)

**Vai trÃ²**: Hive Metastore lÃ  **metadata cache layer** giÃºp `SHOW TABLES` nhanh hÆ¡n.

**Mode:** Auto-registration via Airflow  
**Trigger:** Sau khi Gold job hoÃ n thÃ nh  
**Tables registered:** 7 tables (Bronze, Silver, Gold)

**âš ï¸ QUAN TRá»ŒNG - Hiá»ƒu Ä‘Ãºng vai trÃ²:**

- âœ… **Hive catalog** (`hive.*`): List metadata (SHOW TABLES) - nhanh
- âŒ **Hive catalog**: KHÃ”NG query Ä‘Æ°á»£c Delta tables (lá»—i "Cannot query Delta Lake table")
- âœ… **Delta catalog** (`delta.*`): Query data thá»±c táº¿ - Báº®T BUá»˜C dÃ¹ng cho SELECT

**CÃ³ thá»ƒ bá» Hive Metastore khÃ´ng?**

- CÃ“ - Delta connector tá»± discover tables tá»« S3
- NHÆ¯NG: `SHOW TABLES` sáº½ cháº­m hÆ¡n (scan MinIO má»—i láº§n)
- KHUYáº¾N NGHá»Š: Giá»¯ láº¡i Ä‘á»ƒ tá»‘i Æ°u performance (Ä‘Ã£ config sáºµn)

```bash
docker exec -it trino trino --server localhost:8081
```

```sql
-- List catalogs vÃ  schemas
SHOW CATALOGS;  -- Expect: delta, hive, system
SHOW SCHEMAS FROM delta;  -- Expect: bronze, silver, gold

-- List tables (Cáº£ 2 catalog Ä‘á»u OK)
SHOW TABLES FROM delta.gold;  -- âœ… Khuyáº¿n nghá»‹ (consistent)
SHOW TABLES FROM hive.gold;   -- âœ… OK (tá»« metadata cache)
-- Output: dim_customer, dim_merchant, dim_time, dim_location, fact_transactions

-- âš ï¸ QUAN TRá»ŒNG: Query data CHá»ˆ dÃ¹ng Delta catalog!
-- LÃ½ do: Hive connector KHÃ”NG Ä‘á»c Ä‘Æ°á»£c Delta format

-- âœ… ÄÃšNG - Query via Delta catalog
SELECT COUNT(*) FROM delta.bronze.transactions;
SELECT COUNT(*) FROM delta.silver.transactions;
SELECT COUNT(*) FROM delta.gold.fact_transactions;

-- âŒ SAI - Query via Hive catalog (sáº½ lá»—i!)
-- SELECT COUNT(*) FROM hive.bronze.transactions;  -- Error: Cannot query Delta Lake table

quit;
```

---

### 6. Truy cáº­p Services

| Service             | URL                   | Username / Password                             | Ghi chÃº                                           |
| ------------------- | --------------------- | ----------------------------------------------- | ------------------------------------------------- |
| **Airflow**         | http://localhost:8081 | `admin` / `admin`                               | Workflow orchestration & DAG management           |
| Spark Master UI     | http://localhost:8080 | KhÃ´ng cáº§n                                       | Monitoring Spark jobs                             |
| MinIO Console       | http://localhost:9001 | `minio` / `minio123`                            | Quáº£n lÃ½ buckets vÃ  files (Data Lake)              |
| MLflow UI           | http://localhost:5000 | KhÃ´ng cáº§n                                       | ML model tracking & registry                      |
| Kafka UI            | http://localhost:9002 | KhÃ´ng cáº§n                                       | Xem topics, messages, consumer groups             |
| Trino UI            | http://localhost:8085 | KhÃ´ng cáº§n                                       | Query engine monitoring                           |
| Metabase            | http://localhost:3000 | TÃ¹y chá»n (vÃ­ dá»¥:`admin@admin.com` / `admin123`) | BI Dashboard, tá»± táº¡o tÃ i khoáº£n admin láº§n Ä‘áº§u      |
| Fraud Detection API | http://localhost:8000 | KhÃ´ng cáº§n                                       | Real-time prediction endpoint                     |
| Kafka Broker        | localhost:9092        | KhÃ´ng cáº§n                                       | Kafka bootstrap server                            |
| PostgreSQL (Source) | localhost:5432        | `postgres` / `postgres`                         | Database `frauddb`                                |
| Metabase DB         | Internal              | `postgres` / `postgres`                         | Database `metabase` (khÃ´ng cáº§n truy cáº­p thá»§ cÃ´ng) |
| Hive Metastore DB   | Internal (9083)       | `hive` / `hive`                                 | Postgres cho Hive (khÃ´ng expose ra ngoÃ i)         |

> **LÆ°u Ã½ quan trá»ng:**
>
> - **MinIO, PostgreSQL:** Credentials cá»‘ Ä‘á»‹nh trong `docker-compose.yml` (cÃ³ thá»ƒ Ä‘á»•i trÆ°á»›c khi khá»Ÿi Ä‘á»™ng).
> - **Metabase:** Táº¡o tÃ i khoáº£n admin khi truy cáº­p láº§n Ä‘áº§u, email/password tÃ¹y chá»n (vÃ­ dá»¥: `admin@admin.com` / `admin123`).
> - **Airflow:** TÃ i khoáº£n máº·c Ä‘á»‹nh `admin/admin` (Ä‘Ã£ Ä‘Æ°á»£c tá»± Ä‘á»™ng táº¡o khi khá»Ÿi Ä‘á»™ng).
> - **Spark UI, Kafka UI, Trino UI:** KhÃ´ng yÃªu cáº§u Ä‘Äƒng nháº­p.

#### Káº¿t ná»‘i Trino qua DBeaver/SQL Client

Äá»ƒ truy váº¥n dá»¯ liá»‡u Lakehouse qua SQL client (DBeaver, DataGrip, v.v.):

**ThÃ´ng tin káº¿t ná»‘i:**

- **Driver**: Trino (hoáº·c Presto)
- **Host**: `localhost`
- **Port**: `8085`
- **Database/Catalog**: `delta` âš ï¸ (KHÃ”NG pháº£i `hive`)
- **Schema**: `gold`, `silver`, hoáº·c `bronze`
- **Username**: `trino` (hoáº·c báº¥t ká»³)
- **Password**: Ä‘á»ƒ trá»‘ng
- **JDBC URL**: `jdbc:trino://localhost:8085/delta`

**âš ï¸ QUAN TRá»ŒNG:** DÃ¹ng catalog `delta` Ä‘á»ƒ query, khÃ´ng pháº£i `hive`!

**VÃ­ dá»¥ truy váº¥n:**

```sql
-- Xem táº¥t cáº£ schemas
SHOW SCHEMAS FROM delta;

-- Xem tables trong Gold layer
SHOW TABLES FROM delta.gold;

-- Query fact table
SELECT * FROM delta.gold.fact_transactions LIMIT 10;

-- Join vá»›i dimension tables
SELECT
    f.trans_num,
    c.first_name,
    c.last_name,
    m.merchant_name,
    f.amt,
    f.is_fraud
FROM delta.gold.fact_transactions f
JOIN delta.gold.dim_customer c ON f.customer_key = c.customer_key
JOIN delta.gold.dim_merchant m ON f.merchant_key = m.merchant_key
WHERE f.is_fraud = 1
LIMIT 20;
```

---

### 7. Airflow Workflow Orchestration

Há»‡ thá»‘ng sá»­ dá»¥ng **Apache Airflow 2.8.0** Ä‘á»ƒ quáº£n lÃ½ cÃ¡c workflow batch processing:

#### Truy cáº­p Airflow UI

```bash
# URL: http://localhost:8081
# Username: admin
# Password: admin
```

#### DAGs cÃ³ sáºµn

**1. `lakehouse_pipeline_taskflow`** - Lakehouse ETL Pipeline (TaskFlow API)

- **Schedule**: Má»—i 5 phÃºt (`*/5 * * * *`)
- **Tasks**:
  1. `check_bronze_data` - Kiá»ƒm tra Bronze streaming Ä‘ang cháº¡y
  2. `run_silver_transformation` - Cháº¡y Silver job (Bronze â†’ Features)
  3. `run_gold_transformation` - Cháº¡y Gold job (Silver â†’ Star Schema)

### 5. Airflow Workflow Orchestration

Há»‡ thá»‘ng sá»­ dá»¥ng **Apache Airflow 2.8.0** Ä‘á»ƒ quáº£n lÃ½ batch ETL vÃ  ML training:

#### Truy cáº­p Airflow UI

**URL:** http://localhost:8081  
**Credentials:** `admin` / `admin`

#### DAGs cÃ³ sáºµn

**1. `lakehouse_pipeline_taskflow`** - Batch ETL Pipeline

- **Schedule**: Má»—i 5 phÃºt (`*/5 * * * *`)
- **Má»¥c Ä‘Ã­ch**: Transform Bronze â†’ Silver â†’ Gold
- **Tasks**:
  1. âœ… `check_bronze_data` - Verify Bronze cÃ³ data má»›i
  2. ğŸ”„ `run_silver_transformation` - Feature engineering (40 features)
  3. â­ `run_gold_transformation` - Star schema (5 tables)
  4. ğŸ“‹ `register_tables_to_hive` - Register vÃ o Hive Metastore
  5. âœ”ï¸ `verify_trino_access` - Test Trino query
  6. ğŸ“Š `send_pipeline_summary` - Log summary

**Xem logs:**

```bash
# Trong Airflow UI
DAGs â†’ lakehouse_pipeline_taskflow â†’ Graph â†’ Click task â†’ Logs

# Hoáº·c check summary trong logs
docker logs airflow-scheduler | grep "LAKEHOUSE PIPELINE"
```

**Output máº«u:**

```
[2025-12-04, 21:16:50] INFO - ============================================================
[2025-12-04, 21:16:50] INFO - Bronze Check: ready
[2025-12-04, 21:16:50] INFO - Silver Status: success
[2025-12-04, 21:16:50] INFO -   - Records Processed: 8,547
[2025-12-04, 21:16:50] INFO - Gold Status: success
[2025-12-04, 21:16:50] INFO -   - Tables Created: 5
[2025-12-04, 21:16:50] INFO - Hive Registration: success
[2025-12-04, 21:16:50] INFO -   - Tables Registered: 7
[2025-12-04, 21:16:50] INFO - Trino Verification: verified
[2025-12-04, 21:16:50] INFO - ============================================================
```

---

**2. `model_retraining_taskflow`** - ML Model Training

- **Schedule**: HÃ ng ngÃ y lÃºc 02:00 AM (`0 2 * * *`)
- **Má»¥c Ä‘Ã­ch**: Train RandomForest + LogisticRegression models
- **Tasks**:
  1. ğŸ“Š `check_data_availability` - Verify Silver layer cÃ³ data
  2. ğŸ§  `train_ml_models` - Train 2 models vá»›i MLflow tracking
  3. âœ… `verify_models_registered` - Check MLflow registry
  4. ğŸ“¬ `send_notification` - Training summary

**âš ï¸ Quan trá»ng - Resource Management:**

ML training tiÃªu tá»‘n nhiá»u CPU/RAM. **TrÆ°á»›c khi cháº¡y**, giáº£i phÃ³ng tÃ i nguyÃªn:

```powershell
# Táº¯t services khÃ´ng cáº§n thiáº¿t (giáº£i phÃ³ng ~2GB RAM + 1-2 CPU cores)
.\scripts\prepare-ml-training.ps1

# Sau Ä‘Ã³ trigger DAG qua Airflow UI hoáº·c:
# Airflow UI â†’ model_retraining_taskflow â†’ Trigger DAG (â–¶ï¸)

# Sau khi training xong, khÃ´i phá»¥c services
.\scripts\restore-services.ps1
```

**Services bá»‹ táº¯t khi training:**

- âŒ Trino (query engine - khÃ´ng cáº§n cho training)
- âŒ Metabase (dashboard - khÃ´ng cáº§n cho training)
- âŒ Hive Metastore (catalog - training dÃ¹ng Delta trá»±c tiáº¿p)
- âŒ Debezium (CDC connector - táº¡m dá»«ng data ingestion)
- âŒ Data Producer (data generator - táº¡m dá»«ng)

**Services váº«n cháº¡y:**

- âœ… Spark (master + worker) - Cháº¡y training job
- âœ… MLflow - Track experiments vÃ  lÆ°u models
- âœ… MinIO - Storage cho Delta Lake + artifacts
- âœ… PostgreSQL - Airflow DB + MLflow backend
- âœ… Airflow - Orchestration

**Spark config tá»‘i Æ°u cho training:**

```python
'--conf', 'spark.cores.max=2',           # Giáº£m tá»« 4â†’2 cores
'--conf', 'spark.executor.cores=1',      # 1 core/executor
'--conf', 'spark.executor.memory=1g',    # Giáº£m tá»« 2gâ†’1g
'--conf', 'spark.driver.memory=1g',      # Giáº£m tá»« 2gâ†’1g
```

**Xem training logs:**

```bash
# Trong Airflow UI
DAGs â†’ model_retraining_taskflow â†’ Graph â†’ train_ml_models â†’ Logs

# Hoáº·c check Spark job output
docker logs spark-master | grep "FraudDetectionMLTraining"
```

**Verify models trong MLflow:**

- Truy cáº­p MLflow UI: http://localhost:5000
- Check experiment `fraud_detection_production`
- Verify cÃ³ 2 registered models: `fraud_detection_random_forest`, `fraud_detection_logistic_regression`

### XÃ¡c minh CDC (INSERT/UPDATE/DELETE) vÃ  giÃ¡ trá»‹ trÆ°á»ng `amt`

**1. Láº¥y trans_num thá»±c táº¿ Ä‘á»ƒ test:**

```sql
SELECT trans_num FROM transactions LIMIT 5;
```

**2. Thá»±c hiá»‡n cÃ¡c thao tÃ¡c trÃªn PostgreSQL:**

```sql
-- UPDATE
UPDATE transactions SET amt = amt + 1 WHERE trans_num = '<trans_num thá»±c táº¿>';
-- DELETE
DELETE FROM transactions WHERE trans_num = '<trans_num thá»±c táº¿>';
```

**3. Kiá»ƒm tra message CDC trÃªn Kafka:**

```bash
docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic postgres.public.transactions --from-beginning --max-messages 100000 --timeout-ms 3000 2>$null | Select-String -Pattern "<trans_num thá»±c táº¿>"
```

Káº¿t quáº£:

- `"op":"c"` = insert, `"op":"u"` = update, `"op":"d"` = delete.
- TrÆ°á»ng `amt` sáº½ á»Ÿ dáº¡ng mÃ£ hÃ³a Base64 (vÃ­ dá»¥: "amt":"Ark=").

**4. Decode giÃ¡ trá»‹ amt (PowerShell):**

```powershell
[System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String("Ark="))
```

Viá»‡c decode nÃ y chá»‰ Ä‘á»ƒ xem giÃ¡ trá»‹ thá»±c, khÃ´ng áº£nh hÆ°á»Ÿng pipeline.

**5. Xem Kafka messages qua UI:**

Truy cáº­p Kafka UI táº¡i http://localhost:9002 Ä‘á»ƒ xem topics, messages, consumer groups qua giao diá»‡n web thÃ¢n thiá»‡n.

**Monitor Spark jobs:**

---

### 7. Query Data vá»›i Trino + Metabase

#### Kiá»ƒm tra Tables Ä‘Ã£ Ä‘Æ°á»£c Register

Há»‡ thá»‘ng tá»± Ä‘á»™ng register Delta Lake tables vÃ o Hive Metastore má»—i giá»:

```bash
# Kiá»ƒm tra registration logs
docker logs hive-registration --tail 30

# Verify tables trong Trino
docker exec trino trino --server localhost:8081 --execute "SHOW TABLES FROM delta.gold"
```

**Output mong Ä‘á»£i:**

```
"dim_customer"
"dim_location"
"dim_merchant"
"dim_time"
"fact_transactions"
```

#### Káº¿t ná»‘i Metabase

1. **Truy cáº­p Metabase:** http://localhost:3000
2. **First-time setup:** Táº¡o tÃ i khoáº£n admin
3. **Add Database:**
   - Database type: **Trino**
   - Display name: `Fraud Detection Lakehouse`
   - Host: `trino`
   - Port: `8081`
   - Catalog: `delta`
   - Database: `gold`
   - Username/Password: (Ä‘á»ƒ trá»‘ng)
4. **Save** â†’ Metabase sáº½ sync metadata

#### Sample Queries

````sql
-- Fraud rate by merchant category
SELECT
  dm.category,
  COUNT(*) as total_transactions,
  SUM(CASE WHEN ft.is_fraud = true THEN 1 ELSE 0 END) as fraud_count,
### 6. Troubleshooting & Maintenance

#### Reset toÃ n bá»™ há»‡ thá»‘ng

**Cáº£nh bÃ¡o:** XÃ³a toÃ n bá»™ data vÃ  volumes!

```bash
docker compose down -v
docker compose up -d --build
````

#### XÃ³a chá»‰ data lakehouse (giá»¯ láº¡i containers)

```bash
# Stop data ingestion
docker compose stop data-producer bronze-streaming

# XÃ³a Delta Lake data (PowerShell)
docker exec minio mc rm -r --force lakehouse/bronze/ lakehouse/silver/ lakehouse/gold/ lakehouse/checkpoints/

# Restart pipeline
docker compose up -d bronze-streaming data-producer
```

#### Check logs khi cÃ³ lá»—i

```bash
# Airflow scheduler (quáº£n lÃ½ DAGs)
docker logs airflow-scheduler --tail 100

# Bronze streaming (CDC ingestion)
docker logs bronze-streaming --tail 50

# Spark Master (job allocation)
docker logs spark-master --tail 50

# MLflow (model tracking)
docker logs mlflow --tail 50
```

#### Common Issues

**1. Airflow task timeout (SIGTERM)**

**NguyÃªn nhÃ¢n:** Spark job chiáº¿m quÃ¡ nhiá»u CPU â†’ Airflow DB connection timeout

**Giáº£i phÃ¡p:**

```powershell
# Giáº£i phÃ³ng tÃ i nguyÃªn trÆ°á»›c khi cháº¡y ML training
.\scripts\prepare-ml-training.ps1
```

**2. High CPU usage (>500%)**

**Kiá»ƒm tra:**

```bash
docker stats --no-stream
```

**BÃ¬nh thÆ°á»ng:**

- `bronze-streaming`: ~195% CPU (continuous)
- `spark-master`: ~50-100% CPU khi cÃ³ job
- `airflow-*`: ~10-30% CPU

**Báº¥t thÆ°á»ng:** Náº¿u tá»•ng CPU >600%, restart services:

```bash
docker compose restart bronze-streaming spark-master spark-worker
```

**3. No data in Silver/Gold**

**Debug:**

```bash
# 1. Check Bronze cÃ³ data khÃ´ng
docker exec -it trino trino --execute "SELECT COUNT(*) FROM hive.bronze.transactions"

# 2. Check Airflow DAG lakehouse_pipeline_taskflow cÃ³ cháº¡y khÃ´ng
# â†’ Airflow UI: http://localhost:8081

# 3. Check logs cá»§a Silver transformation
# â†’ Airflow UI â†’ lakehouse_pipeline_taskflow â†’ run_silver_transformation â†’ Logs
```

**4. MLflow empty (no models)**

**NguyÃªn nhÃ¢n:** Training job chÆ°a cháº¡y hoáº·c fail do insufficient data

**Giáº£i phÃ¡p:**

```bash
# 1. Verify Silver cÃ³ data (cáº§n Ã­t nháº¥t 1000 records)
docker exec -it trino trino --execute "SELECT COUNT(*) FROM hive.silver.transactions WHERE amt > 0"

# 2. Trigger training DAG manually
# Airflow UI â†’ model_retraining_taskflow â†’ Trigger DAG

# 3. Check training logs
# Airflow UI â†’ model_retraining_taskflow â†’ train_ml_models â†’ Logs
```

**5. Debezium amt field = NULL (Base64 encoding)**

**NguyÃªn nhÃ¢n:** Debezium máº·c Ä‘á»‹nh encode NUMERIC/DECIMAL as Base64

**Giáº£i phÃ¡p:** Connector Ä‘Ã£ fix vá»›i `decimal.handling.mode=double` trong `deployment/debezium/setup-connector.sh`

**Verify fix:**

```bash
# Check Kafka message format
docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic postgres.public.transactions --max-messages 1
```

**Output Ä‘Ãºng:** `"amt": 23.45` (plain double, khÃ´ng pháº£i `"amt": "AfE="`)

**ğŸ“– Chi tiáº¿t:** Xem [`docs/TROUBLESHOOTING.md`](docs/TROUBLESHOOTING.md) cho 15+ issues khÃ¡c
docker logs bronze-streaming --tail 50

# Check Silver batch

docker logs silver-job --tail 50

# Check Gold batch

docker logs gold-job --tail 50

# Check Spark Master

docker logs spark-master

```

#### Common issues

**High CPU usage**:

- Bronze streaming: ~195% CPU (bÃ¬nh thÆ°á»ng)
- Silver/Gold batch: 0% CPU khi sleep, spike khi cháº¡y (bÃ¬nh thÆ°á»ng)
- Náº¿u cáº£ 3 jobs Ä‘á»u >200% CPU: Xem xÃ©t giáº£m batch size hoáº·c tÄƒng sleep interval

**Job fails to start**:

- Check Spark Master UI: http://localhost:8080
- Verify MinIO accessible: http://localhost:9001
- Check Kafka messages: `docker logs kafka`

---

### 8. Lakehouse Structure

```

s3a://lakehouse/
â”œâ”€â”€ bronze/transactions/ # Raw CDC data (Debezium format parsed)
â”‚ â””â”€â”€ \_delta_log/ # Delta Lake transaction logs
â”œâ”€â”€ silver/transactions/ # 40 engineered features
â”‚ â””â”€â”€ \_delta_log/
â”œâ”€â”€ gold/ # Star Schema (5 tables)
â”‚ â”œâ”€â”€ dim_customer/
â”‚ â”œâ”€â”€ dim_merchant/
â”‚ â”œâ”€â”€ dim_time/
â”‚ â”œâ”€â”€ dim_location/
â”‚ â””â”€â”€ fact_transactions/
â”œâ”€â”€ checkpoints/ # Spark streaming checkpoints
â”‚ â”œâ”€â”€ kafka_to_bronze/ # Bronze streaming state
â”‚ â”œâ”€â”€ bronze_to_silver_batch/ # Silver batch watermark
â”‚ â””â”€â”€ silver_to_gold_batch/ # Gold batch watermark
â””â”€â”€ models/ # ML models & artifacts (future)

```

---

### 9. Kiáº¿n trÃºc Data Flow

**Luá»“ng xá»­ lÃ½ hoÃ n chá»‰nh:**

```

PostgreSQL INSERT
â†“ Debezium CDC (Change Data Capture)
Kafka Topic: postgres.public.transactions
â†“ Bronze Streaming (Continuous, ~195% CPU)
Bronze Delta Lake (s3a://lakehouse/bronze/)
â†“ Silver Batch (Every 5 minutes, spike to ~100% CPU then sleep)
Silver Delta Lake (40 features, s3a://lakehouse/silver/)
â†“ Gold Batch (Every 5 minutes, spike to ~100% CPU then sleep)
Gold Delta Lake (5 tables, s3a://lakehouse/gold/)
â†“ Hive Metastore (Auto-registration every 1 hour)
â†“ Trino Query Engine (Delta Catalog via Hive Metastore)
Metabase Dashboard / Analytics

```

**Latency:**

- Bronze: Real-time (~1-2 seconds from PostgreSQL INSERT)
- Silver: 5-10 minutes (batch interval + processing time)
- Gold: 10-15 minutes (waits for Silver + processing time)

**Resource Usage:**

- Bronze: 195% CPU (continuous streaming)
- Silver: 0% CPU (95% of time), spike when processing
- Gold: 0% CPU (95% of time), spike when processing
- **Total**: ~195-400% CPU (depending on batch cycle)

---

### 10. Services Container Map

| Service             | URL                   | Credentials            | Purpose                                  |
| ------------------- | --------------------- | ---------------------- | ---------------------------------------- |
| Spark Master UI     | http://localhost:8080 | None                   | Monitor Spark jobs & resource allocation |
| MinIO Console       | http://localhost:9001 | minio / minio123       | S3-compatible Data Lake storage          |
| Trino UI            | http://localhost:8085 | None                   | Distributed SQL query engine             |
| Metabase            | http://localhost:3000 | (setup on first visit) | BI Dashboard & visualization             |
| MLflow UI           | http://localhost:5000 | None                   | ML model tracking & registry             |
| Kafka UI            | http://localhost:9002 | None                   | Kafka topics & messages monitoring       |
| Fraud Detection API | http://localhost:8000 | None                   | Real-time prediction endpoint (future)   |
| Kafka Broker        | localhost:9092        | None                   | Message streaming platform               |
| PostgreSQL          | localhost:5432        | postgres / postgres    | Source database (frauddb)                |
| Hive Metastore      | localhost:9083        | None (Thrift)          | Table metadata store for Trino           |

---

### 11. Key Features & Achievements

âœ… **Hybrid Architecture**: Streaming (Bronze) + Batch (Silver/Gold) for optimal CPU usage
âœ… **Real-time CDC**: Debezium captures INSERT/UPDATE/DELETE from PostgreSQL
âœ… **ACID Transactions**: Delta Lake ensures data consistency
âœ… **Incremental Processing**: Only process new data (watermark-based)
âœ… **Schema Evolution**: Support for ancient dates with LEGACY mode
âœ… **40 Features**: Geographic, demographic, time-based, amount-based
âœ… **Star Schema**: 4 dimensions + 1 fact table for analytics
âœ… **Trino Query Engine**: Delta connector queries directly from `_delta_log/` + MinIO
âœ… **Metadata Cache**: Hive Metastore speeds up `SHOW TABLES` (~100ms vs ~1-2s)
âœ… **Auto Registration**: Tables auto-register to Metastore via Airflow
âœ… **Metabase Ready**: Pre-configured for BI dashboards (Delta catalog)
âœ… **60% CPU Reduction**: From 300%+ to ~195% by moving to batch processing

### 12. Understanding Hive Metastore Role

**â“ Táº¡i sao cáº§n Hive Metastore náº¿u Delta Lake tá»± quáº£n lÃ½ metadata?**

| Aspect | Delta Lake (_delta_log/) | Hive Metastore |
|--------|-------------------------|----------------|
| **Primary role** | Data storage + transaction logs | Metadata cache |
| **Query data** | âœ… YES (via Delta connector) | âŒ NO (Hive connector cannot read Delta) |
| **List tables** | âœ… YES (~1-2s, scan S3) | âœ… YES (~100ms, cache hit) |
| **SHOW SCHEMAS** | âœ… YES (slow) | âœ… YES (fast) |
| **Required?** | âœ… MANDATORY | âš ï¸ OPTIONAL (performance optimization) |

**ğŸ¯ Káº¿t luáº­n:**
- **Hive Metastore = Metadata cache** (giÃºp discovery nhanh hÆ¡n 10-20x)
- **Delta connector = Query engine** (Ä‘á»c trá»±c tiáº¿p tá»« `_delta_log/` + S3)
- **CÃ³ thá»ƒ bá» Hive?** CÃ“ - nhÆ°ng `SHOW TABLES` sáº½ cháº­m hÆ¡n
- **NÃªn giá»¯?** NÃŠN - setup Ä‘Ã£ tá»‘i Æ°u, khÃ´ng tá»‘n nhiá»u resources (~300MB RAM)

**ğŸ“š Chi tiáº¿t:** Xem [`docs/HIVE_METASTORE_ROLE.md`](docs/HIVE_METASTORE_ROLE.md)

---

## Chi tiáº¿t ká»¹ thuáº­t

Xem file `docs/PROJECT_SPECIFICATION.md` Ä‘á»ƒ hiá»ƒu rÃµ:

- Kiáº¿n trÃºc há»‡ thá»‘ng chi tiáº¿t
- YÃªu cáº§u nghiá»‡p vá»¥
- Data flow vÃ  processing layers
- ML pipeline specifications

## Additional Documentation

- **[METABASE_SETUP.md](docs/METABASE_SETUP.md)** - Metabase connection guide (Delta catalog) + 7 sample fraud detection queries
- **[HIVE_METASTORE_ROLE.md](docs/HIVE_METASTORE_ROLE.md)** - â­ **Giáº£i thÃ­ch vai trÃ² Hive Metastore** (metadata cache vs query engine)
- **[HIVE_TRINO_FIX.md](docs/HIVE_TRINO_FIX.md)** - Hive Metastore setup fixes + Trino CLI usage
- **[TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)** - Detailed solutions for 6 major issues encountered during setup
- **[PROJECT_SPECIFICATION.md](docs/PROJECT_SPECIFICATION.md)** - Full architecture specifications and requirements
- **[IMPLEMENTATION_PLAN_A.md](docs/IMPLEMENTATION_PLAN_A.md)** - Hybrid architecture implementation details
```
