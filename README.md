# Real-Time Fraud Detection Data Lakehouse

H·ªá th·ªëng Data Lakehouse ph√°t hi·ªán gian l·∫≠n th·∫ª t√≠n d·ª•ng trong th·ªùi gian th·ª±c s·ª≠ d·ª•ng **Sparkov Credit Card Transactions Dataset** v·ªõi ki·∫øn tr√∫c Medallion (Bronze-Silver-Gold).

![Architecture Diagram](docs/architecture.png)

## T·ªïng quan

D·ª± √°n x√¢y d·ª±ng pipeline x·ª≠ l√Ω d·ªØ li·ªáu end-to-end:

1. **Thu th·∫≠p d·ªØ li·ªáu**: PostgreSQL ‚Üí Debezium CDC ‚Üí Kafka (real-time streaming)
2. **X·ª≠ l√Ω d·ªØ li·ªáu**: Apache Spark v·ªõi Delta Lake (Bronze/Silver/Gold layers)
3. **Feature Engineering**: 15 features t·ª´ d·ªØ li·ªáu ƒë·ªãa l√Ω, nh√¢n kh·∫©u h·ªçc, giao d·ªãch
4. **Machine Learning**: Random Forest & Logistic Regression (99%+ accuracy)
5. **Model Serving**: FastAPI cho prediction real-time

## Tech Stack

| Component         | Technology               | M√¥ t·∫£                          |
| ----------------- | ------------------------ | ------------------------------ |
| **Source DB**     | PostgreSQL 14            | OLTP database v·ªõi CDC enabled  |
| **CDC**           | Debezium 2.5             | Change Data Capture connector  |
| **Streaming**     | Apache Kafka             | Message broker                 |
| **Processing**    | Apache Spark 3.4.1       | Stream & batch processing      |
| **Storage**       | Delta Lake 2.4.0 + MinIO | ACID transactions, time travel |
| **Metastore**     | Hive Metastore           | Table metadata management      |
| **Query Engine**  | Trino                    | Distributed SQL query engine   |
| **Visualization** | Metabase                 | BI dashboards & analytics      |
| **ML**            | Scikit-learn, MLflow     | Model training & registry      |
| **API**           | FastAPI                  | Real-time prediction service   |
| **Orchestration** | Apache Airflow           | Workflow scheduling            |

## C·∫•u tr√∫c th∆∞ m·ª•c

```
real-time-fraud-detection-lakehouse/
‚îú‚îÄ‚îÄ airflow/dags/              # Airflow DAGs (model retraining, reports)
‚îú‚îÄ‚îÄ config/                    # Service configurations
‚îÇ   ‚îú‚îÄ‚îÄ metastore/             # Hive metastore config
‚îÇ   ‚îú‚îÄ‚îÄ spark/                 # Spark defaults
‚îÇ   ‚îî‚îÄ‚îÄ trino/                 # Trino settings
‚îú‚îÄ‚îÄ data/                      # Sparkov dataset (CSV files)
‚îú‚îÄ‚îÄ database/                  # PostgreSQL initialization
‚îÇ   ‚îî‚îÄ‚îÄ init_postgres.sql      # Schema setup (22 columns)
‚îú‚îÄ‚îÄ deployment/                # Infrastructure automation
‚îÇ   ‚îú‚îÄ‚îÄ debezium/              # CDC configuration scripts
‚îÇ   ‚îî‚îÄ‚îÄ minio/                 # MinIO bucket setup
‚îú‚îÄ‚îÄ docs/                      # Documentation
‚îÇ   ‚îî‚îÄ‚îÄ PROJECT_SPECIFICATION.md
‚îú‚îÄ‚îÄ notebooks/                 # Jupyter notebooks (EDA, experiments)
‚îú‚îÄ‚îÄ services/                  # Microservices
‚îÇ   ‚îú‚îÄ‚îÄ data-producer/         # PostgreSQL data simulator
‚îÇ   ‚îú‚îÄ‚îÄ fraud-detection-api/   # FastAPI prediction service
‚îÇ   ‚îî‚îÄ‚îÄ mlflow/                # MLflow tracking server
‚îú‚îÄ‚îÄ spark/                     # Custom Spark with ML libraries
‚îÇ   ‚îú‚îÄ‚îÄ app/                   # PySpark jobs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ streaming_job.py   # Bronze: Kafka CDC ‚Üí Delta Lake (continuous)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ silver_job.py      # Silver: Feature engineering (batch every 5 min)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ gold_job.py        # Gold: Star Schema (batch every 5 min)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ml_training_job.py # Model training pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ run_silver.sh      # Shell wrapper for silver batch
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ run_gold.sh        # Shell wrapper for gold batch
‚îÇ   ‚îî‚îÄ‚îÄ Dockerfile             # Spark + MLflow + ML libraries
‚îú‚îÄ‚îÄ sql/                       # SQL views for Gold layer
‚îÇ   ‚îî‚îÄ‚îÄ gold_layer_views.sql   # Materialized views for dashboards
‚îú‚îÄ‚îÄ docker-compose.yml         # 11 services orchestration
‚îî‚îÄ‚îÄ README.md
```

## Dataset

**Sparkov Credit Card Transactions Fraud Detection Dataset** ([Kaggle](https://www.kaggle.com/datasets/kartik2112/fraud-detection))

- `data/fraudTrain.csv` - 1,296,675 transactions (01/2019 - 12/2020)
- `data/fraudTest.csv` - 555,719 transactions
- **22 columns**: Geographic (lat/long), demographic (age, gender, job), transaction (amount, merchant, category)

### Schema ch√≠nh

| Column                    | Type     | Description                  |
| ------------------------- | -------- | ---------------------------- |
| `trans_date_trans_time`   | DateTime | Th·ªùi gian giao d·ªãch          |
| `cc_num`                  | Long     | S·ªë th·∫ª t√≠n d·ª•ng              |
| `merchant`                | String   | T√™n c·ª≠a h√†ng                 |
| `category`                | String   | Danh m·ª•c (grocery, gas, ...) |
| `amt`                     | Double   | S·ªë ti·ªÅn giao d·ªãch            |
| `gender`                  | String   | Gi·ªõi t√≠nh (M/F)              |
| `lat`, `long`             | Double   | V·ªã tr√≠ kh√°ch h√†ng            |
| `merch_lat`, `merch_long` | Double   | V·ªã tr√≠ c·ª≠a h√†ng              |
| `is_fraud`                | Integer  | Nh√£n gian l·∫≠n (0/1)          |

### Feature Engineering (40 features)

**Geographic** (2): `distance_km` (Haversine), `is_distant_transaction`  
**Demographic** (2): `age`, `gender_encoded`  
**Time** (6): `hour`, `day_of_week`, `is_weekend`, `is_late_night`, `hour_sin`, `hour_cos`  
**Amount** (4): `log_amount`, `amount_bin`, `is_zero_amount`, `is_high_amount`  
**Original** (26): All columns from Bronze layer preserved

## Ki·∫øn tr√∫c h·ªá th·ªëng

### Medallion Architecture (Hybrid: Streaming + Batch)

H·ªá th·ªëng s·ª≠ d·ª•ng **ki·∫øn tr√∫c lai** ƒë·ªÉ t·ªëi ∆∞u CPU v√† latency:

```
PostgreSQL (Source)
    ‚Üì Debezium CDC
Kafka (postgres.public.transactions)
    ‚Üì Bronze Streaming (Continuous, ~195% CPU)
Bronze Delta Lake (s3a://lakehouse/bronze/)
    ‚Üì Silver Batch (Every 5 minutes, 0% CPU during sleep)
Silver Delta Lake (s3a://lakehouse/silver/)
    ‚Üì Gold Batch (Every 5 minutes, 0% CPU during sleep)
Gold Delta Lake (s3a://lakehouse/gold/) - 5 tables
    ‚Üì Trino Delta Catalog (Direct access, no Hive Metastore)
Query Layer (Trino + Metabase)
```

**L·ª£i √≠ch:**

- ‚úÖ **Bronze Layer**: Real-time CDC capture t·ª´ Kafka (streaming li√™n t·ª•c)
- ‚úÖ **Silver Layer**: Feature engineering m·ªói 5 ph√∫t (batch) - gi·∫£m 60% CPU
- ‚úÖ **Gold Layer**: Star schema m·ªói 5 ph√∫t (batch) - data s·∫µn s√†ng cho analytics
- ‚úÖ **Latency**: 5-10 ph√∫t t·ª´ source ƒë·∫øn Gold (ch·∫•p nh·∫≠n ƒë∆∞·ª£c cho fraud detection analytics)
- ‚úÖ **Resource**: Bronze ~195% CPU, Silver/Gold 0% CPU khi sleep

### Delta Lake Integration

**Kh√¥ng s·ª≠ d·ª•ng Hive Metastore** - Delta Lake t·ª± qu·∫£n l√Ω metadata qua `_delta_log/`:

- ‚úÖ ACID transactions
- ‚úÖ Time travel (Delta Lake history)
- ‚úÖ Schema evolution v·ªõi `overwriteSchema=true`
- ‚úÖ Trino query tr·ª±c ti·∫øp qua Delta catalog

## H∆∞·ªõng d·∫´n ch·∫°y

### 1. Y√™u c·∫ßu h·ªá th·ªëng

- Docker & Docker Compose
- Python 3.9+
- 8GB RAM, 20GB disk space

---

### 2. Kh·ªüi ƒë·ªông h·ªá th·ªëng

```bash
# Clone repository
git clone https://github.com/bin-bard/real-time-fraud-detection-lakehouse.git
cd real-time-fraud-detection-lakehouse

# Kh·ªüi ƒë·ªông to√†n b·ªô h·ªá th·ªëng
docker-compose up -d
```

> **L∆∞u √Ω:** T·∫•t c·∫£ services t·ª± ƒë·ªông start, bao g·ªìm Bronze streaming v√† Silver/Gold batch jobs.

---

### 3. Ki·ªÉm tra Data Pipeline

Pipeline t·ª± ƒë·ªông ch·∫°y v·ªõi **4 services ch√≠nh**:

#### Ki·ªÉm tra logs

```bash
# Xem t·∫•t c·∫£ jobs
docker-compose logs -f bronze-streaming silver-job gold-job hive-registration

# Ho·∫∑c t·ª´ng job ri√™ng l·∫ª
docker logs -f bronze-streaming    # Bronze: CDC ‚Üí Delta Lake
docker logs -f silver-job          # Silver: Feature engineering (every 5 min)
docker logs -f gold-job            # Gold: Star schema (every 5 min)
docker logs -f hive-registration   # Hive: Auto-register tables (every 1 hour)
```

#### Verify th√†nh c√¥ng

**Bronze streaming** (continuous):

```
Writing batch 100 to Bronze layer...
Batch 100 written to Bronze successfully.
```

**Silver batch** (every 5 minutes):

```
ü•à Starting Bronze to Silver layer BATCH processing...
Found 86427 new records to process
‚úÖ Successfully processed 86427 records to Silver layer!
‚úÖ Silver batch completed. Sleeping 5 minutes...
```

**Gold batch** (every 5 minutes):

```
‚ú® Gold layer batch processing completed!
üìä Processed 86527 records from Silver layer
üìä Updated tables:
   - dim_customer -> s3a://lakehouse/gold/dim_customer
   - dim_merchant -> s3a://lakehouse/gold/dim_merchant
   - dim_time -> s3a://lakehouse/gold/dim_time
   - dim_location -> s3a://lakehouse/gold/dim_location
   - fact_transactions -> s3a://lakehouse/gold/fact_transactions
```

**Hive registration** (auto, every 1 hour):

```
üîß Hive Metastore Registration Service
‚è≥ Waiting for Gold layer to have data...
üöÄ Running Delta to Hive registration...
‚úÖ Registered bronze.transactions (1,296,675 records)
‚úÖ Registered silver.transactions (1,296,675 records)
‚úÖ Registered gold.dim_customer (997 records)
‚úÖ Registered gold.dim_merchant (693 records)
‚úÖ Registered gold.dim_time (17,520 records)
‚úÖ Registered gold.dim_location (956 records)
‚úÖ Registered gold.fact_transactions (1,296,675 records)
‚úÖ Registration completed successfully!
‚ôªÔ∏è  Entering maintenance loop (re-register every 1 hour)...
```

#### Ki·ªÉm tra CPU usage

```bash
docker stats --no-stream --format "table {{.Name}}\t{{.CPUPerc}}\t{{.MemUsage}}" | grep -E "bronze|silver|gold"
```

**Output mong ƒë·ª£i:**

```
bronze-streaming   195.96%   935.5MiB / 7.76GiB
silver-job         0.00%     219.5MiB / 7.76GiB
gold-job           0.00%     2.555MiB / 7.76GiB
```

‚úÖ **Bronze**: ~195% CPU (streaming li√™n t·ª•c)  
‚úÖ **Silver**: 0% CPU (ƒëang sleep 5 ph√∫t)  
‚úÖ **Gold**: 0% CPU (ƒëang sleep 5 ph√∫t)

---

### 4. C·∫•u tr√∫c Spark Jobs

#### Bronze Layer (`streaming_job.py`)

- **Mode**: Structured Streaming (continuous)
- **Input**: Kafka topic `postgres.public.transactions`
- **Processing**: Parse Debezium CDC format (`$.after.*`)
- **Output**: Delta Lake `s3a://lakehouse/bronze/transactions`
- **Partitioning**: By `year`, `month`, `day`

#### Silver Layer (`silver_job.py`)

- **Mode**: Batch (every 5 minutes)
- **Input**: Bronze Delta Lake
- **Processing**:
  - Data quality checks
  - Type casting (String ‚Üí Double/Long/Date)
  - Feature engineering (40 features)
  - Incremental processing (only new data)
- **Output**: Delta Lake `s3a://lakehouse/silver/transactions`
- **Config**: Ancient date support (`datetimeRebaseModeInWrite=LEGACY`)

#### Gold Layer (`gold_job.py`)

- **Mode**: Batch (every 5 minutes)
- **Input**: Silver Delta Lake
- **Processing**:
  - Star schema transformation
  - Hash-based surrogate keys
  - Incremental processing
- **Output**: 5 Delta tables
  - `dim_customer`: Customer dimension (cc_num, first, last, gender, dob, etc.)
  - `dim_merchant`: Merchant dimension (merchant, category, merch_lat, merch_long)
  - `dim_time`: Time dimension (date, hour, day_of_week, is_weekend)
  - `dim_location`: Location dimension (city, state, zip, lat, long)
  - `fact_transactions`: Fact table (foreign keys + measures)
  - `dim_merchant` - Dimension table (c·ª≠a h√†ng)
  - `dim_time` - Dimension table (th·ªùi gian)
  - `dim_location` - Dimension table (ƒë·ªãa ƒëi·ªÉm)
  - `fact_transactions` - Fact table (giao d·ªãch v·ªõi metrics)
- Checkpoint ƒë∆∞·ª£c l∆∞u t·∫°i `s3a://lakehouse/checkpoints/silver_to_gold/*`
- Trigger m·ªói 30 gi√¢y ƒë·ªÉ x·ª≠ l√Ω micro-batch
- **Kh√¥ng t·∫Øt terminal n√†y** - ƒë·ªÉ job ch·∫°y li√™n t·ª•c

---

**Lu·ªìng x·ª≠ l√Ω ho√†n ch·ªânh (End-to-End):**

```
PostgreSQL INSERT ‚Üí Debezium CDC ‚Üí Kafka
  ‚Üì (Bronze Streaming - Auto)
Bronze Layer (Delta Lake)
  ‚Üì (Silver Streaming - 30s trigger)
Silver Layer + Feature Engineering (15 features)
  ‚Üì (Gold Streaming - 30s trigger)
Gold Layer (Star Schema: 4 Dims + 1 Fact)
```

**∆Øu ƒëi·ªÉm ki·∫øn tr√∫c streaming:**

- ‚úÖ **Near Real-time**: ƒê·ªô tr·ªÖ ~30-60 gi√¢y t·ª´ INSERT ƒë·∫øn Gold
- ‚úÖ **T·ª± ƒë·ªông**: Kh√¥ng c·∫ßn trigger th·ªß c√¥ng
- ‚úÖ **Scalable**: X·ª≠ l√Ω ƒë∆∞·ª£c millions records/day
- ‚úÖ **Fault-tolerant**: Checkpoint ƒë·∫£m b·∫£o exactly-once processing

---

### 5. Query Data v·ªõi Trino

Trino c√≥ th·ªÉ query tr·ª±c ti·∫øp Delta Lake tables **kh√¥ng c·∫ßn Hive Metastore**:

```sql
-- Truy c·∫≠p Trino CLI
docker exec -it trino trino

-- List catalogs
SHOW CATALOGS;

-- Query Bronze layer
SELECT COUNT(*) FROM delta.default."s3a://lakehouse/bronze/transactions";

-- Query Silver layer (v·ªõi features)
SELECT trans_num, amt, distance_km, age, is_fraud
FROM delta.default."s3a://lakehouse/silver/transactions"
LIMIT 10;

-- Query Gold layer - Star Schema
SELECT
  f.transaction_amount,
  c.first_name || ' ' || c.last_name AS customer_name,
  m.merchant_name,
  t.hour,
  l.state
FROM delta.default."s3a://lakehouse/gold/fact_transactions" f
JOIN delta.default."s3a://lakehouse/gold/dim_customer" c ON f.customer_key = c.customer_key
JOIN delta.default."s3a://lakehouse/gold/dim_merchant" m ON f.merchant_key = m.merchant_key
JOIN delta.default."s3a://lakehouse/gold/dim_time" t ON f.time_key = t.time_key
JOIN delta.default."s3a://lakehouse/gold/dim_location" l ON f.location_key = l.location_key
WHERE f.is_fraud = 1
LIMIT 20;
```

---

### 6. Truy c·∫≠p Services

| Service             | URL                   | Username / Password                             | Ghi ch√∫                                           |
| ------------------- | --------------------- | ----------------------------------------------- | ------------------------------------------------- |
| **Airflow**         | http://localhost:8081 | `admin` / `admin`                               | Workflow orchestration & DAG management           |
| Spark Master UI     | http://localhost:8080 | Kh√¥ng c·∫ßn                                       | Monitoring Spark jobs                             |
| MinIO Console       | http://localhost:9001 | `minio` / `minio123`                            | Qu·∫£n l√Ω buckets v√† files (Data Lake)              |
| MLflow UI           | http://localhost:5000 | Kh√¥ng c·∫ßn                                       | ML model tracking & registry                      |
| Kafka UI            | http://localhost:9002 | Kh√¥ng c·∫ßn                                       | Xem topics, messages, consumer groups             |
| Trino UI            | http://localhost:8085 | Kh√¥ng c·∫ßn                                       | Query engine monitoring                           |
| Metabase            | http://localhost:3000 | T√πy ch·ªçn (v√≠ d·ª•:`admin@admin.com` / `admin123`) | BI Dashboard, t·ª± t·∫°o t√†i kho·∫£n admin l·∫ßn ƒë·∫ßu      |
| Fraud Detection API | http://localhost:8000 | Kh√¥ng c·∫ßn                                       | Real-time prediction endpoint                     |
| Kafka Broker        | localhost:9092        | Kh√¥ng c·∫ßn                                       | Kafka bootstrap server                            |
| PostgreSQL (Source) | localhost:5432        | `postgres` / `postgres`                         | Database `frauddb`                                |
| Metabase DB         | Internal              | `postgres` / `postgres`                         | Database `metabase` (kh√¥ng c·∫ßn truy c·∫≠p th·ªß c√¥ng) |
| Hive Metastore DB   | Internal (9083)       | `hive` / `hive`                                 | Postgres cho Hive (kh√¥ng expose ra ngo√†i)         |

> **L∆∞u √Ω quan tr·ªçng:**
>
> - **MinIO, PostgreSQL:** Credentials c·ªë ƒë·ªãnh trong `docker-compose.yml` (c√≥ th·ªÉ ƒë·ªïi tr∆∞·ªõc khi kh·ªüi ƒë·ªông).
> - **Metabase:** T·∫°o t√†i kho·∫£n admin khi truy c·∫≠p l·∫ßn ƒë·∫ßu, email/password t√πy ch·ªçn (v√≠ d·ª•: `admin@admin.com` / `admin123`).
> - **Airflow:** T√†i kho·∫£n m·∫∑c ƒë·ªãnh `admin/admin` (ƒë√£ ƒë∆∞·ª£c t·ª± ƒë·ªông t·∫°o khi kh·ªüi ƒë·ªông).
> - **Spark UI, Kafka UI, Trino UI:** Kh√¥ng y√™u c·∫ßu ƒëƒÉng nh·∫≠p.

---

### 7. Airflow Workflow Orchestration

H·ªá th·ªëng s·ª≠ d·ª•ng **Apache Airflow 2.8.0** ƒë·ªÉ qu·∫£n l√Ω c√°c workflow batch processing:

#### Truy c·∫≠p Airflow UI

```bash
# URL: http://localhost:8081
# Username: admin
# Password: admin
```

#### DAGs c√≥ s·∫µn

**1. `lakehouse_pipeline_taskflow`** - Lakehouse ETL Pipeline (TaskFlow API)

- **Schedule**: M·ªói 5 ph√∫t (`*/5 * * * *`)
- **Tasks**:
  1. `check_bronze_data` - Ki·ªÉm tra Bronze streaming ƒëang ch·∫°y
  2. `run_silver_transformation` - Ch·∫°y Silver job (Bronze ‚Üí Features)
  3. `run_gold_transformation` - Ch·∫°y Gold job (Silver ‚Üí Star Schema)
  4. `register_tables_to_hive` - ƒêƒÉng k√Ω Delta tables v√†o Hive Metastore
  5. `verify_trino_access` - Verify Trino c√≥ th·ªÉ query tables
  6. `send_pipeline_summary` - T·ªïng k·∫øt k·∫øt qu·∫£

**2. `model_retraining_taskflow`** - Automated ML Model Retraining (TaskFlow API)

- **Schedule**: H√†ng ng√†y l√∫c 02:00 (`0 2 * * *`)
- **Tasks**:
  1. `stop_streaming_jobs` - D·ª´ng Silver/Gold streaming ƒë·ªÉ gi·∫£i ph√≥ng CPU
  2. `verify_jobs_stopped` - Ki·ªÉm tra jobs ƒë√£ d·ª´ng ho√†n to√†n
  3. `check_data_availability` - Validate Silver layer c√≥ ƒë·ªß d·ªØ li·ªáu
  4. `train_ml_models` - Hu·∫•n luy·ªán RandomForest + LogisticRegression
  5. `verify_models_registered` - Check MLflow c√≥ 2 registered models
  6. `restart_streaming_jobs` - Kh·ªüi ƒë·ªông l·∫°i Silver/Gold streaming
  7. `send_notification` - Th√¥ng b√°o k·∫øt qu·∫£ training

#### Trigger DAG th·ªß c√¥ng

```bash
# Qua Airflow UI
# 1. Truy c·∫≠p http://localhost:8081
# 2. Click v√†o DAG name
# 3. Click n√∫t "Trigger DAG" (play button)

# Ho·∫∑c qua CLI
docker exec airflow-scheduler airflow dags trigger lakehouse_pipeline_taskflow
docker exec airflow-scheduler airflow dags trigger model_retraining_taskflow
```

#### Xem logs c·ªßa task

```bash
# Qua UI: DAG ‚Üí Run ‚Üí Task ‚Üí Logs

# Ho·∫∑c qua CLI
docker exec airflow-scheduler airflow tasks logs lakehouse_pipeline_taskflow run_silver_transformation <execution_date>
```

**L·ª£i √≠ch TaskFlow API:**

- Code g·ªçn g√†ng, d·ªÖ ƒë·ªçc h∆°n Operator-based
- T·ª± ƒë·ªông handle XCom gi·ªØa c√°c tasks
- Type hints support
- Retry t·ª± ƒë·ªông khi fail
- Log chi ti·∫øt t·ª´ng b∆∞·ªõc

---

### 5. Ki·ªÉm tra Pipeline

**Check Bronze layer data:**

```bash
# MinIO console: http://localhost:9001
# Navigate to: lakehouse/bronze/transactions/
```

**Check Kafka messages:**

```bash
docker exec -it kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic postgres.public.transactions \
  --from-beginning --max-messages 5
```

### X√°c minh CDC (INSERT/UPDATE/DELETE) v√† gi√° tr·ªã tr∆∞·ªùng `amt`

**1. L·∫•y trans_num th·ª±c t·∫ø ƒë·ªÉ test:**

```sql
SELECT trans_num FROM transactions LIMIT 5;
```

**2. Th·ª±c hi·ªán c√°c thao t√°c tr√™n PostgreSQL:**

```sql
-- UPDATE
UPDATE transactions SET amt = amt + 1 WHERE trans_num = '<trans_num th·ª±c t·∫ø>';
-- DELETE
DELETE FROM transactions WHERE trans_num = '<trans_num th·ª±c t·∫ø>';
```

**3. Ki·ªÉm tra message CDC tr√™n Kafka:**

```bash
docker exec kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic postgres.public.transactions --from-beginning --max-messages 100000 --timeout-ms 3000 2>$null | Select-String -Pattern "<trans_num th·ª±c t·∫ø>"
```

K·∫øt qu·∫£:

- `"op":"c"` = insert, `"op":"u"` = update, `"op":"d"` = delete.
- Tr∆∞·ªùng `amt` s·∫Ω ·ªü d·∫°ng m√£ h√≥a Base64 (v√≠ d·ª•: "amt":"Ark=").

**4. Decode gi√° tr·ªã amt (PowerShell):**

```powershell
[System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String("Ark="))
```

Vi·ªác decode n√†y ch·ªâ ƒë·ªÉ xem gi√° tr·ªã th·ª±c, kh√¥ng ·∫£nh h∆∞·ªüng pipeline.

**5. Xem Kafka messages qua UI:**

Truy c·∫≠p Kafka UI t·∫°i http://localhost:9002 ƒë·ªÉ xem topics, messages, consumer groups qua giao di·ªán web th√¢n thi·ªán.

**Monitor Spark jobs:**

---

### 7. Query Data v·ªõi Trino + Metabase

#### Ki·ªÉm tra Tables ƒë√£ ƒë∆∞·ª£c Register

H·ªá th·ªëng t·ª± ƒë·ªông register Delta Lake tables v√†o Hive Metastore m·ªói gi·ªù:

```bash
# Ki·ªÉm tra registration logs
docker logs hive-registration --tail 30

# Verify tables trong Trino
docker exec trino trino --server localhost:8081 --execute "SHOW TABLES FROM delta.gold"
```

**Output mong ƒë·ª£i:**

```
"dim_customer"
"dim_location"
"dim_merchant"
"dim_time"
"fact_transactions"
```

#### K·∫øt n·ªëi Metabase

1. **Truy c·∫≠p Metabase:** http://localhost:3000
2. **First-time setup:** T·∫°o t√†i kho·∫£n admin
3. **Add Database:**
   - Database type: **Trino**
   - Display name: `Fraud Detection Lakehouse`
   - Host: `trino`
   - Port: `8081`
   - Catalog: `delta`
   - Database: `gold`
   - Username/Password: (ƒë·ªÉ tr·ªëng)
4. **Save** ‚Üí Metabase s·∫Ω sync metadata

#### Sample Queries

```sql
-- Fraud rate by merchant category
SELECT
  dm.category,
  COUNT(*) as total_transactions,
  SUM(CASE WHEN ft.is_fraud = true THEN 1 ELSE 0 END) as fraud_count,
  ROUND(100.0 * SUM(CASE WHEN ft.is_fraud = true THEN 1 ELSE 0 END) / COUNT(*), 2) as fraud_rate
FROM delta.gold.fact_transactions ft
JOIN delta.gold.dim_merchant dm ON ft.merchant_key = dm.merchant_key
GROUP BY dm.category
ORDER BY fraud_rate DESC;

-- Geographic fraud distribution
SELECT
  dl.state,
  COUNT(*) as total_transactions,
  SUM(CASE WHEN ft.is_fraud = true THEN 1 ELSE 0 END) as fraud_count
FROM delta.gold.fact_transactions ft
JOIN delta.gold.dim_location dl ON ft.location_key = dl.location_key
GROUP BY dl.state
ORDER BY fraud_count DESC
LIMIT 10;
```

**üìñ Chi ti·∫øt:**

- **Setup guide:** [`docs/METABASE_SETUP.md`](docs/METABASE_SETUP.md) - Connection settings & 7 sample queries
- **Troubleshooting:** [`docs/TROUBLESHOOTING.md`](docs/TROUBLESHOOTING.md) - Gi·∫£i quy·∫øt c√°c v·∫•n ƒë·ªÅ th∆∞·ªùng g·∫∑p

---

### 8. Troubleshooting & Maintenance

#### Reset to√†n b·ªô h·ªá th·ªëng

```bash
docker-compose down -v
docker-compose up -d --build
```

#### Check logs khi c√≥ l·ªói

```bash
# Check Bronze streaming
docker logs bronze-streaming --tail 50

# Check Silver batch
docker logs silver-job --tail 50

# Check Gold batch
docker logs gold-job --tail 50

# Check Spark Master
docker logs spark-master
```

#### Common issues

**High CPU usage**:

- Bronze streaming: ~195% CPU (b√¨nh th∆∞·ªùng)
- Silver/Gold batch: 0% CPU khi sleep, spike khi ch·∫°y (b√¨nh th∆∞·ªùng)
- N·∫øu c·∫£ 3 jobs ƒë·ªÅu >200% CPU: Xem x√©t gi·∫£m batch size ho·∫∑c tƒÉng sleep interval

**Job fails to start**:

- Check Spark Master UI: http://localhost:8080
- Verify MinIO accessible: http://localhost:9001
- Check Kafka messages: `docker logs kafka`

---

### 8. Lakehouse Structure

```
s3a://lakehouse/
‚îú‚îÄ‚îÄ bronze/transactions/          # Raw CDC data (Debezium format parsed)
‚îÇ   ‚îî‚îÄ‚îÄ _delta_log/              # Delta Lake transaction logs
‚îú‚îÄ‚îÄ silver/transactions/          # 40 engineered features
‚îÇ   ‚îî‚îÄ‚îÄ _delta_log/
‚îú‚îÄ‚îÄ gold/                         # Star Schema (5 tables)
‚îÇ   ‚îú‚îÄ‚îÄ dim_customer/
‚îÇ   ‚îú‚îÄ‚îÄ dim_merchant/
‚îÇ   ‚îú‚îÄ‚îÄ dim_time/
‚îÇ   ‚îú‚îÄ‚îÄ dim_location/
‚îÇ   ‚îî‚îÄ‚îÄ fact_transactions/
‚îú‚îÄ‚îÄ checkpoints/                  # Spark streaming checkpoints
‚îÇ   ‚îú‚îÄ‚îÄ kafka_to_bronze/         # Bronze streaming state
‚îÇ   ‚îú‚îÄ‚îÄ bronze_to_silver_batch/  # Silver batch watermark
‚îÇ   ‚îî‚îÄ‚îÄ silver_to_gold_batch/    # Gold batch watermark
‚îî‚îÄ‚îÄ models/                       # ML models & artifacts (future)
```

---

### 9. Ki·∫øn tr√∫c Data Flow

**Lu·ªìng x·ª≠ l√Ω ho√†n ch·ªânh:**

```
PostgreSQL INSERT
    ‚Üì Debezium CDC (Change Data Capture)
Kafka Topic: postgres.public.transactions
    ‚Üì Bronze Streaming (Continuous, ~195% CPU)
Bronze Delta Lake (s3a://lakehouse/bronze/)
    ‚Üì Silver Batch (Every 5 minutes, spike to ~100% CPU then sleep)
Silver Delta Lake (40 features, s3a://lakehouse/silver/)
    ‚Üì Gold Batch (Every 5 minutes, spike to ~100% CPU then sleep)
Gold Delta Lake (5 tables, s3a://lakehouse/gold/)
    ‚Üì Hive Metastore (Auto-registration every 1 hour)
    ‚Üì Trino Query Engine (Delta Catalog via Hive Metastore)
Metabase Dashboard / Analytics
```

**Latency:**

- Bronze: Real-time (~1-2 seconds from PostgreSQL INSERT)
- Silver: 5-10 minutes (batch interval + processing time)
- Gold: 10-15 minutes (waits for Silver + processing time)

**Resource Usage:**

- Bronze: 195% CPU (continuous streaming)
- Silver: 0% CPU (95% of time), spike when processing
- Gold: 0% CPU (95% of time), spike when processing
- **Total**: ~195-400% CPU (depending on batch cycle)

---

### 10. Services Container Map

| Service             | URL                   | Credentials            | Purpose                                  |
| ------------------- | --------------------- | ---------------------- | ---------------------------------------- |
| Spark Master UI     | http://localhost:8080 | None                   | Monitor Spark jobs & resource allocation |
| MinIO Console       | http://localhost:9001 | minio / minio123       | S3-compatible Data Lake storage          |
| Trino UI            | http://localhost:8085 | None                   | Distributed SQL query engine             |
| Metabase            | http://localhost:3000 | (setup on first visit) | BI Dashboard & visualization             |
| MLflow UI           | http://localhost:5000 | None                   | ML model tracking & registry             |
| Kafka UI            | http://localhost:9002 | None                   | Kafka topics & messages monitoring       |
| Fraud Detection API | http://localhost:8000 | None                   | Real-time prediction endpoint (future)   |
| Kafka Broker        | localhost:9092        | None                   | Message streaming platform               |
| PostgreSQL          | localhost:5432        | postgres / postgres    | Source database (frauddb)                |
| Hive Metastore      | localhost:9083        | None (Thrift)          | Table metadata store for Trino           |

---

### 11. Key Features & Achievements

‚úÖ **Hybrid Architecture**: Streaming (Bronze) + Batch (Silver/Gold) for optimal CPU usage  
‚úÖ **Real-time CDC**: Debezium captures INSERT/UPDATE/DELETE from PostgreSQL  
‚úÖ **ACID Transactions**: Delta Lake ensures data consistency  
‚úÖ **Incremental Processing**: Only process new data (watermark-based)  
‚úÖ **Schema Evolution**: Support for ancient dates with LEGACY mode  
‚úÖ **40 Features**: Geographic, demographic, time-based, amount-based  
‚úÖ **Star Schema**: 4 dimensions + 1 fact table for analytics  
‚úÖ **Trino Query Engine**: Distributed SQL with Hive Metastore integration  
 **Auto Registration**: Tables auto-register to Metastore every hour  
 **Metabase Ready**: Pre-configured for BI dashboards and visualizations  
‚úÖ **60% CPU Reduction**: From 300%+ to ~195% by moving to batch processing

---

## Chi ti·∫øt k·ªπ thu·∫≠t

Xem file `docs/PROJECT_SPECIFICATION.md` ƒë·ªÉ hi·ªÉu r√µ:

- Ki·∫øn tr√∫c h·ªá th·ªëng chi ti·∫øt
- Y√™u c·∫ßu nghi·ªáp v·ª•
- Data flow v√† processing layers
- ML pipeline specifications

## Additional Documentation

- **[METABASE_SETUP.md](docs/METABASE_SETUP.md)** - Complete Metabase setup guide with 7 sample fraud detection queries
- **[TROUBLESHOOTING.md](docs/TROUBLESHOOTING.md)** - Detailed solutions for 6 major issues encountered during setup
- **[PROJECT_SPECIFICATION.md](docs/PROJECT_SPECIFICATION.md)** - Full architecture specifications and requirements
