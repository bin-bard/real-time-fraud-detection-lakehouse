# Há»‡ Thá»‘ng Data Lakehouse PhÃ¡t Hiá»‡n Gian Láº­n TÃ i ChÃ­nh Trong Thá»i Gian Thá»±c

Dá»± Ã¡n nÃ y lÃ  tiá»ƒu luáº­n chuyÃªn ngÃ nh, trÃ¬nh bÃ y viá»‡c thiáº¿t káº¿ vÃ  triá»ƒn khai má»™t há»‡ thá»‘ng Data Lakehouse toÃ n diá»‡n Ä‘á»ƒ phÃ¡t hiá»‡n vÃ  há»— trá»£ xÃ¡c minh cÃ¡c giao dá»‹ch gian láº­n tháº» tÃ­n dá»¥ng trong thá»i gian thá»±c.

![Architecture Diagram](docs/architecture.png)

## Má»¥c tiÃªu

XÃ¢y dá»±ng má»™t pipeline dá»¯ liá»‡u end-to-end, cÃ³ kháº£ nÄƒng:

1. **Thu tháº­p** luá»“ng dá»¯ liá»‡u giao dá»‹ch gáº§n nhÆ° tá»©c thá»i tá»« PostgreSQL qua Debezium CDC.
2. **Xá»­ lÃ½ vÃ  lÃ m giÃ u** dá»¯ liá»‡u trÃªn má»™t kiáº¿n trÃºc Lakehouse tin cáº­y vá»›i Delta Lake.
3. **Ãp dá»¥ng mÃ´ hÃ¬nh Machine Learning** Ä‘á»ƒ dá»± Ä‘oÃ¡n vÃ  gáº¯n cá» cÃ¡c giao dá»‹ch Ä‘Ã¡ng ngá» vá»›i Ä‘á»™ trá»… tháº¥p.
4. **Cung cáº¥p Dashboard** giÃ¡m sÃ¡t trá»±c quan cÃ¡c hoáº¡t Ä‘á»™ng gian láº­n (coming soon).
5. **Trang bá»‹ Chatbot thÃ´ng minh** cho phÃ©p cÃ¡c chuyÃªn viÃªn phÃ¢n tÃ­ch Ä‘iá»u tra vÃ  xÃ¡c minh cáº£nh bÃ¡o báº±ng ngÃ´n ngá»¯ tá»± nhiÃªn (coming soon).

## Kiáº¿n trÃºc vÃ  CÃ´ng nghá»‡ sá»­ dá»¥ng

Há»‡ thá»‘ng Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn kiáº¿n trÃºc Data Lakehouse vÃ  Ã¡p dá»¥ng mÃ´ hÃ¬nh xá»­ lÃ½ Medallion (Bronze, Silver, Gold). CÃ¡c cÃ´ng nghá»‡ Ä‘Æ°á»£c sá»­ dá»¥ng lÃ  cÃ¡c cÃ´ng cá»¥ mÃ£ nguá»“n má»Ÿ, máº¡nh máº½ vÃ  phá»• biáº¿n trong ngÃ nh dá»¯ liá»‡u lá»›n.

| Lá»›p (Layer)          | CÃ´ng nghá»‡                              | Vai trÃ² vÃ  Chá»©c nÄƒng                                                                                                                                                                                                  |
| :------------------- | :------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. Ingestion**     | **PostgreSQL, Debezium, Apache Kafka** | Giáº£ láº­p CSDL nguá»“n (PostgreSQL), sá»­ dá»¥ng Debezium Ä‘á»ƒ báº¯t cÃ¡c thay Ä‘á»•i (CDC) vÃ  Ä‘áº©y vÃ o Kafka dÆ°á»›i dáº¡ng luá»“ng sá»± kiá»‡n thá»i gian thá»±c.                                                                                  |
| **2. Storage**       | **MinIO, Delta Lake, Hive Metastore**  | Sá»­ dá»¥ng MinIO lÃ m Data Lake váº­t lÃ½, Delta Lake Ä‘á»ƒ quáº£n lÃ½ cÃ¡c báº£ng dá»¯ liá»‡u vá»›i tÃ­nh nÄƒng ACID vÃ  Time Travel, vÃ  Hive Metastore lÃ m catalog trung tÃ¢m.                                                                |
| **3. Processing**    | **Apache Spark, Trino**                | **Spark (Structured Streaming)** lÃ  engine chÃ­nh Ä‘á»ƒ xá»­ lÃ½ luá»“ng, lÃ m giÃ u dá»¯ liá»‡u vÃ  phÃ¡t hiá»‡n gian láº­n. **Trino** lÃ  engine truy váº¥n SQL tá»‘c Ä‘á»™ cao, phá»¥c vá»¥ cho nhu cáº§u truy váº¥n tÆ°Æ¡ng tÃ¡c tá»« Dashboard vÃ  Chatbot. |
| **4. ML & MLOps**    | **MLflow, FastAPI**                    | **MLflow** quáº£n lÃ½ toÃ n bá»™ vÃ²ng Ä‘á»i mÃ´ hÃ¬nh (huáº¥n luyá»‡n, lÆ°u trá»¯, Ä‘Äƒng kÃ½). MÃ´ hÃ¬nh tá»‘t nháº¥t Ä‘Æ°á»£c Ä‘Ã³ng gÃ³i vÃ  phá»¥c vá»¥ (serving) thÃ´ng qua má»™t **API service báº±ng FastAPI**.                                           |
| **5. Orchestration** | **Apache Airflow**                     | Äiá»u phá»‘i cÃ¡c pipeline xá»­ lÃ½ theo lÃ´ (batch), cháº³ng háº¡n nhÆ° tÃ¡c vá»¥ huáº¥n luyá»‡n láº¡i mÃ´ hÃ¬nh hÃ ng Ä‘Ãªm.                                                                                                                   |
| **6. Visualization** | **Metabase**                           | XÃ¢y dá»±ng Dashboard giÃ¡m sÃ¡t gian láº­n (Fraud Monitoring Dashboard) trá»±c quan, káº¿t ná»‘i vá»›i Trino Ä‘á»ƒ cÃ³ hiá»‡u nÄƒng cao.                                                                                                   |
| **7. Verification**  | **Streamlit, LangChain, OpenAI API**   | XÃ¢y dá»±ng á»©ng dá»¥ng**Chatbot "Trá»£ lÃ½ PhÃ¢n tÃ­ch Gian láº­n"**: Giao diá»‡n báº±ng **Streamlit**, logic xá»­ lÃ½ báº±ng **LangChain**, vÃ  kháº£ nÄƒng hiá»ƒu ngÃ´n ngá»¯ tá»± nhiÃªn tá»« **API cá»§a OpenAI**.                                     |

## Cáº¥u trÃºc thÆ° má»¥c

```text
real-time-fraud-detection-lakehouse/
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/ # DAGs Airflow (huáº¥n luyá»‡n láº¡i, bÃ¡o cÃ¡o)
â”‚   â””â”€â”€ plugins/ # Plugins má»Ÿ rá»™ng (náº¿u cáº§n)
â”œâ”€â”€ config/ # Cáº¤U HÃŒNH Táº¬P TRUNG
â”‚   â”œâ”€â”€ metastore/hive-site.xml # Káº¿t ná»‘i Hive Metastore (Postgres, driver, creds)
â”‚   â”œâ”€â”€ spark/spark-defaults.conf # Má»Ÿ rá»™ng Delta Lake, tinh chá»‰nh Spark
â”‚   â””â”€â”€ trino/config.properties # Cáº¥u hÃ¬nh Trino coordinator
â”œâ”€â”€ data/ # Dá»¯ liá»‡u nguá»“n/bá»™ máº«u Kaggle
â”œâ”€â”€ docs/ # TÃ i liá»‡u, sÆ¡ Ä‘á»“ kiáº¿n trÃºc
â”œâ”€â”€ notebooks/ # PhÃ¢n tÃ­ch & thá»­ nghiá»‡m mÃ´ hÃ¬nh
â”œâ”€â”€ scripts/ # Script tiá»‡n Ã­ch (khá»Ÿi táº¡o DB, dá»n dáº¹p)
â”‚   â”œâ”€â”€ init_postgres.sql
â”‚   â””â”€â”€ cleanup.sh
â”œâ”€â”€ services/ # CÃ¡c service (API, chatbot, producer)
â”‚   â”œâ”€â”€ fraud-detection-api/
â”‚   â”œâ”€â”€ chatbot-app/
â”‚   â””â”€â”€ data-producer/
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ app/ # Jobs Spark (streaming + batch)
â”‚   â”‚   â”œâ”€â”€ streaming_job.py
â”‚   â”‚   â””â”€â”€ batch_job.py
â”‚   â”œâ”€â”€ app/jars/ # JAR Delta Lake
â”‚   â”‚   â””â”€â”€ delta-core_2.12-x.x.x.jar
â”‚   â””â”€â”€ requirements.txt # Python cho Spark (pyspark, delta-spark)
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## Dá»¯ liá»‡u

Dá»± Ã¡n sá»­ dá»¥ng bá»™ dá»¯ liá»‡u cÃ´ng khai **Sparkov Credit Card Transactions Fraud Detection Dataset** tá»« Kaggle.

- **Nguá»“n:** [https://www.kaggle.com/datasets/kartik2112/fraud-detection](https://www.kaggle.com/datasets/kartik2112/fraud-detection)
- **Files:**
  - `data/fraudTrain.csv` - Training dataset (1,296,675 transactions)
  - `data/fraudTest.csv` - Test dataset (555,719 transactions)
- **Äáº·c Ä‘iá»ƒm:** Dá»¯ liá»‡u chá»©a cÃ¡c giao dá»‹ch tháº» tÃ­n dá»¥ng Ä‘Æ°á»£c táº¡o bá»Ÿi Sparkov Data Generation tá»« 01/01/2019 Ä‘áº¿n 31/12/2020. Bá»™ dá»¯ liá»‡u bao gá»“m thÃ´ng tin Ä‘á»‹a lÃ½ chi tiáº¿t (vÄ© Ä‘á»™/kinh Ä‘á»™ cá»§a khÃ¡ch hÃ ng vÃ  cá»­a hÃ ng), thÃ´ng tin nhÃ¢n kháº©u há»c (tuá»•i, giá»›i tÃ­nh, nghá» nghiá»‡p), vÃ  cÃ¡c Ä‘áº·c Ä‘iá»ƒm giao dá»‹ch thá»±c táº¿.

### Schema Dá»¯ Liá»‡u

| Cá»™t                              | Kiá»ƒu dá»¯ liá»‡u | MÃ´ táº£                                                |
| -------------------------------- | ------------ | ---------------------------------------------------- |
| `trans_date_trans_time`          | DateTime     | Thá»i gian giao dá»‹ch                                  |
| `cc_num`                         | Long         | Sá»‘ tháº» tÃ­n dá»¥ng                                      |
| `merchant`                       | String       | TÃªn cá»­a hÃ ng                                         |
| `category`                       | String       | Danh má»¥c sáº£n pháº©m (grocery_pos, gas_transport, etc.) |
| `amt`                            | Double       | Sá»‘ tiá»n giao dá»‹ch                                    |
| `first`, `last`                  | String       | Há» tÃªn khÃ¡ch hÃ ng                                    |
| `gender`                         | String       | Giá»›i tÃ­nh (M/F)                                      |
| `street`, `city`, `state`, `zip` | String/Int   | Äá»‹a chá»‰ khÃ¡ch hÃ ng                                   |
| `lat`, `long`                    | Double       | **Vá»‹ trÃ­ Ä‘á»‹a lÃ½ khÃ¡ch hÃ ng**                         |
| `city_pop`                       | Integer      | DÃ¢n sá»‘ thÃ nh phá»‘                                     |
| `job`                            | String       | Nghá» nghiá»‡p                                          |
| `dob`                            | Date         | NgÃ y sinh                                            |
| `trans_num`                      | String       | MÃ£ giao dá»‹ch (unique identifier)                     |
| `unix_time`                      | Long         | Unix timestamp                                       |
| `merch_lat`, `merch_long`        | Double       | **Vá»‹ trÃ­ Ä‘á»‹a lÃ½ cá»­a hÃ ng**                           |
| `is_fraud`                       | Integer      | NhÃ£n gian láº­n (0: Normal, 1: Fraud)                  |

### Features Engineering (Silver Layer)

Há»‡ thá»‘ng tá»± Ä‘á»™ng táº¡o **15 features** tá»« dá»¯ liá»‡u gá»‘c:

1. **Geographic Features:**

   - `distance_km`: Khoáº£ng cÃ¡ch Haversine giá»¯a khÃ¡ch hÃ ng vÃ  cá»­a hÃ ng
   - `is_distant_transaction`: Cá» giao dá»‹ch xa (>100km)

2. **Demographic Features:**

   - `age`: Tuá»•i tÃ­nh tá»« ngÃ y sinh
   - `gender_encoded`: MÃ£ hÃ³a giá»›i tÃ­nh (0/1)

3. **Time Features:**

   - `hour`, `day_of_week`, `is_weekend`
   - `hour_sin`, `hour_cos`: Cyclic encoding cho giá»
   - `is_late_night`: Cá» giao dá»‹ch Ä‘Ãªm khuya (11PM-5AM)

4. **Transaction Amount Features:**
   - `log_amount`: Log transformation cá»§a sá»‘ tiá»n
   - `amount_bin`: PhÃ¢n loáº¡i khoáº£ng tiá»n (0-5)
   - `is_zero_amount`, `is_high_amount`: Cá» sá»‘ tiá»n Ä‘áº·c biá»‡t

## HÆ°á»›ng dáº«n cÃ i Ä‘áº·t vÃ  cháº¡y dá»± Ã¡n

> **âš ï¸ QUAN TRá»ŒNG - ÄÃƒ Cáº¬P NHáº¬T LÃŠN SPARKOV DATASET (v2.0)**
>
> Dá»± Ã¡n Ä‘Ã£ Ä‘Æ°á»£c **hoÃ n toÃ n cáº­p nháº­t** Ä‘á»ƒ sá»­ dá»¥ng bá»™ dá»¯ liá»‡u **Sparkov Credit Card Transactions** thay vÃ¬ dataset PCA cÅ©.
>
> **Thay Ä‘á»•i chÃ­nh:**
>
> - âœ… Dataset: `fraudTrain.csv` & `fraudTest.csv` (thay vÃ¬ `creditcard.csv`)
> - âœ… Schema: 22 cá»™t vá»›i thÃ´ng tin Ä‘á»‹a lÃ½, nhÃ¢n kháº©u há»c Ä‘áº§y Ä‘á»§ (thay vÃ¬ V1-V28 PCA)
> - âœ… Feature Engineering: Distance (Haversine), Age, Time features (thay vÃ¬ PCA interactions)
> - âœ… Ingestion: PostgreSQL + Debezium CDC (thay vÃ¬ Kafka trá»±c tiáº¿p)
> - âœ… Bronze Layer: Raw Sparkov transactions
> - âœ… Silver Layer: 15 engineered features cho ML
> - âœ… Gold Layer: Geographic, category, state aggregations
> - âœ… ML Training: Random Forest vá»›i balanced sampling
> - âœ… FastAPI: Endpoints cho Sparkov features
>
> **Chi tiáº¿t:** Xem file `docs/PROJECT_SPECIFICATION.md` Ä‘á»ƒ hiá»ƒu rÃµ kiáº¿n trÃºc vÃ  yÃªu cáº§u.

### 1. YÃªu cáº§u há»‡ thá»‘ng

- **Docker & Docker Compose** (phiÃªn báº£n má»›i nháº¥t)
- **Python 3.9+** vá»›i pip
- **Git**
- Tá»‘i thiá»ƒu **8GB RAM** vÃ  **20GB dung lÆ°á»£ng trá»‘ng**

### 2. CÃ i Ä‘áº·t dá»± Ã¡n

```bash
# Clone repository
git clone https://github.com/bin-bard/real-time-fraud-detection-lakehouse.git
cd real-time-fraud-detection-lakehouse
```

### 3. Khá»Ÿi Ä‘á»™ng há»‡ thá»‘ng

#### BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng toÃ n bá»™ infrastructure

```bash
# Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
docker-compose up -d

# Kiá»ƒm tra status
docker ps
```

#### BÆ°á»›c 2: Khá»Ÿi táº¡o Data Lakehouse

```bash
# Setup MinIO buckets vÃ  folder structure
docker-compose --profile setup run --rm minio-setup
```

**Káº¿t quáº£ mong Ä‘á»£i:**

```
ğŸ”§ MinIO Data Lakehouse Setup Script
âœ… MinIO is ready!
âœ… Bucket 'lakehouse' created successfully.
ğŸ‰ MinIO setup completed successfully!
```

#### BÆ°á»›c 3: Kiá»ƒm tra cÃ¡c services

```bash
# Xem logs cÃ¡c services
docker logs kafka
docker logs data-producer
docker logs minio

# Kiá»ƒm tra táº¥t cáº£ containers
docker ps
```

### 4. Truy cáº­p cÃ¡c dá»‹ch vá»¥

| Service             | URL                     | Username | Password |
| ------------------- | ----------------------- | -------- | -------- |
| **Spark Master UI** | http://localhost:8080   | -        | -        |
| **MinIO Console**   | http://localhost:9001   | minio    | minio123 |
| **Kafka**           | localhost:9092          | -        | -        |
| **Hive Metastore**  | thrift://localhost:9083 | -        | -        |

### 5. Cháº¡y ML Pipeline vÃ  Feature Engineering

**âš ï¸ THá»¨ Tá»° QUAN TRá»ŒNG**: Pháº£i cháº¡y streaming pipeline trÆ°á»›c Ä‘á»ƒ cÃ³ dá»¯ liá»‡u Bronze layer!

#### BÆ°á»›c 1: Start Real-time Data Streaming (Báº®T BUá»˜C Äáº¦U TIÃŠN)

```bash
# 1.1. Start data producer Ä‘á»ƒ sinh fake transactions
docker-compose up -d data-producer

# 1.2. Start Spark streaming job Ä‘á»ƒ ghi vÃ o Bronze layer
docker exec -it spark-master bash -c "/opt/spark/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.4 --conf 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension' --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog' /app/streaming_job.py" &

# 1.3. Äá»ƒ streaming cháº¡y Ã­t nháº¥t 2-3 phÃºt Ä‘á»ƒ cÃ³ Ä‘á»§ data
# Báº¡n cÃ³ thá»ƒ Ctrl+C Ä‘á»ƒ dá»«ng streaming job khi Ä‘Ã£ cÃ³ Ä‘á»§ data
```

**ğŸ“Š Kiá»ƒm tra dá»¯ liá»‡u Bronze layer:**

```bash
# Check MinIO cÃ³ bronze data chÆ°a
docker exec -it minio mc alias set minio http://localhost:9000 minio minio123
docker exec -it minio mc ls minio/lakehouse/bronze/transactions/ --recursive
```

#### BÆ°á»›c 2: CÃ i Ä‘áº·t thÆ° viá»‡n ML cho Spark (Required)

**âš ï¸ Quan trá»ng**: Spark containers cáº§n cÃ i Ä‘áº·t thÃªm thÆ° viá»‡n ML Ä‘á»ƒ cháº¡y training pipeline:

```bash
# CÃ i Ä‘áº·t cho Spark Master (báº¯t buá»™c)
docker exec -it spark-master bash -c "pip install numpy pandas scikit-learn mlflow boto3 psycopg2-binary"

# CÃ i Ä‘áº·t cho Spark Worker (khuyáº¿n nghá»‹ cho distributed processing)
docker exec -it spark-worker bash -c "pip install numpy pandas scikit-learn mlflow boto3 psycopg2-binary"
```

**LÆ°u Ã½ vá» dependency conflicts:**

- Error `urllib3 2.2.3 incompatible` cÃ³ thá»ƒ xuáº¥t hiá»‡n nhÆ°ng khÃ´ng áº£nh hÆ°á»Ÿng chá»©c nÄƒng
- CÃ¡c thÆ° viá»‡n ML váº«n hoáº¡t Ä‘á»™ng bÃ¬nh thÆ°á»ng vá»›i warning nÃ y

#### BÆ°á»›c 3: Cháº¡y Silver Layer Processing (Feature Engineering)

```bash
# Cháº¡y Silver layer job Ä‘á»ƒ táº¡o features cho ML
docker exec -it spark-master bash -c "/opt/spark/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.4 --conf 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension' --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog' /app/silver_layer_job.py"
```

**Káº¿t quáº£ mong Ä‘á»£i:**

```
ğŸ¥ˆ Starting Bronze to Silver layer processing...
Reading from Bronze layer...
Bronze data count: 6621
Performing data quality checks...
Starting feature engineering...
Feature engineering completed. Total features: 42
Writing to Silver layer...
âœ… Silver layer processing completed successfully!
ğŸ“Š Silver Layer Statistics:
   Total transactions: 6610
   Normal transactions: 6558 (99.21%)
   Fraudulent transactions: 52 (0.79%)
```

#### BÆ°á»›c 4: Cháº¡y ML Training Pipeline

```bash
# Huáº¥n luyá»‡n models vá»›i Random Forest vÃ  Logistic Regression
docker exec -it spark-master bash -c "/opt/spark/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.4 --conf 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension' --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog' /app/ml_training_job.py"
```

**Káº¿t quáº£ mong Ä‘á»£i:**

```
ğŸ”„ Training random_forest model...
ğŸ“Š Model Performance:
   AUC: 0.9999
   Accuracy: 0.9976
   Precision: 0.9976
   Recall: 0.9976
   F1-Score: 0.9976
   Fraud Detection Rate: 0.8333

ğŸ”„ Training logistic_regression model...
ğŸ“Š Model Performance:
   AUC: 0.9993
   Accuracy: 0.9953
   Precision: 0.9950
   Recall: 0.9953
   F1-Score: 0.9951
   Fraud Detection Rate: 0.6667

ğŸ‰ All models training completed!
```

#### BÆ°á»›c 5: Kiá»ƒm tra ML Pipeline

**Kiá»ƒm tra Silver layer data:**

1. Truy cáº­p MinIO Console: http://localhost:9001
2. Browse `lakehouse/silver/transactions/` Ä‘á»ƒ xem transformed data
3. Verify 42 features Ä‘Ã£ Ä‘Æ°á»£c táº¡o

**Kiá»ƒm tra Gold layer data (optional):**

```bash
# Cháº¡y Gold layer aggregation
docker exec -it spark-master bash -c "/opt/spark/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.4 --conf 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension' --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog' /app/gold_layer_job.py"
```

### 6. Cháº¡y Spark Streaming Job

**LÆ°u Ã½**: Sá»­ dá»¥ng Spark 3.4.1 Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i Delta Lake 2.4.0

```bash
# Cháº¡y streaming job trá»±c tiáº¿p tá»« PowerShell/Terminal
docker exec -it spark-master bash -c "/opt/spark/bin/spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.4 --conf 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension' --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog' /app/streaming_job.py"
```

**Hoáº·c cÃ³ thá»ƒ vÃ o container Ä‘á»ƒ debug:**

```bash
# VÃ o Spark Master container
docker exec -it spark-master bash

# Cháº¡y streaming job vá»›i Delta Lake
/opt/spark/bin/spark-submit \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.4 \
    --conf 'spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension' \
    --conf 'spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog' \
    /app/streaming_job.py
```

**Káº¿t quáº£ mong Ä‘á»£i:**

```
âœ… Spark Session with Delta Lake created successfully.
Bronze layer streaming started. Writing to MinIO...
Writing batch 0 to Bronze layer...
Batch 0 written to Bronze successfully.
Writing batch 1 to Bronze layer...
Batch 1 written to Bronze successfully.
...
```

### 7. Kiá»ƒm tra dá»¯ liá»‡u

#### Kiá»ƒm tra Kafka data:

```bash
# VÃ o kafka container
docker exec -it kafka bash

# Xem data trong topic
kafka-console-consumer --bootstrap-server localhost:9092 --topic credit_card_transactions --from-beginning --max-messages 5
```

#### Kiá»ƒm tra Delta Lake data trong MinIO:

1. **Truy cáº­p MinIO Console**: http://localhost:9001

   - Username: `minio`
   - Password: `minio123`

2. **Browse bucket `lakehouse`**

3. **Kiá»ƒm tra cÃ¡c folder:**
   - `bronze/transactions/` - Raw transaction data tá»« Kafka
   - `checkpoints/bronze/` - Spark streaming checkpoints
   - CÃ¡c file Parquet Ä‘Æ°á»£c táº¡o theo partition `year/month/day`

#### Kiá»ƒm tra Spark Streaming Ä‘ang cháº¡y:

```bash
# Kiá»ƒm tra Spark UI
# Truy cáº­p: http://localhost:8080
# Xem Streaming tab Ä‘á»ƒ theo dÃµi job

# Hoáº·c kiá»ƒm tra logs
docker logs spark-master
```

### 8. Troubleshooting

#### Lá»—i thÆ°á»ng gáº·p:

**1. ML Library Dependency Conflicts:**

```bash
# Error: urllib3 2.2.3 incompatible
# Giáº£i phÃ¡p: Warning nÃ y khÃ´ng áº£nh hÆ°á»Ÿng chá»©c nÄƒng, cÃ³ thá»ƒ bá» qua
# Hoáº·c force reinstall specific versions:
docker exec -it spark-master bash -c "pip install urllib3==1.26.20 --force-reinstall"
```

**2. MLflow Connection Refused:**

```bash
# Náº¿u MLflow tracking server chÆ°a ready
docker-compose restart mlflow
docker logs mlflow  # Check logs

# MLflow Ä‘ang táº¡m thá»i disabled trong training code Ä‘á»ƒ test
# Sáº½ Ä‘Æ°á»£c enable sau khi fix network connectivity
```

**3. Hive Metastore schema error:**

```bash
# Reset volumes vÃ  restart
docker-compose down -v
docker-compose up -d
docker-compose --profile setup run --rm minio-setup
```

**4. Spark-Delta Lake compatibility error:**

- Äáº£m báº£o sá»­ dá»¥ng Spark 3.4.1 vá»›i Delta Lake 2.4.0
- Version packages trong spark-submit pháº£i match

**5. MinIO bucket not found:**

```bash
# Cháº¡y láº¡i setup
docker-compose --profile setup run --rm minio-setup
```

#### Monitoring vÃ  logs:

```bash
# Xem logs real-time
docker logs -f data-producer
docker logs -f spark-master
docker logs -f minio

# Restart specific service
docker-compose restart kafka
docker-compose restart spark-master

# Reset toÃ n bá»™ há»‡ thá»‘ng (xÃ³a dá»¯ liá»‡u)
docker-compose down -v
docker-compose up -d
docker-compose --profile setup run --rm minio-setup
```

### 9. Architecture Verification

Sau khi setup thÃ nh cÃ´ng, báº¡n sáº½ cÃ³:

1. **âœ… Data Ingestion**: Credit card transactions Ä‘Æ°á»£c stream tá»« CSV â†’ Kafka
2. **âœ… Data Lake**: MinIO vá»›i structure Bronze/Silver/Gold
3. **âœ… Stream Processing**: Spark Ä‘á»c tá»« Kafka vÃ  ghi vÃ o Delta Lake vá»›i ACID transactions
4. **âœ… Metadata Management**: Hive Metastore quáº£n lÃ½ table schemas
5. **âœ… Storage Format**: Delta Lake cung cáº¥p ACID transactions vÃ  Time Travel
6. **âœ… ML Pipeline**: Feature engineering (Silver) vÃ  model training vá»›i 99%+ accuracy
7. **âœ… MLflow Integration**: ML experiment tracking vÃ  model registry (coming soon)

**Kiá»ƒm tra hoáº¡t Ä‘á»™ng:**

- **Kafka Producer**: `docker logs data-producer` - data Ä‘Æ°á»£c publish liÃªn tá»¥c
- **Spark Streaming**: Batch processing messages hiá»ƒn thá»‹ "Batch X written to Bronze successfully"
- **MinIO Storage**: Parquet files xuáº¥t hiá»‡n trong `lakehouse/bronze/transactions/`
- **Delta Lake**: Transaction logs trong `_delta_log/` folder
- **Silver Layer**: 42 features Ä‘Æ°á»£c táº¡o cho fraud detection
- **ML Training**: Random Forest Ä‘áº¡t 99.99% AUC, 83.33% fraud detection rate

### 10. Tiáº¿p theo

Sau khi Data Lakehouse vÃ  ML Pipeline hoáº¡t Ä‘á»™ng á»•n Ä‘á»‹nh, cÃ¡c bÆ°á»›c phÃ¡t triá»ƒn tiáº¿p theo:

- âœ… **Machine Learning Pipeline**: HoÃ n thÃ nh vá»›i 99%+ accuracy fraud detection
- ğŸ”§ **MLflow Integration**: Setup tracking server vÃ  model registry
- ğŸ“Š **Analytics Dashboard**: Metabase cho real-time fraud monitoring
- ğŸ¤– **AI Chatbot**: LangChain + OpenAI Ä‘á»ƒ intelligent querying
- ğŸ”„ **Workflow Orchestration**: Airflow cho automated model retraining
- ğŸš€ **Model Serving**: FastAPI service cho real-time prediction
- ğŸ¯ **Real-time Scoring**: Integrate model vá»›i streaming pipeline

### 11. Cáº¥u trÃºc dá»¯ liá»‡u Lakehouse

```
s3a://lakehouse/
â”œâ”€â”€ bronze/           # Raw data tá»« Kafka
â”‚   â””â”€â”€ transactions/
â”‚       â””â”€â”€ year=2025/month=11/day=9/  # Partitioned by date
â”œâ”€â”€ silver/           # Cleaned & enriched data vá»›i 42 features
â”‚   â”œâ”€â”€ transactions/
â”‚   â””â”€â”€ features/     # ML-ready feature sets
â”œâ”€â”€ gold/             # Aggregated analytics data
â”‚   â”œâ”€â”€ aggregated/
â”‚   â””â”€â”€ reports/      # Fraud summary reports
â”œâ”€â”€ checkpoints/      # Spark streaming checkpoints
â”‚   â”œâ”€â”€ bronze/
â”‚   â”œâ”€â”€ silver/
â”‚   â””â”€â”€ gold/
â””â”€â”€ models/           # ML models vÃ  artifacts
    â”œâ”€â”€ fraud_detection/
    â””â”€â”€ experiments/
```

### 12. ML Pipeline Performance

**Feature Engineering (Silver Layer):**

- ğŸ”¢ **42 Features** Ä‘Æ°á»£c táº¡o tá»« raw transaction data
- ğŸ“Š **Statistical Features**: log_amount, amount_ranges, rolling averages
- â° **Time Features**: hour_sin, hour_cos, time-based patterns
- ğŸ”— **Interaction Features**: V1-V2 combinations, cross-features
- âœ… **Data Quality**: 6610 valid transactions, 99.21% normal, 0.79% fraud

**Model Performance Comparison:**

| Model                   | AUC    | Accuracy | Precision | Recall | F1-Score | Fraud Detection Rate |
| ----------------------- | ------ | -------- | --------- | ------ | -------- | -------------------- |
| **Random Forest**       | 99.99% | 99.76%   | 99.76%    | 99.76% | 99.76%   | **83.33%** â­        |
| **Logistic Regression** | 99.93% | 99.53%   | 99.50%    | 99.53% | 99.51%   | 66.67%               |

**ğŸ† Random Forest** Ä‘Æ°á»£c chá»n lÃ m model chÃ­nh vá»›i:

- Fraud detection rate cao nháº¥t: **83.33%**
- AUC gáº§n hoÃ n háº£o: **99.99%**
- Balanced performance across all metrics
- Suitable cho production fraud detection system

### 13. Production Readiness

**âœ… ÄÃ£ hoÃ n thÃ nh:**

- Data ingestion pipeline vá»›i Kafka
- Lakehouse architecture vá»›i Bronze/Silver/Gold layers
- Feature engineering vá»›i 42 fraud-specific features
- ML training pipeline vá»›i model comparison
- High-performance fraud detection (83.33% detection rate)

**ğŸ”§ Äang phÃ¡t triá»ƒn:**

- MLflow model registry vÃ  experiment tracking
- Real-time model serving vá»›i FastAPI
- Fraud monitoring dashboard vá»›i Metabase
- AI chatbot cho fraud investigation

**ğŸ“ˆ Metrics Ä‘á»ƒ monitor:**

- **Latency**: Streaming processing < 10s per batch
- **Accuracy**: Model performance > 99% AUC
- **Detection Rate**: Fraud catching rate > 80%
- **Throughput**: Process 1000+ transactions/minute

---

## ğŸ”§ Troubleshooting

### Common Issues

**1. MLflow Connection Issues:**

```bash
# Check MLflow service
docker logs realtime-fraud-detection-lakehouse-mlflow-1

# Restart MLflow service
docker-compose restart mlflow
```

**2. Dependency Conflicts:**

- `urllib3` version conflicts are **warnings only** - khÃ´ng áº£nh hÆ°á»Ÿng functionality
- Spark containers sá»­ dá»¥ng isolated environments
- Production deployment sáº½ cÃ³ fixed dependency versions

**3. Memory Issues:**

```bash
# Increase Docker memory limit
# Docker Desktop > Settings > Resources > Memory: 8GB+

# Monitor Spark resource usage
docker exec spark-master spark-submit --help
```

**4. Delta Lake Issues:**

```bash
# Clear Delta checkpoints náº¿u cÃ³ lá»—i
docker exec spark-master rm -rf /opt/spark/work-dir/checkpoints/*

# Restart streaming jobs
docker-compose restart spark-master spark-worker
```

---

## ğŸš€ Next Steps

### Phase 1: Model Deployment

- [ ] Fix MLflow connectivity cho model registry
- [ ] Deploy Random Forest model to production
- [ ] Create FastAPI endpoint cho real-time fraud scoring
- [ ] Implement model A/B testing framework

### Phase 2: Analytics Dashboard

- [ ] Test Gold layer aggregation pipeline
- [ ] Configure Metabase vá»›i Trino connection
- [ ] Create fraud monitoring dashboards
- [ ] Setup alerting cho high-risk transactions

### Phase 3: AI Chatbot

- [ ] Implement LangChain fraud investigation chatbot
- [ ] Connect vá»›i Trino query engine
- [ ] Deploy Streamlit interface
- [ ] Add natural language fraud pattern analysis

### Phase 4: Production Optimization

- [ ] Scale Kafka cluster cho high throughput
- [ ] Optimize Spark streaming performance
- [ ] Implement data quality monitoring
- [ ] Add comprehensive logging vÃ  monitoring

---

## ğŸ“Š Architecture Verification

**âœ… Verified Components:**

- âœ… Kafka: Streaming data ingestion
- âœ… Spark: Bronze/Silver layer processing
- âœ… Delta Lake: ACID transactions
- âœ… MinIO: S3-compatible storage
- âœ… PostgreSQL: Metadata storage
- âœ… ML Pipeline: 99%+ accuracy fraud detection

**ğŸ”§ In Progress:**

- ğŸ”„ MLflow: Model tracking (infrastructure ready)
- ğŸ”„ Gold Layer: Analytics aggregation (code ready)
- ğŸ”„ Trino: Query engine (configured)
- ğŸ”„ Metabase: Dashboard (waiting for data)

**ğŸ“… Roadmap:**

- Week 1: Complete MLflow integration + Gold layer testing
- Week 2: Deploy fraud detection API + dashboards
- Week 3: Implement AI chatbot + production optimization
- Week 4: Performance tuning + monitoring setup
