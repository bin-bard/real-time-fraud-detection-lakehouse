from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
from delta.tables import DeltaTable
import logging
import math

# Cáº¥u hÃ¬nh logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def haversine_distance(lat1, lon1, lat2, lon2):
    """
    TÃ­nh khoáº£ng cÃ¡ch Haversine giá»¯a 2 Ä‘iá»ƒm Ä‘á»‹a lÃ½ (km)
    Formula: https://en.wikipedia.org/wiki/Haversine_formula
    """
    from pyspark.sql.functions import sin, cos, sqrt, atan2, radians, lit
    
    # Earth radius in kilometers
    R = 6371.0
    
    # Convert to radians
    lat1_rad = radians(lat1)
    lon1_rad = radians(lon1)
    lat2_rad = radians(lat2)
    lon2_rad = radians(lon2)
    
    # Haversine formula
    dlat = lat2_rad - lat1_rad
    dlon = lon2_rad - lon1_rad
    
    a = sin(dlat/2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon/2)**2
    c = 2 * atan2(sqrt(a), sqrt(1-a))
    
    distance = R * c
    return distance

def create_spark_session():
    """Khá»Ÿi táº¡o Spark Session vá»›i Delta Lake"""
    return SparkSession.builder \
        .appName("SilverLayerProcessing") \
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
        .config("spark.hadoop.fs.s3a.access.key", "minio") \
        .config("spark.hadoop.fs.s3a.secret.key", "minio123") \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .getOrCreate()

def feature_engineering(df):
    """
    Táº¡o features cho fraud detection model tá»« Sparkov dataset
    Dataset columns: trans_date_trans_time, cc_num, merchant, category, amt, first, last, 
                     gender, street, city, state, zip, lat, long, city_pop, job, dob, 
                     trans_num, unix_time, merch_lat, merch_long, is_fraud
    """
    logger.info("Starting feature engineering...")
    
    # Cast amt from String to Double (Debezium encodes as string)
    # Fill NULL vá»›i 0.0 Ä‘á»ƒ trÃ¡nh lá»—i trong cÃ¡c phÃ©p tÃ­nh sau
    df = df.withColumn("amt", 
                       when(col("amt").isNull(), lit(0.0))
                       .otherwise(col("amt").cast("double")))
    
    # Parse dob: trong Bronze, dob lÃ  sá»‘ ngÃ y ká»ƒ tá»« epoch (integer)
    # Convert to date: epoch day 0 = 1970-01-01
    df = df.withColumn("dob_date", 
                       when(col("dob").isNotNull(), 
                            expr("date_add('1970-01-01', CAST(dob AS INT))"))
                       .otherwise(lit(None).cast("date")))
    
    # 1. GEOGRAPHIC FEATURES
    # Khoáº£ng cÃ¡ch Haversine giá»¯a customer location vÃ  merchant location
    # Null-safe: chá»‰ tÃ­nh khi cÃ³ Ä‘á»§ 4 tá»a Ä‘á»™, otherwise fill -1 Ä‘á»ƒ Ä‘Ã¡nh dáº¥u missing
    # LÃ½ do dÃ¹ng -1 thay vÃ¬ null: model cÃ³ thá»ƒ há»c pattern "khÃ´ng cÃ³ thÃ´ng tin vá»‹ trÃ­"
    df = df.withColumn("distance_km", 
                       when((col("lat").isNotNull()) & (col("long").isNotNull()) & 
                            (col("merch_lat").isNotNull()) & (col("merch_long").isNotNull()),
                            haversine_distance(col("lat"), col("long"), 
                                             col("merch_lat"), col("merch_long")))
                       .otherwise(lit(-1.0)))
    
    # 2. DEMOGRAPHIC FEATURES  
    # Tuá»•i khÃ¡ch hÃ ng
    # Null-safe: fill -1 náº¿u khÃ´ng cÃ³ dob (model há»c pattern "unknown age")
    df = df.withColumn("age", 
                       when((col("trans_timestamp").isNotNull()) & (col("dob_date").isNotNull()),
                            floor(datediff(col("trans_timestamp"), col("dob_date")) / 365.25))
                       .otherwise(lit(-1)))
    
    # 3. TIME FEATURES
    # Thá»i gian trong ngÃ y, ngÃ y trong tuáº§n
    # Xá»­ lÃ½ NULL cho trans_timestamp
    df = df.withColumn("hour", 
                       when(col("trans_timestamp").isNotNull(), hour(col("trans_timestamp")))
                       .otherwise(lit(0)))
    df = df.withColumn("day_of_week", 
                       when(col("trans_timestamp").isNotNull(), dayofweek(col("trans_timestamp")))
                       .otherwise(lit(1)))
    df = df.withColumn("is_weekend", 
                       when((col("day_of_week") == 1) | (col("day_of_week") == 7), 1).otherwise(0))
    
    # Cyclic encoding cho hour (Ä‘á»ƒ model hiá»ƒu 23h gáº§n 0h)
    df = df.withColumn("hour_sin", sin(col("hour") * 2 * 3.14159 / 24))
    df = df.withColumn("hour_cos", cos(col("hour") * 2 * 3.14159 / 24))
    
    # 4. TRANSACTION AMOUNT FEATURES
    # amt Ä‘Ã£ Ä‘Æ°á»£c Ä‘áº£m báº£o not null á»Ÿ trÃªn
    df = df.withColumn("log_amount", log(col("amt") + 1))
    df = df.withColumn("is_zero_amount", when(col("amt") == 0, 1).otherwise(0))
    df = df.withColumn("is_high_amount", when(col("amt") > 500, 1).otherwise(0))
    
    # Amount bins cho categorical analysis
    df = df.withColumn("amount_bin",
                       when(col("amt") == 0, 0)
                       .when(col("amt") <= 50, 1)
                       .when(col("amt") <= 100, 2)
                       .when(col("amt") <= 250, 3)
                       .when(col("amt") <= 500, 4)
                       .otherwise(5))
    
    # 5. CATEGORICAL ENCODING
    # Gender: M=1, F=0, null/other=0 (assume female as default)
    df = df.withColumn("gender_encoded", when(col("gender") == "M", 1).otherwise(0))
    
    # 6. RISK INDICATORS
    # Transaction xa (>100km cÃ³ thá»ƒ Ä‘Ã¡ng ngá»)
    # Null-safe: náº¿u distance_km = -1 (missing), khÃ´ng Ä‘Ã¡nh dáº¥u lÃ  distant
    df = df.withColumn("is_distant_transaction", 
                       when((col("distance_km") > 100) & (col("distance_km") >= 0), 1).otherwise(0))
    
    # Transaction Ä‘Ãªm khuya (11PM-5AM) - hour luÃ´n cÃ³ giÃ¡ trá»‹ (tá»« trans_timestamp)
    df = df.withColumn("is_late_night",
                       when((col("hour") >= 23) | (col("hour") <= 5), 1).otherwise(0))
    
    logger.info(f"After transformations count: {df.count()}")
    
    # Select ALL columns for Silver layer (original + engineered features)
    df_features = df.select(
        # Original identification from Kaggle dataset
        "trans_num", "cc_num", "trans_timestamp",
        
        # Transaction details
        "merchant", "category", "amt", "unix_time",
        
        # Customer info
        "first", "last", "gender", "street", "city", "state", "zip", "job", "dob",
        
        # Geographic data from dataset
        "lat", "long", "city_pop", "merch_lat", "merch_long",
        
        # Target variable
        "is_fraud",
        
        # ENGINEERED FEATURES (features we actually created above)
        # Geographic
        "distance_km", "is_distant_transaction",
        
        # Demographic  
        "age",
        
        # Time features
        "hour", "day_of_week", "is_weekend", "hour_sin", "hour_cos", "is_late_night",
        
        # Amount features
        "log_amount", "is_zero_amount", "is_high_amount", "amount_bin",
        
        # Categorical encoding
        "gender_encoded",
        
        # Metadata
        "ingestion_time",
        
        # Partitioning columns
        year(col("trans_timestamp")).alias("year"),
        month(col("trans_timestamp")).alias("month"),
        dayofmonth(col("trans_timestamp")).alias("day")
    )
    
    logger.info(f"After select count: {df_features.count()}")
    logger.info(f"Feature engineering completed. Total features: {len(df_features.columns)}")
    return df_features

def process_bronze_to_silver():
    """
    Xá»­ lÃ½ dá»¯ liá»‡u tá»« Bronze layer sang Silver layer
    """
    spark = create_spark_session()
    spark.sparkContext.setLogLevel("WARN")
    
    logger.info("ðŸ¥ˆ Starting Bronze to Silver layer processing...")
    
    # ÄÆ°á»ng dáº«n
    bronze_path = "s3a://lakehouse/bronze/transactions"
    silver_path = "s3a://lakehouse/silver/transactions"
    
    try:
        # Äá»c dá»¯ liá»‡u tá»« Bronze layer
        logger.info("Reading from Bronze layer...")
        bronze_df = spark.read.format("delta").load(bronze_path)
        
        logger.info(f"Bronze data count: {bronze_df.count()}")
        
        # Data quality checks
        logger.info("Performing data quality checks...")
        
        # 1. Loáº¡i bá» cÃ¡c records khÃ´ng thá»ƒ trace (trans_num hoáº·c cc_num null)
        # Theo spec: trans_num lÃ  mÃ£ giao dá»‹ch, cc_num lÃ  ID khÃ¡ch hÃ ng - cáº£ 2 Ä‘á»u critical
        bronze_df = bronze_df.filter(
            col("trans_num").isNotNull() & 
            col("cc_num").isNotNull() &
            col("trans_timestamp").isNotNull()  # Partition key cÅ©ng cáº§n cÃ³
        )
        logger.info(f"After filtering null critical fields: {bronze_df.count()} records")
        
        # 2. Loáº¡i bá» duplicates based on trans_num
        bronze_df = bronze_df.dropDuplicates(["trans_num"])
        logger.info(f"After deduplication: {bronze_df.count()} records")
        
        # 3. Fill null cho cÃ¡c cá»™t quan trá»ng nhÆ°ng cÃ³ thá»ƒ thiáº¿u
        # amt: sá»‘ tiá»n giao dá»‹ch - fill 0 náº¿u null (giao dá»‹ch khÃ´ng há»£p lá»‡ nhÆ°ng váº«n ghi nháº­n)
        bronze_df = bronze_df.withColumn("amt", coalesce(col("amt"), lit("0")))
        
        # is_fraud: label - fill 0 náº¿u null (assume normal náº¿u khÃ´ng cÃ³ label)
        bronze_df = bronze_df.withColumn("is_fraud", coalesce(col("is_fraud"), lit("0")))
        
        # lat, long, merch_lat, merch_long: vá»‹ trÃ­ - giá»¯ null, sáº½ xá»­ lÃ½ trong feature engineering
        # LÃ½ do: null á»Ÿ Ä‘Ã¢y cÃ³ Ã½ nghÄ©a (khÃ´ng cÃ³ thÃ´ng tin vá»‹ trÃ­) vs fillna sai thÃ´ng tin
        
        # 4. Feature engineering vá»›i null-safe logic
        silver_df = feature_engineering(bronze_df)
        
        # Ghi vÃ o Silver layer
        logger.info("Writing to Silver layer...")
        
        # Debug: count before write
        record_count = silver_df.count()
        logger.info(f"Records to write: {record_count}")
        
        if record_count == 0:
            logger.error("âŒ No records to write to Silver layer!")
            return False
        
        silver_df.write \
            .format("delta") \
            .mode("overwrite") \
            .partitionBy("year", "month", "day") \
            .option("overwriteSchema", "true") \
            .option("mergeSchema", "true") \
            .save(silver_path)
            
        logger.info("âœ… Silver layer processing completed successfully!")
        logger.info(f"ðŸ“Š Total records written: {record_count}")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Error in Silver layer processing: {str(e)}")
        return False
    finally:
        spark.stop()

if __name__ == "__main__":
    success = process_bronze_to_silver()
    if success:
        print("ðŸŽ‰ Silver layer processing completed successfully!")
    else:
        print("âŒ Silver layer processing failed!")
        exit(1)