# H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t - Setup Guide

H∆∞·ªõng d·∫´n c√†i ƒë·∫∑t h·ªá th·ªëng Real-Time Fraud Detection Lakehouse t·ª´ ƒë·∫ßu.

---

## M·ª•c l·ª•c

1. [Y√™u c·∫ßu h·ªá th·ªëng](#y√™u-c·∫ßu-h·ªá-th·ªëng)
2. [Quick Start (3 b∆∞·ªõc)](#quick-start)
3. [C√†i ƒë·∫∑t chi ti·∫øt](#c√†i-ƒë·∫∑t-chi-ti·∫øt)
4. [C·∫•u h√¨nh Gemini API](#c·∫•u-h√¨nh-gemini-api)
5. [Real-time Detection Setup](#real-time-detection-setup)
6. [Dashboard Access](#dashboard-access)
7. [Verification & Testing](#verification--testing)
8. [Troubleshooting](#troubleshooting)

---

## Y√™u c·∫ßu h·ªá th·ªëng

### Ph·∫ßn c·ª©ng

| Th√†nh ph·∫ßn  | T·ªëi thi·ªÉu       | Khuy·∫øn ngh·ªã | Ghi ch√∫                        |
| ----------- | --------------- | ----------- | ------------------------------ |
| **CPU**     | 6 cores         | 8+ cores    | Spark + Airflow c·∫ßn multi-core |
| **RAM**     | 10 GB           | 16 GB       | Spark executors chi·∫øm 4-6GB    |
| **Disk**    | 30 GB free      | 50 GB free  | Delta Lake + Docker images     |
| **Network** | Stable Internet | High-speed  | Download Docker images (~10GB) |

### Ph·∫ßn m·ªÅm

- **Docker**: Version 24.0+ (B·∫Øt bu·ªôc)
- **Docker Compose**: Version 2.20+ (B·∫Øt bu·ªôc)
- **Git**: Version 2.x+ (B·∫Øt bu·ªôc)
- **PowerShell** (Windows) ho·∫∑c **Bash** (Linux/Mac)
- **Gemini API Key** (Mi·ªÖn ph√≠ - cho Chatbot)
- **Slack Webhook URL** (T√πy ch·ªçn - cho Real-time Alerts)

---

## Quick Start

### 3 b∆∞·ªõc kh·ªüi ƒë·ªông h·ªá th·ªëng

**B∆∞·ªõc 1: Clone v√† c·∫•u h√¨nh**

```bash
# Clone repository
git clone https://github.com/bin-bard/real-time-fraud-detection-lakehouse.git
cd real-time-fraud-detection-lakehouse

# T·∫°o file .env t·ª´ template
cp .env.example .env

# Ch·ªânh s·ª≠a .env v√† th√™m Gemini API key
# L·∫•y key mi·ªÖn ph√≠ t·∫°i: https://aistudio.google.com/app/apikey
notepad .env  # Windows
nano .env     # Linux/Mac
```

**B∆∞·ªõc 2: Kh·ªüi ƒë·ªông services**

```bash
# Kh·ªüi ƒë·ªông t·∫•t c·∫£ 16 services
docker-compose up -d

# ƒê·ª£i 3-5 ph√∫t ƒë·ªÉ c√°c services kh·ªüi ƒë·ªông ho√†n to√†n
# PostgreSQL s·∫Ω t·ª± ƒë·ªông t·∫°o database schema (init_postgres.sql)
```

**B∆∞·ªõc 3: Load d·ªØ li·ªáu & Train model**

```bash
# Option A: Bulk load 50K transactions (Nhanh - 10 gi√¢y)
docker exec postgres psql -U postgres -d frauddb -c "\COPY transactions(trans_date_trans_time, cc_num, merchant, category, amt, first, last, gender, street, city, state, zip, lat, long, city_pop, job, dob, trans_num, unix_time, merch_lat, merch_long, is_fraud) FROM '/data/fraudTrain.csv' WITH (FORMAT csv, HEADER true, DELIMITER ',') LIMIT 50000;"

# Trigger ML training (ho·∫∑c ƒë·ª£i t·ª± ƒë·ªông ch·∫°y v√†o 2h s√°ng)
docker exec airflow-scheduler airflow dags trigger model_retraining_taskflow
```

**Ho√†n t·∫•t!** Truy c·∫≠p:

- Chatbot: http://localhost:8501
- Airflow: http://localhost:8081 (admin/admin)
- MLflow: http://localhost:5001

---

## C√†i ƒë·∫∑t chi ti·∫øt

### 1. Clone repository

```bash
git clone https://github.com/bin-bard/real-time-fraud-detection-lakehouse.git
cd real-time-fraud-detection-lakehouse
```

### 2. C·∫•u h√¨nh file .env

H·ªá th·ªëng s·ª≠ d·ª•ng **1 file `.env` duy nh·∫•t** t·∫°i root folder.

```bash
# T·∫°o t·ª´ template
cp .env.example .env
```

**N·ªôi dung file .env c·∫ßn thi·∫øt:**

```bash
# ============ GEMINI API (B·∫Øt bu·ªôc cho Chatbot) ============
GEMINI_API_KEY=your_gemini_api_key_here

# ============ PostgreSQL ============
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
POSTGRES_DB=frauddb
POSTGRES_USER=postgres
POSTGRES_PASSWORD=postgres123

# ============ MinIO (Object Storage) ============
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin
MINIO_ENDPOINT=http://minio:9000
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin

# ============ Kafka ============
KAFKA_BOOTSTRAP_SERVERS=kafka:9092

# ============ Trino ============
TRINO_HOST=trino
TRINO_PORT=8085

# ============ MLflow ============
MLFLOW_TRACKING_URI=http://mlflow:5000
MODEL_STAGE=Production

# ============ Slack Alerts (T√πy ch·ªçn) ============
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL

# ============ API ============
API_HOST=fraud-detection-api
API_PORT=8000
```

**L∆∞u √Ω:**

- `GEMINI_API_KEY`: **B·∫Øt bu·ªôc** ƒë·ªÉ Chatbot ho·∫°t ƒë·ªông
- `SLACK_WEBHOOK_URL`: T√πy ch·ªçn, n·∫øu kh√¥ng c√≥ th√¨ b·ªè tr·ªëng (Real-time alerts s·∫Ω kh√¥ng g·ª≠i Slack)
- C√°c bi·∫øn kh√°c gi·ªØ nguy√™n gi√° tr·ªã m·∫∑c ƒë·ªãnh

---

## C·∫•u h√¨nh Gemini API

### L·∫•y API Key mi·ªÖn ph√≠

1. Truy c·∫≠p: https://aistudio.google.com/app/apikey
2. ƒêƒÉng nh·∫≠p b·∫±ng Google Account
3. Click **"Create API Key"**
4. Ch·ªçn project (ho·∫∑c t·∫°o m·ªõi)
5. Copy API key (d·∫°ng: `AIzaSy...`)

### Paste v√†o file .env

```bash
GEMINI_API_KEY=AIzaSyXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX
```

### Test API Key

Sau khi kh·ªüi ƒë·ªông Chatbot, ki·ªÉm tra t·∫°i sidebar:

- ‚úÖ **Gemini API Status**: Connected
- N·∫øu l·ªói: Ki·ªÉm tra l·∫°i API key ho·∫∑c network

---

## 3. Kh·ªüi ƒë·ªông services

### Option 1: Kh·ªüi ƒë·ªông to√†n b·ªô (Khuy·∫øn ngh·ªã)

```bash
docker-compose up -d
```

**16 services s·∫Ω ƒë∆∞·ª£c kh·ªüi ƒë·ªông:**

1. **postgres** - OLTP database (5432)
2. **zookeeper** - Kafka coordination
3. **kafka** - Message broker (9092)
4. **debezium-connect** - CDC connector (8083)
5. **minio** - Object storage (9000, 9001)
6. **hive-metastore** - Metadata cache (9083) [Optional]
7. **spark-streaming** - Bronze layer streaming
8. **spark-silver** - Silver layer batch job
9. **spark-gold** - Gold layer batch job
10. **spark-realtime-prediction** - Real-time alert service
11. **trino** - Query engine (8085)
12. **mlflow** - ML tracking (5001)
13. **fraud-detection-api** - Prediction API (8000)
14. **fraud-chatbot** - Streamlit chatbot (8501)
15. **airflow-scheduler** - Workflow orchestration
16. **airflow-webserver** - Airflow UI (8081)

### Option 2: Kh·ªüi ƒë·ªông t·ª´ng nh√≥m

**A. Core services (Database + Storage)**

```bash
docker-compose up -d postgres minio kafka zookeeper debezium-connect
```

**B. Processing layer**

```bash
docker-compose up -d spark-streaming spark-silver spark-gold
```

**C. ML & API**

```bash
docker-compose up -d mlflow fraud-detection-api
```

**D. Chatbot only**

```bash
docker-compose up -d fraud-chatbot
```

### Th·ªùi gian kh·ªüi ƒë·ªông

| Service           | Th·ªùi gian | Ghi ch√∫                          |
| ----------------- | --------- | -------------------------------- |
| PostgreSQL        | 5-10s     | T·ª± ƒë·ªông ch·∫°y `init_postgres.sql` |
| Kafka + Zookeeper | 15-20s    |                                  |
| Debezium          | 30-40s    | T·ª± ƒë·ªông t·∫°o CDC connector        |
| MinIO             | 5s        | T·ª± ƒë·ªông t·∫°o bucket `lakehouse`   |
| Spark services    | 20-30s    |                                  |
| Airflow           | 60-90s    | Init database + DAGs             |
| API + Chatbot     | 10-15s    | Load ML model                    |

**T·ªïng th·ªùi gian**: 3-5 ph√∫t cho to√†n b·ªô h·ªá th·ªëng.

---

## 4. Verify services

### Ki·ªÉm tra tr·∫°ng th√°i containers

```bash
docker-compose ps
```

**Expected output:**

```
NAME                        STATE       PORTS
postgres                    Up          0.0.0.0:5432->5432/tcp
kafka                       Up          0.0.0.0:9092->9092/tcp
debezium-connect            Up          0.0.0.0:8083->8083/tcp
minio                       Up          0.0.0.0:9000-9001->9000-9001/tcp
trino                       Up          0.0.0.0:8085->8085/tcp
fraud-detection-api         Up          0.0.0.0:8000->8000/tcp
fraud-chatbot               Up          0.0.0.0:8501->8501/tcp
airflow-webserver           Up          0.0.0.0:8081->8081/tcp
mlflow                      Up          0.0.0.0:5001->5000/tcp
...
```

### Health checks

**PostgreSQL database schema:**

```bash
docker exec postgres psql -U postgres -d frauddb -c "\dt"
```

Expected tables:

- `transactions` (Main OLTP table)
- `fraud_predictions` (ML prediction results)
- `producer_checkpoint` (Streaming offset tracking)
- `chat_history` (Chatbot conversation history)

**Kafka topics:**

```bash
docker exec kafka kafka-topics.sh --bootstrap-server localhost:9092 --list
```

Expected: `postgres.public.transactions` (CDC topic)

**Debezium connector:**

```bash
curl http://localhost:8083/connectors/postgres-connector/status
```

Expected: `"state": "RUNNING"`

**API health:**

```bash
curl http://localhost:8000/health
```

Expected: `{"status": "ok", "model_loaded": true}`

**MinIO buckets:**

```bash
docker exec minio mc ls minio/lakehouse/
```

Expected: `bronze/`, `silver/`, `gold/`, `checkpoints/`

---

## 5. Load d·ªØ li·ªáu

C√≥ **3 c√°ch** ƒë·ªÉ load d·ªØ li·ªáu v√†o h·ªá th·ªëng:

### Option A: Bulk Load (Khuy·∫øn ngh·ªã - Nhanh nh·∫•t)

Load 50,000 transactions trong ~10 gi√¢y:

```bash
docker exec postgres psql -U postgres -d frauddb -c "\COPY transactions(trans_date_trans_time, cc_num, merchant, category, amt, first, last, gender, street, city, state, zip, lat, long, city_pop, job, dob, trans_num, unix_time, merch_lat, merch_long, is_fraud) FROM '/data/fraudTrain.csv' WITH (FORMAT csv, HEADER true, DELIMITER ',') LIMIT 50000;"
```

**L∆∞u √Ω:**

- Transactions ƒë∆∞·ª£c INSERT tr·ª±c ti·∫øp v√†o PostgreSQL
- Debezium CDC s·∫Ω t·ª± ƒë·ªông capture v√† g·ª≠i v√†o Kafka
- Spark streaming s·∫Ω ghi v√†o Bronze layer

### Option B: Streaming Load (M√¥ ph·ªèng real-time)

Ch·∫°y data producer ƒë·ªÉ stream t·ª´ng transaction:

```bash
# Ch·∫°y producer v·ªõi t·ªëc ƒë·ªô 10 tx/gi√¢y
docker-compose up -d data-producer
```

**Producer configuration:**

- File: `services/data-producer/producer.py`
- T·ªëc ƒë·ªô m·∫∑c ƒë·ªãnh: 10 transactions/gi√¢y
- Checkpoint: T·ª± ƒë·ªông l∆∞u offset, c√≥ th·ªÉ resume

**Monitor producer:**

```bash
docker logs data-producer --tail 50 -f
```

**D·ª´ng producer:**

```bash
docker-compose stop data-producer
```

### Option C: Auto-load (Load to√†n b·ªô dataset)

Load t·∫•t c·∫£ 1.2M transactions (ch·∫≠m - ~20 ph√∫t):

```bash
docker exec postgres psql -U postgres -d frauddb -c "\COPY transactions(trans_date_trans_time, cc_num, merchant, category, amt, first, last, gender, street, city, state, zip, lat, long, city_pop, job, dob, trans_num, unix_time, merch_lat, merch_long, is_fraud) FROM '/data/fraudTrain.csv' WITH (FORMAT csv, HEADER true, DELIMITER ',');"
```

**C·∫£nh b√°o**: Load to√†n b·ªô s·∫Ω t·∫°o h√†ng tri·ªáu CDC events, c√≥ th·ªÉ l√†m ch·∫≠m h·ªá th·ªëng.

---

## 6. Train ML Model

C√≥ **2 c√°ch** ƒë·ªÉ train model:

### Option A: Trigger manual (Ngay l·∫≠p t·ª©c)

```bash
# Trigger Airflow DAG
docker exec airflow-scheduler airflow dags trigger model_retraining_taskflow

# Monitor DAG run
docker exec airflow-scheduler airflow dags list-runs -d model_retraining_taskflow
```

**Th·ªùi gian training**: 5-10 ph√∫t (t√πy s·ªë l∆∞·ª£ng transactions)

### Option B: Automatic (ƒê·ª£i schedule)

Airflow DAG `model_retraining_taskflow` t·ª± ƒë·ªông ch·∫°y **h√†ng ng√†y v√†o 2h s√°ng**.

**Schedule:**

```python
schedule_interval="0 2 * * *"  # Cron: 2:00 AM daily
```

**Kh√¥ng c·∫ßn l√†m g√¨**, model s·∫Ω t·ª± ƒë·ªông:

1. Extract features t·ª´ Silver layer
2. Train RandomForest + LogisticRegression
3. Evaluate metrics (Accuracy, AUC, Precision, Recall)
4. Register model v√†o MLflow
5. Promote to "Production" stage
6. Reload model trong FastAPI

---

## Real-time Detection Setup

### Kh·ªüi ƒë·ªông Real-time Alert Service

```bash
# Start Spark Streaming alert service
docker-compose up -d spark-realtime-prediction
```

### Lu·ªìng x·ª≠ l√Ω Real-time

```
Transaction INSERT ‚Üí PostgreSQL
    ‚Üì Debezium CDC
Kafka Topic: postgres.public.transactions
    ‚Üì Spark Streaming (10-second micro-batch)
Read CDC event ‚Üí Call FastAPI /predict/raw
    ‚Üì ML Prediction
Save to fraud_predictions table
    ‚Üì If is_fraud = 1
Send Slack Alert (ALL risk levels: LOW/MEDIUM/HIGH)
```

### C·∫•u h√¨nh Slack Webhook

**1. T·∫°o Slack Incoming Webhook:**

- Truy c·∫≠p: https://api.slack.com/apps
- Ch·ªçn app (ho·∫∑c t·∫°o m·ªõi)
- "Incoming Webhooks" ‚Üí "Add New Webhook to Workspace"
- Ch·ªçn channel ƒë·ªÉ nh·∫≠n alerts
- Copy Webhook URL

**2. C·∫≠p nh·∫≠t .env:**

```bash
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR_WORKSPACE_ID/YOUR_CHANNEL_ID/YOUR_TOKEN
```

**3. Rebuild service:**

```bash
docker-compose up -d --build spark-realtime-prediction
```

### Test Real-time Flow

**Ch√®n transaction th·ªß c√¥ng ƒë·ªÉ test:**

```bash
docker exec postgres psql -U postgres -d frauddb -c "INSERT INTO transactions (trans_date_trans_time, cc_num, merchant, category, amt, first, last, gender, street, city, state, zip, lat, long, city_pop, job, dob, trans_num, unix_time, merch_lat, merch_long, is_fraud) VALUES (NOW(), 8888888888888888, 'REALTIME_TEST', 'gas_transport', 8888.88, 'Test', 'User', 'F', '999 Test St', 'TestCity', 'NY', 10001, 40.71, -74.00, 500000, 'Tester', '1990-01-01', 'TEST_' || EXTRACT(epoch FROM NOW())::bigint, EXTRACT(epoch FROM NOW())::int, 40.72, -74.01, 1) RETURNING trans_num, amt, is_fraud;"
```

**Expected:**

1. Debezium capture change ‚Üí Kafka
2. Spark reads CDC event
3. API predicts fraud
4. Save to `fraud_predictions` table
5. Slack alert sent (n·∫øu c√≥ webhook)

**Check logs:**

```bash
docker logs spark-realtime-prediction --tail 100 -f
```

Expected output:

```
INFO - üíæ Saved prediction to DB: <prediction_id>
INFO - ‚úÖ Slack alert sent: <trans_num> (HIGH)
INFO - üö® ALERT sent for <trans_num> (HIGH risk)
```

---

## Dashboard Access

### T·∫•t c·∫£ c√°c URLs v√† credentials

| Service        | URL                        | Credentials             | M√¥ t·∫£                             |
| -------------- | -------------------------- | ----------------------- | --------------------------------- |
| **Chatbot**    | http://localhost:8501      | -                       | Streamlit AI Chatbot (ti·∫øng Vi·ªát) |
| **Airflow**    | http://localhost:8081      | admin / admin           | Workflow orchestration            |
| **MLflow**     | http://localhost:5001      | -                       | ML experiment tracking            |
| **FastAPI**    | http://localhost:8000/docs | -                       | Swagger API documentation         |
| **MinIO**      | http://localhost:9001      | minioadmin / minioadmin | Object storage console            |
| **Trino**      | http://localhost:8085      | -                       | SQL query engine                  |
| **Kafka UI**   | -                          | -                       | Not included (optional: AKHQ)     |
| **Metabase**   | http://localhost:3000      | -                       | BI Dashboard (if configured)      |
| **PostgreSQL** | localhost:5432             | postgres / postgres123  | Direct DB access (psql, DBeaver)  |

### Chatbot Features

**Truy c·∫≠p**: http://localhost:8501

**Sidebar ki·ªÉm tra:**

- ‚úÖ Gemini API Status
- ‚úÖ ML Model Info (version, accuracy, AUC)
- ‚úÖ Database Connection

**3 lo·∫°i c√¢u h·ªèi:**

1. **SQL Analytics**: "Top 5 bang c√≥ fraud rate cao nh·∫•t?"
2. **Fraud Prediction**: "D·ª± ƒëo√°n $850 l√∫c 2h s√°ng, 150km"
3. **General Knowledge**: "L·ªãch s·ª≠ d·ª± ƒëo√°n c·ªßa t√¥i?"

**C√¥ng c·ª• b·ªï sung:**

- Manual Prediction Form
- CSV Batch Upload

### Airflow DAGs

**Truy c·∫≠p**: http://localhost:8081 (admin/admin)

**2 DAGs ch√≠nh:**

1. **lakehouse_pipeline_taskflow**: ETL Bronze ‚Üí Silver ‚Üí Gold (M·ªói 5 ph√∫t)
2. **model_retraining_taskflow**: ML training (H√†ng ng√†y 2h s√°ng)

**Trigger manual:**

```bash
docker exec airflow-scheduler airflow dags trigger lakehouse_pipeline_taskflow
docker exec airflow-scheduler airflow dags trigger model_retraining_taskflow
```

### MLflow Tracking

**Truy c·∫≠p**: http://localhost:5001

**Xem:**

- Experiments: Model training runs
- Models: Registered models v·ªõi version history
- Metrics: Accuracy, AUC, Precision, Recall
- Artifacts: Model files, confusion matrix plots

**Model stages:**

- `None`: Newly trained
- `Staging`: Testing
- `Production`: Active (FastAPI s·ª≠ d·ª•ng)
- `Archived`: Old versions

---

## Verification & Testing

### 1. Ki·ªÉm tra Database

```bash
# S·ªë l∆∞·ª£ng transactions
docker exec postgres psql -U postgres -d frauddb -c "SELECT COUNT(*) FROM transactions;"

# S·ªë l∆∞·ª£ng fraud predictions
docker exec postgres psql -U postgres -d frauddb -c "SELECT COUNT(*) FROM fraud_predictions;"

# Fraud rate
docker exec postgres psql -U postgres -d frauddb -c "SELECT ROUND(100.0 * SUM(CASE WHEN is_fraud=1 THEN 1 ELSE 0 END) / COUNT(*), 2) AS fraud_rate_pct FROM transactions;"
```

### 2. Ki·ªÉm tra Delta Lake

```bash
# List Bronze tables
docker exec minio mc ls minio/lakehouse/bronze/

# List Silver tables
docker exec minio mc ls minio/lakehouse/silver/

# List Gold tables
docker exec minio mc ls minio/lakehouse/gold/
```

### 3. Test Trino Queries

```bash
# Show catalogs
docker exec trino trino --execute "SHOW CATALOGS;"

# Show tables in delta catalog
docker exec trino trino --catalog delta --schema default --execute "SHOW TABLES;"

# Query Gold layer
docker exec trino trino --catalog delta --schema default --execute "SELECT state, COUNT(*) as fraud_count FROM fact_transactions WHERE is_fraud=1 GROUP BY state ORDER BY fraud_count DESC LIMIT 5;"
```

### 4. Test API Endpoints

**Health check:**

```bash
curl http://localhost:8000/health
```

**Model info:**

```bash
curl http://localhost:8000/model/info
```

**Predict single transaction:**

```bash
curl -X POST http://localhost:8000/predict/raw \
  -H "Content-Type: application/json" \
  -d '{
    "amt": 850.0,
    "hour": 2,
    "distance_km": 150.0,
    "age": 45,
    "category": "shopping_net",
    "merchant": "fraud_TestMerchant",
    "city_pop": 500000
  }'
```

### 5. Test Chatbot

**Truy c·∫≠p**: http://localhost:8501

**Test queries:**

```
1. "Top 5 bang c√≥ t·ª∑ l·ªá gian l·∫≠n cao nh·∫•t?"
2. "D·ª± ƒëo√°n giao d·ªãch $850 l√∫c 2h s√°ng c√°ch nh√† 150km"
3. "Model hi·ªán t·∫°i c√≥ ƒë·ªô ch√≠nh x√°c bao nhi√™u?"
```

**Expected:**

- C√¢u 1: Tr·∫£ v·ªÅ b·∫£ng SQL results
- C√¢u 2: Tr·∫£ v·ªÅ prediction v·ªõi risk level + explanation
- C√¢u 3: Tr·∫£ v·ªÅ model metrics t·ª´ MLflow

---

## Troubleshooting

### 1. Services kh√¥ng kh·ªüi ƒë·ªông

**L·ªói: "port already allocated"**

```bash
# Ki·ªÉm tra port ƒëang s·ª≠ d·ª•ng
netstat -ano | findstr :8501  # Windows
lsof -i :8501                 # Linux/Mac

# Gi·∫£i quy·∫øt: D·ª´ng process ho·∫∑c ƒë·ªïi port trong docker-compose.yml
```

**L·ªói: "insufficient memory"**

```bash
# TƒÉng RAM cho Docker Desktop
# Settings ‚Üí Resources ‚Üí Memory ‚Üí Increase to 8GB+
```

**L·ªói: "no space left on device"**

```bash
# D·ªçn d·∫πp Docker
docker system prune -a --volumes
```

### 2. PostgreSQL kh√¥ng t·∫°o tables

**Ki·ªÉm tra init script:**

```bash
docker logs postgres | grep "init_postgres.sql"
```

**Expected**: "CREATE TABLE transactions", "CREATE TABLE fraud_predictions"

**N·∫øu kh√¥ng th·∫•y:**

```bash
# X√≥a volume v√† restart
docker-compose down -v
docker-compose up -d postgres
```

### 3. Debezium kh√¥ng t·∫°o CDC connector

**Ki·ªÉm tra connector:**

```bash
curl http://localhost:8083/connectors
```

**N·∫øu r·ªóng:**

```bash
# T·∫°o connector th·ªß c√¥ng
docker exec -it debezium-connect curl -X POST -H "Content-Type: application/json" --data @/config/connector-config.json http://localhost:8083/connectors
```

### 4. Spark jobs failed

**Ki·ªÉm tra logs:**

```bash
docker logs spark-streaming --tail 100
docker logs spark-silver --tail 100
docker logs spark-gold --tail 100
```

**L·ªói th∆∞·ªùng g·∫∑p:**

- "Connection refused to MinIO" ‚Üí Ki·ªÉm tra MinIO running
- "Table not found" ‚Üí Ch·∫°y Bronze job tr∆∞·ªõc
- "Out of memory" ‚Üí TƒÉng RAM cho Docker

### 5. ML Model kh√¥ng load

**L·ªói: "No model found in Production stage"**

```bash
# Trigger model training
docker exec airflow-scheduler airflow dags trigger model_retraining_taskflow

# Ki·ªÉm tra MLflow
curl http://localhost:5001/api/2.0/mlflow/registered-models/get?name=fraud_detection_model
```

**N·∫øu model t·ªìn t·∫°i nh∆∞ng kh√¥ng load:**

```bash
# Restart API
docker-compose restart fraud-detection-api
docker logs fraud-detection-api
```

### 6. Chatbot kh√¥ng k·∫øt n·ªëi Gemini

**L·ªói: "Invalid API key"**

- Ki·ªÉm tra `GEMINI_API_KEY` trong `.env`
- L·∫•y key m·ªõi t·∫°i: https://aistudio.google.com/app/apikey

**L·ªói: "Connection timeout"**

- Ki·ªÉm tra network/firewall
- Test: `curl https://generativelanguage.googleapis.com/`

**Rebuild chatbot:**

```bash
docker-compose up -d --build fraud-chatbot
```

### 7. Slack alerts kh√¥ng g·ª≠i (404 - no_service)

**Nguy√™n nh√¢n**: Webhook URL kh√¥ng h·ª£p l·ªá ho·∫∑c ƒë√£ b·ªã x√≥a

**Gi·∫£i quy·∫øt:**

1. T·∫°o webhook m·ªõi: https://api.slack.com/apps ‚Üí Incoming Webhooks
2. C·∫≠p nh·∫≠t `SLACK_WEBHOOK_URL` trong `.env`
3. Rebuild: `docker-compose up -d --build spark-realtime-prediction`

**Test webhook:**

```bash
curl -X POST $SLACK_WEBHOOK_URL \
  -H "Content-Type: application/json" \
  -d '{"text":"Test alert from Fraud Detection System"}'
```

### 8. Prediction time sai timezone

**L·ªói**: `prediction_time` trong database l√† UTC nh∆∞ng local time l√† GMT+7

**Nguy√™n nh√¢n**: PostgreSQL m·∫∑c ƒë·ªãnh d√πng UTC

**Gi·∫£i quy·∫øt Option 1** (ƒê·ªïi timezone PostgreSQL):

```bash
docker exec postgres psql -U postgres -c "ALTER DATABASE frauddb SET timezone TO 'Asia/Ho_Chi_Minh';"
docker-compose restart postgres
```

**Gi·∫£i quy·∫øt Option 2** (ƒê·ªïi trong code):

```python
# Trong spark/app/realtime_prediction_job.py
# Thay NOW() b·∫±ng:
prediction_time = CURRENT_TIMESTAMP AT TIME ZONE 'Asia/Ho_Chi_Minh'
```

### 9. Airflow DAGs kh√¥ng ch·∫°y

**L·ªói: "DAG not found"**

```bash
# List DAGs
docker exec airflow-scheduler airflow dags list

# N·∫øu kh√¥ng th·∫•y DAG
docker-compose restart airflow-scheduler airflow-webserver
```

**L·ªói: "Executor timeout"**

- TƒÉng RAM cho Docker
- Gi·∫£m s·ªë task concurrent trong `airflow.cfg`

### 10. Trino query timeout

**L·ªói: "Query exceeded maximum time"**

```bash
# TƒÉng timeout trong trino config
# File: config/trino/config.properties
# query.max-execution-time=10m
```

---

## Useful Commands

### Docker Management

```bash
# Xem logs service
docker logs <service_name> --tail 100 -f

# Restart service
docker-compose restart <service_name>

# Rebuild service
docker-compose up -d --build <service_name>

# Stop all services
docker-compose down

# Stop and remove volumes (RESET EVERYTHING)
docker-compose down -v

# View resource usage
docker stats
```

### Database Commands

```bash
# V√†o PostgreSQL shell
docker exec -it postgres psql -U postgres -d frauddb

# Truncate table
docker exec postgres psql -U postgres -d frauddb -c "TRUNCATE TABLE transactions, fraud_predictions CASCADE;"

# Export query result to CSV
docker exec postgres psql -U postgres -d frauddb -c "\COPY (SELECT * FROM fraud_predictions LIMIT 100) TO '/tmp/predictions.csv' CSV HEADER;"
```

### Airflow Commands

```bash
# Trigger DAG
docker exec airflow-scheduler airflow dags trigger <dag_id>

# List DAG runs
docker exec airflow-scheduler airflow dags list-runs -d <dag_id>

# Pause/unpause DAG
docker exec airflow-scheduler airflow dags pause <dag_id>
docker exec airflow-scheduler airflow dags unpause <dag_id>
```

### MinIO Commands

```bash
# List buckets
docker exec minio mc ls minio/

# List objects in bucket
docker exec minio mc ls minio/lakehouse/bronze/

# Remove old checkpoints (reset streaming)
docker exec minio mc rm -r --force minio/lakehouse/checkpoints/
```

---

## Next Steps

Sau khi ho√†n th√†nh setup:

1. ‚úÖ **ƒê·ªçc User Manual**: [USER_MANUAL.md](USER_MANUAL.md) - H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng Chatbot, API, Dashboards
2. ‚úÖ **T√¨m hi·ªÉu Architecture**: [ARCHITECTURE.md](ARCHITECTURE.md) - Ki·∫øn tr√∫c 6 t·∫ßng, data flow
3. ‚úÖ **Development**: [DEVELOPER_GUIDE.md](DEVELOPER_GUIDE.md) - Code structure, optimization
4. ‚úÖ **Troubleshooting**: [CHANGELOG.md](CHANGELOG.md) - Bug fixes history, FAQ

---

**G·∫∑p v·∫•n ƒë·ªÅ?** M·ªü issue t·∫°i: https://github.com/bin-bard/real-time-fraud-detection-lakehouse/issues
