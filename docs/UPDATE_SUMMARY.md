# TÃ³m Táº¯t Cáº­p Nháº­t Há»‡ Thá»‘ng - Sparkov Dataset v2.0

## ðŸ“‹ Tá»•ng Quan

Há»‡ thá»‘ng Data Lakehouse phÃ¡t hiá»‡n gian láº­n Ä‘Ã£ Ä‘Æ°á»£c **hoÃ n toÃ n cáº­p nháº­t** Ä‘á»ƒ lÃ m viá»‡c vá»›i **Sparkov Credit Card Transactions Dataset** thay vÃ¬ dataset PCA cÅ©. ÄÃ¢y lÃ  báº£n cáº­p nháº­t lá»›n vá»›i nhiá»u thay Ä‘á»•i vá» schema, feature engineering, vÃ  logic xá»­ lÃ½.

---

## âœ… CÃ¡c File ÄÃ£ Cáº­p Nháº­t

### 1. Data Producer

**File:** `services/data-producer/producer.py`

**Thay Ä‘á»•i chÃ­nh:**

- Chuyá»ƒn tá»« Kafka Producer sang PostgreSQL Direct Insert
- Äá»c `fraudTrain.csv` (Sparkov) thay vÃ¬ `creditcard.csv` (PCA)
- Schema: 22 cá»™t vá»›i thÃ´ng tin Ä‘áº§y Ä‘á»§
- Dependencies: `psycopg2-binary`, `python-dateutil`

**Chá»©c nÄƒng:**

```python
# Káº¿t ná»‘i PostgreSQL vÃ  insert transactions
conn = psycopg2.connect(host="postgres", database="frauddb", ...)
cursor.execute(INSERT_QUERY, (
    trans_date_trans_time, cc_num, merchant, category, amt,
    first, last, gender, lat, long, merch_lat, merch_long,
    is_fraud, ...
))
```

---

### 2. Spark Streaming Job (Bronze Layer)

**File:** `spark/app/streaming_job.py`

**Thay Ä‘á»•i chÃ­nh:**

- Äá»c tá»« Kafka topic: `postgres.public.transactions` (Debezium CDC)
- Schema má»›i: 22 fields cá»§a Sparkov dataset
- Parse Debezium payload format: `$.payload.after`
- Timestamp conversion vÃ  partitioning

**Schema:**

```python
StructField("trans_date_trans_time", StringType())
StructField("cc_num", StringType())
StructField("amt", DoubleType())
StructField("lat", DoubleType())
StructField("long", DoubleType())
StructField("merch_lat", DoubleType())
StructField("merch_long", DoubleType())
StructField("is_fraud", StringType())
# ... 14 fields ná»¯a
```

---

### 3. Silver Layer (Feature Engineering)

**File:** `spark/app/silver_layer_job.py`

**Thay Ä‘á»•i chÃ­nh:**

- **Haversine Distance Calculation:** TÃ­nh khoáº£ng cÃ¡ch giá»¯a khÃ¡ch hÃ ng vÃ  cá»­a hÃ ng
- **Age Calculation:** TÃ­nh tuá»•i tá»« ngÃ y sinh
- **Time Features:** Hour, day_of_week, is_weekend, is_late_night, hour_sin, hour_cos
- **Amount Features:** log_amount, amount_bin, is_zero_amount, is_high_amount
- **Risk Indicators:** is_distant_transaction, gender_encoded

**Tá»•ng sá»‘ features:** 15 engineered features

**Core Functions:**

```python
def haversine_distance(lat1, lon1, lat2, lon2):
    """TÃ­nh khoáº£ng cÃ¡ch Haversine (km)"""
    # Implementation using PySpark SQL functions
    R = 6371.0  # Earth radius
    # Haversine formula
    ...

def feature_engineering(df):
    """Táº¡o 15 features cho fraud detection"""
    df = df.withColumn("distance_km", haversine_distance(...))
    df = df.withColumn("age", floor(datediff(...) / 365.25))
    df = df.withColumn("hour", hour(col("trans_timestamp")))
    # ... 12 features khÃ¡c
```

---

### 4. Gold Layer (Aggregations)

**File:** `spark/app/gold_layer_job.py`

**Thay Ä‘á»•i chÃ­nh:**

- Cáº­p nháº­t táº¥t cáº£ aggregations cho schema má»›i
- ThÃªm **State-level analysis** (fraud by state)
- ThÃªm **Category analysis** (fraud by merchant category)
- ThÃªm **Distance metrics** vÃ o summaries

**New Aggregations:**

```python
# Daily summary
daily_summary = df.groupBy("year", "month", "day").agg(
    count("*"),
    sum(when(col("is_fraud") == "1", 1)),
    avg("amt"),
    avg("distance_km"),
    max("distance_km")
)

# State summary
state_summary = df.groupBy("state").agg(
    fraud_count, avg_amount, avg_distance, fraud_rate
)

# Category summary
category_summary = df.groupBy("category").agg(...)
```

---

### 5. ML Training Job

**File:** `spark/app/ml_training_job.py`

**Thay Ä‘á»•i chÃ­nh:**

- **Feature Selection:** 15 Sparkov features thay vÃ¬ PCA V1-V28
- **Class Balancing:** Undersampling normal transactions (3:1 ratio)
- **Model Hyperparameters:** Tá»‘i Æ°u cho Sparkov data
  - Random Forest: 200 trees, depth 15
  - Logistic Regression: ElasticNet regularization

**Feature List:**

```python
feature_cols = [
    # Transaction
    "amt", "log_amount", "amount_bin",
    "is_zero_amount", "is_high_amount",

    # Geographic
    "distance_km", "is_distant_transaction",

    # Demographic
    "age", "gender_encoded",

    # Time
    "hour", "day_of_week", "is_weekend",
    "is_late_night", "hour_sin", "hour_cos"
]
```

---

### 6. FastAPI Fraud Detection Service

**File:** `services/fraud-detection-api/app/main.py`

**Thay Ä‘á»•i chÃ­nh:**

- **Input Schema:** Pydantic model vá»›i 15 Sparkov features
- **Endpoints:**
  - `POST /predict`: Real-time fraud scoring
  - `GET /model/info`: Model metadata
  - `GET /health`: Health check
- **Response:** Prediction + fraud probability + risk level

**API Request Example:**

```json
{
  "amt": 123.45,
  "distance_km": 85.2,
  "age": 34,
  "hour": 14,
  "day_of_week": 3,
  "is_weekend": 0,
  "is_late_night": 0,
  "hour_sin": 0.259,
  "hour_cos": 0.966,
  "log_amount": 4.816,
  "amount_bin": 2,
  "is_zero_amount": 0,
  "is_high_amount": 0,
  "gender_encoded": 1,
  "is_distant_transaction": 0,
  "trans_num": "abc123"
}
```

**API Response Example:**

```json
{
  "trans_num": "abc123",
  "is_fraud_predicted": 0,
  "fraud_probability": 0.234,
  "risk_level": "LOW",
  "model_version": "rule_based_v1"
}
```

---

### 7. PostgreSQL Initialization

**File:** `scripts/init_postgres.sql`

**Thay Ä‘á»•i chÃ­nh:**

- **Table `transactions`:** 22 columns vá»›i Sparkov schema
- **Indexes:** 7 indexes Ä‘á»ƒ tá»‘i Æ°u query
  - `trans_date_trans_time`, `cc_num`, `is_fraud`
  - `category`, `state`, `merchant`, `amt`
  - Composite index: `(is_fraud, trans_date_trans_time, amt)`
- **Table `fraud_predictions`:** LÆ°u káº¿t quáº£ ML predictions

**Schema:**

```sql
CREATE TABLE transactions (
    trans_date_trans_time TIMESTAMP NOT NULL,
    cc_num BIGINT NOT NULL,
    merchant VARCHAR(255),
    category VARCHAR(100),
    amt NUMERIC(10, 2),
    lat DOUBLE PRECISION,
    long DOUBLE PRECISION,
    merch_lat DOUBLE PRECISION,
    merch_long DOUBLE PRECISION,
    is_fraud SMALLINT,
    trans_num VARCHAR(100) PRIMARY KEY,
    -- ... 11 columns khÃ¡c
);
```

---

### 8. Documentation Updates

**Files Updated:**

1. `README.md` - Updated dataset info, architecture diagram, setup guide
2. `docs/PROJECT_SPECIFICATION.md` - Detailed specification (already provided)
3. `docs/MIGRATION_TO_SPARKOV.md` - Migration guide (newly created)

**README Changes:**

- Added architecture diagram reference
- Updated dataset section with Sparkov details
- Added schema table with 22 columns
- Added features engineering section
- Added warning box about v2.0 update

---

## ðŸŽ¯ Kiáº¿n TrÃºc Má»›i (Sparkov v2.0)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  fraudTrain.csv â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   PostgreSQL    â”‚  â—„â”€â”€ Data source (OLTP simulation)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Debezium     â”‚  â—„â”€â”€ CDC (Change Data Capture)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Kafka      â”‚  â—„â”€â”€ Message queue
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Apache Spark Streaming                 â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Bronze  â”‚â”€â”€>â”‚ Silver  â”‚â”€â”€>â”‚  Gold   â”‚         â”‚
â”‚  â”‚  (Raw)  â”‚   â”‚(15 feat)â”‚   â”‚(Aggreg) â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                     â”‚                               â”‚
â”‚                     â–¼                               â”‚
â”‚            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚            â”‚  ML Training    â”‚                     â”‚
â”‚            â”‚  (Random Forest)â”‚                     â”‚
â”‚            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚    MLflow     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   FastAPI     â”‚  â—„â”€â”€ /predict endpoint
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“Š So SÃ¡nh Dataset

| Aspect          | Old (PCA)              | New (Sparkov)                     |
| --------------- | ---------------------- | --------------------------------- |
| **File**        | creditcard.csv         | fraudTrain.csv                    |
| **Rows**        | 284,807                | 1,296,675                         |
| **Columns**     | 31                     | 22                                |
| **Features**    | V1-V28 (PCA anonymous) | Geographic, demographic, semantic |
| **Time Range**  | 2 days                 | 2 years (2019-2020)               |
| **Fraud Rate**  | 0.172%                 | ~0.6%                             |
| **Data Type**   | Anonymized PCA         | Real-world simulation             |
| **Geographic**  | âŒ No                  | âœ… Yes (lat/long)                 |
| **Demographic** | âŒ No                  | âœ… Yes (age, gender, job)         |
| **Merchant**    | âŒ No                  | âœ… Yes (name, category)           |

---

## ðŸ”§ Dependencies Changes

### Producer

```diff
- kafka-python
+ psycopg2-binary
+ python-dateutil
```

### API

```diff
  fastapi
  uvicorn[standard]
+ pydantic
+ numpy
  pandas
  scikit-learn
  joblib
+ mlflow
```

---

## ðŸš€ Next Steps

### Immediate (Phase 1)

1. âœ… **Completed:** All code updated for Sparkov
2. ðŸ”„ **Testing:** Validate end-to-end pipeline
3. ðŸ”„ **Debezium:** Configure CDC connector
4. ðŸ”„ **MLflow:** Integrate model registry

### Short-term (Phase 2)

5. â³ **Model Serving:** Load ML model in FastAPI
6. â³ **Real-time Scoring:** Silver layer calls API
7. â³ **Monitoring:** Add metrics collection

### Long-term (Phase 3)

8. â³ **Dashboard:** Metabase + Trino integration
9. â³ **Chatbot:** LangChain fraud investigation
10. â³ **Airflow:** Automated retraining DAGs

---

## ðŸ“ Testing Checklist

- [ ] Producer inserts to PostgreSQL successfully
- [ ] Debezium captures CDC events to Kafka
- [ ] Spark Bronze layer writes raw transactions
- [ ] Spark Silver layer creates 15 features
- [ ] Haversine distance calculation correct
- [ ] Age calculation from DOB correct
- [ ] Time features extracted properly
- [ ] Gold layer aggregations complete
- [ ] ML training runs without errors
- [ ] Model achieves >80% fraud detection
- [ ] FastAPI /predict endpoint works
- [ ] API returns valid predictions

---

## ðŸŽ“ Key Learnings

1. **Geographic Features are Powerful:** Distance between customer and merchant is a strong fraud indicator
2. **Time Patterns Matter:** Late night and weekend transactions have different fraud characteristics
3. **Class Imbalancing:** Undersampling normal transactions improves fraud detection rate
4. **Feature Engineering > Raw Features:** 15 engineered features outperform 22 raw columns
5. **Real Data > Anonymous Data:** Semantic features enable better interpretability

---

## ðŸ“ž Support

**Questions?** Check these resources:

- `docs/PROJECT_SPECIFICATION.md` - Full specification
- `docs/MIGRATION_TO_SPARKOV.md` - Detailed migration guide
- `README.md` - Setup instructions
- Code comments in updated files

**Updated by:** Data Engineering Team  
**Version:** 2.0.0  
**Date:** November 27, 2024  
**Status:** âœ… Ready for Testing
