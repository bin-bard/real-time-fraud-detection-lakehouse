# CHANGELOG & TROUBLESHOOTING

Ghi l·∫°i l·ªãch s·ª≠ ph√°t tri·ªÉn, l·ªói ƒë√£ s·ª≠a, c·∫≠p nh·∫≠t v√† c√¢u h·ªèi th∆∞·ªùng g·∫∑p.

---

## üìÖ Version History

### v6.0 - Final Implementation (December 4, 2025)

**‚úÖ Completed Features:**

- Real-time CDC pipeline (PostgreSQL ‚Üí Kafka ‚Üí Delta Lake)
- Hybrid processing (Streaming Bronze + Batch Silver/Gold)
- Automated ML training (RandomForest + LogisticRegression)
- Airflow orchestration (2 DAGs)
- MLflow experiment tracking
- Trino query engine v·ªõi Delta catalog
- Bulk load feature cho initial data

**üîß Major Fixes:**

- Debezium NUMERIC encoding (Base64 ‚Üí double)
- Hive Metastore restart issue (schema conflict)
- Trino port confusion (8080 ‚Üí 8081)
- ML training sample size explanation
- Data producer checkpoint recovery

---

## üêõ Issues Fixed & Resolutions

### Issue #1: Debezium `amt` Field Returns NULL

**Ng√†y ph√°t hi·ªán:** November 28, 2025

**Tri·ªáu ch·ª©ng:**

- Kafka messages c√≥ `"amt": "AfE="` (Base64 encoded)
- Bronze layer: `amt = NULL`
- Silver/Gold layer: Kh√¥ng c√≥ d·ªØ li·ªáu s·ªë ti·ªÅn

**Root Cause:**

Debezium m·∫∑c ƒë·ªãnh encode NUMERIC/DECIMAL fields as **Base64** ƒë·ªÉ preserve precision. Spark kh√¥ng t·ª± ƒë·ªông decode Base64.

**Gi·∫£i ph√°p:**

C·∫•u h√¨nh Debezium connector v·ªõi `decimal.handling.mode=double`:

```json
{
  "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
  "decimal.handling.mode": "double",
  ...
}
```

**File changed:** `deployment/debezium/setup-connector.sh`

**Verification:**

```bash
# Check Kafka message format
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic postgres.public.transactions \
  --max-messages 1

# ‚úÖ Expected: "amt": 23.45 (plain double)
# ‚ùå Before: "amt": "AfE=" (Base64)
```

**Status:** ‚úÖ Resolved

---

### Issue #2: Hive Metastore Fails to Restart

**Ng√†y ph√°t hi·ªán:** November 30, 2025

**Tri·ªáu ch·ª©ng:**

```
ERROR: relation "BUCKETING_COLS" already exists
FATAL: database system is corrupted
```

Container crash loop m·ªói khi restart.

**Root Cause:**

- Hive Metastore init script (`schematool -initSchema`) ch·∫°y m·ªói l·∫ßn start
- PostgreSQL volume persist schema ‚Üí schema ƒë√£ t·ªìn t·∫°i
- Init script c·ªë t·∫°o l·∫°i schema ‚Üí conflict

**Gi·∫£i ph√°p 1 (ban ƒë·∫ßu):** X√≥a volume persistence

```yaml
# ‚ùå C≈® - G√¢y l·ªói:
metastore-db:
  volumes:
    - metastore_db:/var/lib/postgresql/data

# ‚úÖ M·ªöI - No persistence:
metastore-db:
  # Kh√¥ng c√≥ volumes - fresh DB m·ªói l·∫ßn start
```

**Gi·∫£i ph√°p 2 (final):** Custom entrypoint with schema check

**File:** `deployment/hive-metastore/entrypoint.sh`

```bash
#!/bin/bash
set -e

# Wait for PostgreSQL
until pg_isready -h metastore-db -U hive; do
  echo "Waiting for PostgreSQL..."
  sleep 2
done

# Check if schema exists
SCHEMA_EXISTS=$(psql -h metastore-db -U hive -d metastore -tAc \
  "SELECT 1 FROM information_schema.tables WHERE table_name='BUCKETING_COLS'" || echo "0")

if [ "$SCHEMA_EXISTS" = "1" ]; then
  echo "‚úÖ Schema already exists, skipping init"
else
  echo "üîß Initializing schema..."
  /opt/hive/bin/schematool -dbType postgres -initSchema
fi

# Start Hive Metastore
exec /opt/hive/bin/hive --service metastore
```

**File changed:**

- `deployment/hive-metastore/Dockerfile` - COPY entrypoint.sh
- `docker-compose.yml` - Re-enable volume persistence

**Status:** ‚úÖ Resolved

---

### Issue #3: Hadoop Version Mismatch

**Ng√†y ph√°t hi·ªán:** November 30, 2025

**Tri·ªáu ch·ª©ng:**

```
java.lang.ClassNotFoundException: org.apache.hadoop.fs.s3a.S3AFileSystem
java.lang.NoSuchMethodError: org.apache.hadoop.fs.statistics.IOStatisticsSource.getIOStatistics()
```

**Root Cause:**

- Hive 3.1.3 built with Hadoop 3.1.0
- Custom JARs used Hadoop 3.3.4
- API incompatibility gi·ªØa 3.1.0 v√† 3.3.4

**Gi·∫£i ph√°p:**

Downgrade JARs v·ªÅ compatible versions:

```bash
# deployment/hive-metastore/lib/
hadoop-aws-3.1.0.jar              # ‚Üê T·ª´ 3.3.4
aws-java-sdk-bundle-1.11.375.jar  # ‚Üê T·ª´ 1.12.262

# X√ìA c√°c JARs conflict:
# hadoop-common-3.3.4.jar
# hadoop-shaded-guava-*.jar
```

**File changed:** `deployment/hive-metastore/lib/` directory

**Status:** ‚úÖ Resolved

---

### Issue #4: MinIO Credential Mismatch (403 Forbidden)

**Ng√†y ph√°t hi·ªán:** November 30, 2025

**Tri·ªáu ch·ª©ng:**

```
Status Code: 403, AWS Service: Amazon S3
AWS Error Message: Forbidden
```

**Root Cause:**

- MinIO service: `minio` / `minio123`
- Hive core-site.xml: `minioadmin` / `minioadmin`

**Gi·∫£i ph√°p:**

Update `core-site.xml` credentials:

```xml
<property>
  <name>fs.s3a.access.key</name>
  <value>minio</value>  <!-- ‚Üê T·ª´ minioadmin -->
</property>

<property>
  <name>fs.s3a.secret.key</name>
  <value>minio123</value>  <!-- ‚Üê T·ª´ minioadmin -->
</property>
```

**File changed:** `deployment/hive-metastore/core-site.xml`

**Status:** ‚úÖ Resolved

---

### Issue #5: MSCK REPAIR TABLE Not Supported

**Ng√†y ph√°t hi·ªán:** December 1, 2025

**Tri·ªáu ch·ª©ng:**

```
ERROR: MSCK REPAIR TABLE is not supported for v2 tables
```

Ch·ªâ 2/7 tables registered th√†nh c√¥ng.

**Root Cause:**

- Delta Lake v2 s·ª≠ d·ª•ng `_delta_log/` transaction log
- `MSCK REPAIR TABLE` ch·ªâ cho Hive partitioned tables (Parquet/ORC)
- Delta t·ª± ƒë·ªông manage partitions

**Gi·∫£i ph√°p:**

Remove MSCK REPAIR command:

```python
# spark/app/register_tables_to_hive.py

# ‚ùå C≈® (line 63):
spark.sql(f"MSCK REPAIR TABLE {database}.{table_name}")

# ‚úÖ M·ªöI (line 63-64):
# Note: MSCK REPAIR TABLE not supported for Delta v2 tables
# Delta automatically manages partitions via _delta_log/
```

**Verification:**

```bash
docker logs hive-registration --tail 50

# ‚úÖ Expected:
# Registered bronze.transactions (25,000 records)
# Registered silver.transactions (25,000 records)
# Registered gold.dim_customer
# ...all 7 tables
```

**Status:** ‚úÖ Resolved

---

### Issue #6: Trino Port Confusion (Connection Refused)

**Ng√†y ph√°t hi·ªán:** December 1, 2025

**Tri·ªáu ch·ª©ng:**

```
java.net.ConnectException: Failed to connect to localhost:8080
```

**Root Cause:**

- Trino internal port: **8081**
- Trino external port: **8085**
- Default `trino` CLI assumes port 8080

**Gi·∫£i ph√°p:**

Lu√¥n specify port explicitly:

```bash
# ‚úÖ Inside Docker network:
docker exec trino trino --server localhost:8081

# ‚úÖ From host machine:
trino --server localhost:8085

# ‚ùå Wrong (defaults to 8080):
docker exec trino trino
```

**Metabase config:**

```yaml
Host: trino # Docker service name
Port: 8081 # Internal port
```

**Status:** ‚úÖ Resolved

---

### Issue #7: ML Training v·ªõi √≠t samples (~15-20)

**Ng√†y ph√°t hi·ªán:** December 3, 2025

**Tri·ªáu ch·ª©ng:**

MLflow UI shows:

- `train_samples: 14-17`
- `test_samples: 3-4`

User c√≥ 4000+ records trong Silver nh∆∞ng ch·ªâ 20 samples.

**Root Cause:**

**ƒê√ÇY KH√îNG PH·∫¢I L·ªñI!** Real-world fraud detection behavior:

| Metric                  | Value       | Explanation                 |
| ----------------------- | ----------- | --------------------------- |
| Total Silver records    | ~4,200      | After few minutes streaming |
| Fraud transactions      | ~10 (0.24%) | Real-world rate: 0.5-1%     |
| Non-fraud               | ~4,190      | Majority class              |
| **After class balance** | 10+10=20    | Undersample to 1:1 ratio    |
| Train/Test (80/20)      | 16 + 4      | Final dataset               |

**Gi·∫£i ph√°p:**

**Option 1: Bulk Load (Recommended)**

```bash
# Load 50K transactions ‚Üí ~250 fraud samples
docker exec data-producer python producer.py --bulk-load 50000
```

**Option 2: Wait naturally**

- Fraud rate 0.5% ‚Üí 100 frauds needs ~20K transactions
- Data producer streaming: ~5-10 transactions/second
- Wait ~2-4 hours for sufficient data

**Option 3: Increase streaming speed**

```python
# Modify services/data-producer/producer.py
time.sleep(0.5)  # Instead of time.sleep(5)
```

**Documentation updated:**

- `README.md` - Added bulk load feature
- `docs/TROUBLESHOOTING.md` - Added Issue #7 explanation

**Status:** ‚úÖ Not a bug - Working as designed

---

### Issue #8: MLflow Verification Task Failed

**Ng√†y ph√°t hi·ªán:** December 3, 2025

**Tri·ªáu ch·ª©ng:**

Airflow task `verify_mlflow` failed with "Models not found in registry"

**Root Cause:**

- Task checked MLflow **Model Registry** (registered models for production)
- But training logged to **MLflow Tracking** (experiments/runs)
- Two different concepts in MLflow!

**Gi·∫£i ph√°p:**

Update verification task to check **Tracking** instead of **Registry**:

```python
# airflow/dags/model_retraining_taskflow.py

# ‚úÖ NEW - Check MLflow Tracking (experiments/runs)
response = requests.get(
    "http://mlflow:5000/api/2.0/mlflow/experiments/search"
)
experiments = response.json().get("experiments", [])

# Find experiment
fraud_exp = next(
    (e for e in experiments if e["name"] == "fraud_detection_production"),
    None
)

# Check runs
runs_response = requests.get(
    f"http://mlflow:5000/api/2.0/mlflow/runs/search",
    json={"experiment_ids": [fraud_exp["experiment_id"]]}
)
```

**Also fixed:** Changed from `curl` to Python `requests` (mlflow container lacks curl)

**Status:** ‚úÖ Resolved

---

## ‚ùì Frequently Asked Questions

### Q1: Hive Metastore c√≥ vai tr√≤ g√¨?

**A:** Hive Metastore l√† **metadata cache** (KH√îNG ph·∫£i query engine):

- ‚úÖ TƒÉng t·ªëc `SHOW TABLES` (~100ms vs ~1-2s)
- ‚úÖ Compatibility v·ªõi BI tools c≈©
- ‚ùå KH√îNG d√πng ƒë·ªÉ query data
- ‚ùå KH√îNG b·∫Øt bu·ªôc (Delta t·ª± discover tables)

**Query pattern:**

```sql
-- ‚úÖ ƒê√öNG - Query data
SELECT * FROM delta.gold.fact_transactions;

-- ‚úÖ OK - List metadata
SHOW TABLES FROM hive.gold;

-- ‚ùå SAI - Query qua Hive
-- SELECT * FROM hive.gold.fact_transactions; -- Error!
```

### Q2: T·∫°i sao kh√¥ng d√πng `hive.*` catalog ƒë·ªÉ query?

**A:** Hive connector kh√¥ng hi·ªÉu Delta format:

- Delta Lake s·ª≠ d·ª•ng `_delta_log/` transaction log
- Hive connector ch·ªâ ƒë·ªçc Parquet/ORC thu·∫ßn
- Trino's **Delta connector** ƒë·ªçc tr·ª±c ti·∫øp t·ª´ Delta format

**Khi n√†o d√πng Hive catalog?**

- SHOW TABLES (metadata discovery)
- Query non-Delta tables (Parquet/ORC thu·∫ßn)

**Khi n√†o d√πng Delta catalog?**

- Query Delta Lake tables (PH·∫¢I d√πng!)
- T·∫•t c·∫£ SELECT/INSERT/UPDATE operations

### Q3: Producer t·∫Øt r·ªìi b·∫≠t l·∫°i c√≥ b·ªã l·ªói kh√¥ng?

**A:** KH√îNG - nh·ªù checkpoint mechanism:

```python
# services/data-producer/producer.py

# 1. ƒê·ªçc checkpoint t·ª´ PostgreSQL
last_line = get_last_checkpoint()

# 2. Resume t·ª´ d√≤ng cu·ªëi c√πng
for i, row in enumerate(reader, start=last_line + 1):
    # Process...
    save_checkpoint(i, row['trans_num'])
```

**Checkpoint table:**

```sql
CREATE TABLE producer_checkpoint (
    id INT PRIMARY KEY,
    last_line_processed INT,
    last_trans_num VARCHAR(255),
    updated_at TIMESTAMP
);
```

**An to√†n:**

- ‚úÖ Kh√¥ng duplicate records
- ‚úÖ Resume ƒë√∫ng v·ªã tr√≠
- ‚úÖ Bulk load c≈©ng tu√¢n theo checkpoint

### Q4: Bulk load c√≥ conflict v·ªõi streaming kh√¥ng?

**A:** KH√îNG conflict:

**C∆° ch·∫ø:**

1. Bulk load insert v√†o PostgreSQL
2. Debezium capture INSERT events ‚Üí Kafka
3. Bronze streaming x·ª≠ l√Ω events
4. Silver/Gold batch process sau 5 ph√∫t
5. Producer ti·∫øp t·ª•c streaming t·ª´ d√≤ng ti·∫øp theo

**Checkpoint safe:**

- PostgreSQL SERIAL primary key (auto-increment)
- Debezium LSN (Log Sequence Number)
- Spark streaming checkpoint (exactly-once)

### Q5: L√†m sao bi·∫øt h·ªá th·ªëng ƒëang ch·∫°y t·ªët?

**A:** Check c√°c indicators sau:

**1. Container health:**

```bash
docker ps --format "table {{.Names}}\t{{.Status}}"
# All containers: Up (healthy)
```

**2. Bronze streaming:**

```bash
docker logs bronze-streaming --tail 20
# ‚úÖ "Batch X written successfully"
```

**3. Airflow DAGs:**

- http://localhost:8081
- `lakehouse_pipeline_taskflow`: Success (green)
- Recent runs: < 5 minutes ago

**4. Data count:**

```sql
-- Trino query
SELECT
  'bronze' as layer, COUNT(*) FROM delta.bronze.transactions
UNION ALL
SELECT 'silver', COUNT(*) FROM delta.silver.transactions
UNION ALL
SELECT 'gold', COUNT(*) FROM delta.gold.fact_transactions;

-- ‚úÖ S·ªë l∆∞·ª£ng t∆∞∆°ng ƒë∆∞∆°ng (bronze ‚âà silver ‚âà gold)
```

**5. CPU usage:**

```bash
docker stats --no-stream

# ‚úÖ Normal:
# bronze-streaming: ~195% CPU (continuous)
# spark-master: ~50-100% CPU (when running jobs)
# airflow-*: ~10-30% CPU
```

### Q6: Khi n√†o n√™n restart services?

**A:** Ch·ªâ restart khi:

1. **High CPU (>600% total)**: Spark jobs stuck
2. **Out of memory**: Container crash loop
3. **No data flow**: Bronze/Silver/Gold kh√¥ng c·∫≠p nh·∫≠t
4. **Config changes**: Thay ƒë·ªïi docker-compose.yml

**Restart commands:**

```bash
# Restart specific service
docker compose restart bronze-streaming

# Restart all Spark services
docker compose restart spark-master spark-worker bronze-streaming

# Full restart (keep data)
docker compose down
docker compose up -d

# Nuclear option (remove ALL data)
docker compose down -v
docker compose up -d --build
```

### Q7: Data producer ch·∫°y bao l√¢u?

**A:** T√πy mode:

**Streaming mode (default):**

- Ch·∫°y v√¥ th·ªùi h·∫°n (container restart: always)
- Insert ~5-10 transactions/second
- Fraud rate: 0.5-1%
- Dataset size: 1.8M records ‚Üí ~2-4 tu·∫ßn ƒë·ªÉ h·∫øt

**Bulk load mode:**

```bash
docker exec data-producer python producer.py --bulk-load 50000
# ‚Üí Ch·∫°y ~2-3 ph√∫t r·ªìi exit
# Producer t·ª± ƒë·ªông ti·∫øp t·ª•c streaming sau khi bulk load xong
```

### Q8: L√†m sao backup data?

**A:** Backup 3 components:

**1. MinIO (Data Lake):**

```bash
# Backup bucket
docker exec minio mc mirror lakehouse /backup/lakehouse-$(date +%Y%m%d)

# Restore
docker exec minio mc cp -r /backup/lakehouse-20241204 lakehouse/
```

**2. PostgreSQL (Source + Metastores):**

```bash
# Backup
docker exec postgres pg_dump -U postgres frauddb > backup/frauddb.sql
docker exec airflow-db pg_dump -U airflow airflow > backup/airflow.sql

# Restore
docker exec postgres psql -U postgres < backup/frauddb.sql
```

**3. MLflow artifacts:**

Already in MinIO bucket (`s3a://lakehouse/models/`)

### Q9: Metabase kh√¥ng th·∫•y tables?

**A:** Check connection config:

**Common mistakes:**

```yaml
# ‚ùå Wrong catalog
Catalog: hive # Should be "delta"

# ‚ùå Wrong port
Port: 8085 # Should be 8081 (internal) if Metabase in Docker

# ‚ùå Wrong host
Host: localhost # Should be "trino" if Metabase in Docker
```

**Correct config:**

```yaml
Database Type: Trino
Host: trino # Docker service name
Port: 8081 # Internal port
Catalog: delta # ‚ö†Ô∏è MUST use delta
Database: gold # Or silver/bronze
Username: (empty)
Password: (empty)
```

**Verify Trino working:**

```bash
docker exec trino trino --server localhost:8081 --execute "SHOW TABLES FROM delta.gold"
# ‚úÖ Should list 5 tables
```

### Q10: Model training qu√° l√¢u?

**A:** Optimize resources:

**Before training:**

```powershell
# Free up ~2GB RAM + 1-2 CPU cores
.\scripts\prepare-ml-training.ps1
```

**Spark config (already optimized):**

```python
'--conf', 'spark.cores.max=2',
'--conf', 'spark.executor.cores=1',
'--conf', 'spark.executor.memory=1g',
'--conf', 'spark.driver.memory=1g',
```

**Expected time:**

- 50K records: ~2-3 minutes
- 1M records: ~10-15 minutes

**If still slow:**

- Reduce dataset: Filter recent data only
- Increase resources: Edit `.wslconfig` (Windows)
- Use sampling: Train on 10% data for testing

---

## üîß Common Operations

### Reset Everything (Clean Slate)

```bash
# ‚ö†Ô∏è WARNING: Deletes ALL data!
docker compose down -v
docker compose up -d --build

# Wait ~5 minutes for initialization
docker logs -f bronze-streaming
```

### Stop/Start Services (Keep Data)

```bash
# Stop (preserve volumes)
docker compose down

# Start
docker compose up -d

# Check status
docker compose ps
```

### View Logs

```bash
# Follow logs (Ctrl+C to exit)
docker logs -f bronze-streaming

# Last 50 lines
docker logs bronze-streaming --tail 50

# Filter by keyword
docker logs airflow-scheduler | grep "ERROR"

# Multiple services
docker logs bronze-streaming spark-master --tail 20
```

### Clean Up Disk Space

```bash
# Remove unused images
docker image prune -a

# Remove unused volumes
docker volume prune

# Remove build cache
docker builder prune
```

---

## üìö Additional Resources

### Logs Location

- Container logs: `docker logs <service-name>`
- Airflow logs: Airflow UI ‚Üí DAGs ‚Üí Task ‚Üí Logs
- Spark logs: http://localhost:8080 ‚Üí Application ‚Üí stdout/stderr

### Metrics & Monitoring

- Spark jobs: http://localhost:8080
- Airflow: http://localhost:8081
- MLflow: http://localhost:5000
- Trino: http://localhost:8085
- MinIO: http://localhost:9001

### Documentation Files

- `README.md` - Quick start guide
- `PROJECT_SPECIFICATION.md` - Technical specification
- `CHANGELOG.md` - This file (issues, FAQ)

---

**Document Version:** 1.0  
**Last Updated:** December 4, 2025  
**Maintained By:** Nh√≥m 6
