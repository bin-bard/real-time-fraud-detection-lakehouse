# CHANGELOG & TROUBLESHOOTING

Ghi l·∫°i l·ªãch s·ª≠ ph√°t tri·ªÉn, l·ªói ƒë√£ s·ª≠a, c·∫≠p nh·∫≠t v√† c√¢u h·ªèi th∆∞·ªùng g·∫∑p.

---

## üìÖ L·ªãch s·ª≠ phi√™n b·∫£n

### v6.0 - Final Implementation (4 th√°ng 12, 2025)

**‚úÖ T√≠nh nƒÉng ho√†n th√†nh:**

- Real-time CDC pipeline (PostgreSQL ‚Üí Kafka ‚Üí Delta Lake)
- Hybrid processing (Streaming Bronze + Batch Silver/Gold)
- Automated ML training (RandomForest + LogisticRegression)
- Airflow orchestration (2 DAGs)
- MLflow experiment tracking
- Trino query engine v·ªõi Delta catalog
- Bulk load feature cho initial data
- **FastAPI prediction service** v·ªõi MLflow integration

**üîß S·ª≠a l·ªói ch√≠nh:**

- Debezium NUMERIC encoding (Base64 ‚Üí double)
- Hive Metastore restart issue (schema conflict)
- Trino port confusion (8080 ‚Üí 8081)
- ML training sample size explanation
- Data producer checkpoint recovery
- FastAPI deployment v·ªõi hot model reload

---

## üêõ C√°c l·ªói ƒë√£ s·ª≠a & Gi·∫£i ph√°p

### L·ªói #1: Debezium field `amt` tr·∫£ v·ªÅ NULL

**Ng√†y ph√°t hi·ªán:** 28 th√°ng 11, 2025

**Tri·ªáu ch·ª©ng:**

- Kafka messages c√≥ `"amt": "AfE="` (Base64 encoded)
- Bronze layer: `amt = NULL`
- Silver/Gold layer: Kh√¥ng c√≥ d·ªØ li·ªáu s·ªë ti·ªÅn

**Nguy√™n nh√¢n g·ªëc:**

Debezium m·∫∑c ƒë·ªãnh encode NUMERIC/DECIMAL fields d·∫°ng **Base64** ƒë·ªÉ preserve precision. Spark kh√¥ng t·ª± ƒë·ªông decode Base64.

**Gi·∫£i ph√°p:**

C·∫•u h√¨nh Debezium connector v·ªõi `decimal.handling.mode=double`:

```json
{
  "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
  "decimal.handling.mode": "double",
  ...
}
```

**File ƒë√£ s·ª≠a:** `deployment/debezium/setup-connector.sh`

**Ki·ªÉm tra:**

```bash
# Check Kafka message format
docker exec kafka kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic postgres.public.transactions \
  --max-messages 1

# ‚úÖ Mong ƒë·ª£i: "amt": 23.45 (plain double)
# ‚ùå Tr∆∞·ªõc ƒë√¢y: "amt": "AfE=" (Base64)
```

**Tr·∫°ng th√°i:** ‚úÖ ƒê√£ gi·∫£i quy·∫øt

---

### L·ªói #2: Hive Metastore kh√¥ng kh·ªüi ƒë·ªông l·∫°i ƒë∆∞·ª£c

**Ng√†y ph√°t hi·ªán:** 30 th√°ng 11, 2025

**Tri·ªáu ch·ª©ng:**

```
ERROR: relation "BUCKETING_COLS" already exists
FATAL: database system is corrupted
```

Container crash loop m·ªói khi restart.

**Nguy√™n nh√¢n g·ªëc:**

- Hive Metastore init script (`schematool -initSchema`) ch·∫°y m·ªói l·∫ßn start
- PostgreSQL volume gi·ªØ schema ‚Üí schema ƒë√£ t·ªìn t·∫°i
- Init script c·ªë t·∫°o l·∫°i schema ‚Üí conflict

**Gi·∫£i ph√°p 1 (ban ƒë·∫ßu):** X√≥a volume persistence

```yaml
# ‚ùå C≈® - G√¢y l·ªói:
metastore-db:
  volumes:
    - metastore_db:/var/lib/postgresql/data

# ‚úÖ M·ªöI - No persistence:
metastore-db:
  # Kh√¥ng c√≥ volumes - fresh DB m·ªói l·∫ßn start
```

**Gi·∫£i ph√°p 2 (cu·ªëi c√πng):** Custom entrypoint v·ªõi schema check

**File:** `deployment/hive-metastore/entrypoint.sh`

```bash
#!/bin/bash
set -e

# Wait for PostgreSQL
until pg_isready -h metastore-db -U hive; do
  echo "Waiting for PostgreSQL..."
  sleep 2
done

# Check if schema exists
SCHEMA_EXISTS=$(psql -h metastore-db -U hive -d metastore -tAc \
  "SELECT 1 FROM information_schema.tables WHERE table_name='BUCKETING_COLS'" || echo "0")

if [ "$SCHEMA_EXISTS" = "1" ]; then
  echo "‚úÖ Schema already exists, skipping init"
else
  echo "üîß Initializing schema..."
  /opt/hive/bin/schematool -dbType postgres -initSchema
fi

# Start Hive Metastore
exec /opt/hive/bin/hive --service metastore
```

**Files ƒë√£ s·ª≠a:**

- `deployment/hive-metastore/Dockerfile` - COPY entrypoint.sh
- `docker-compose.yml` - B·∫≠t l·∫°i volume persistence

**Tr·∫°ng th√°i:** ‚úÖ ƒê√£ gi·∫£i quy·∫øt

---

### L·ªói #3: Hadoop Version Mismatch

**Ng√†y ph√°t hi·ªán:** 30 th√°ng 11, 2025

**Tri·ªáu ch·ª©ng:**

```
java.lang.ClassNotFoundException: org.apache.hadoop.fs.s3a.S3AFileSystem
java.lang.NoSuchMethodError: org.apache.hadoop.fs.statistics.IOStatisticsSource.getIOStatistics()
```

**Nguy√™n nh√¢n g·ªëc:**

- Hive 3.1.3 build v·ªõi Hadoop 3.1.0
- Custom JARs d√πng Hadoop 3.3.4
- API incompatibility gi·ªØa 3.1.0 v√† 3.3.4

**Gi·∫£i ph√°p:**

Downgrade JARs v·ªÅ compatible versions:

```bash
# deployment/hive-metastore/lib/
hadoop-aws-3.1.0.jar              # ‚Üê T·ª´ 3.3.4
aws-java-sdk-bundle-1.11.375.jar  # ‚Üê T·ª´ 1.12.262

# X√ìA c√°c JARs conflict:
# hadoop-common-3.3.4.jar
# hadoop-shaded-guava-*.jar
```

**File ƒë√£ s·ª≠a:** `deployment/hive-metastore/lib/` directory

**Tr·∫°ng th√°i:** ‚úÖ ƒê√£ gi·∫£i quy·∫øt

---

### L·ªói #4: MinIO Credential Mismatch (403 Forbidden)

**Ng√†y ph√°t hi·ªán:** 30 th√°ng 11, 2025

**Tri·ªáu ch·ª©ng:**

```
Status Code: 403, AWS Service: Amazon S3
AWS Error Message: Forbidden
```

**Nguy√™n nh√¢n g·ªëc:**

- MinIO service: `minio` / `minio123`
- Hive core-site.xml: `minioadmin` / `minioadmin`

**Gi·∫£i ph√°p:**

C·∫≠p nh·∫≠t credentials trong `core-site.xml`:

```xml
<property>
  <name>fs.s3a.access.key</name>
  <value>minio</value>  <!-- ‚Üê T·ª´ minioadmin -->
</property>

<property>
  <name>fs.s3a.secret.key</name>
  <value>minio123</value>  <!-- ‚Üê T·ª´ minioadmin -->
</property>
```

**File ƒë√£ s·ª≠a:** `deployment/hive-metastore/core-site.xml`

**Tr·∫°ng th√°i:** ‚úÖ ƒê√£ gi·∫£i quy·∫øt

---

### L·ªói #5: MSCK REPAIR TABLE Not Supported

**Ng√†y ph√°t hi·ªán:** 1 th√°ng 12, 2025

**Tri·ªáu ch·ª©ng:**

```
ERROR: MSCK REPAIR TABLE is not supported for v2 tables
```

Ch·ªâ 2/7 tables ƒëƒÉng k√Ω th√†nh c√¥ng.

**Nguy√™n nh√¢n g·ªëc:**

- Delta Lake v2 s·ª≠ d·ª•ng `_delta_log/` transaction log
- `MSCK REPAIR TABLE` ch·ªâ cho Hive partitioned tables (Parquet/ORC)
- Delta t·ª± ƒë·ªông qu·∫£n l√Ω partitions

**Gi·∫£i ph√°p:**

X√≥a l·ªánh MSCK REPAIR:

```python
# spark/app/register_tables_to_hive.py

# ‚ùå C≈® (d√≤ng 63):
spark.sql(f"MSCK REPAIR TABLE {database}.{table_name}")

# ‚úÖ M·ªöI (d√≤ng 63-64):
# Note: MSCK REPAIR TABLE not supported for Delta v2 tables
# Delta t·ª± ƒë·ªông qu·∫£n l√Ω partitions qua _delta_log/
```

**Ki·ªÉm tra:**

```bash
docker logs hive-registration --tail 50

# ‚úÖ Mong ƒë·ª£i:
# Registered bronze.transactions (25,000 records)
# Registered silver.transactions (25,000 records)
# Registered gold.dim_customer
# ...t·∫•t c·∫£ 7 tables
```

**Tr·∫°ng th√°i:** ‚úÖ ƒê√£ gi·∫£i quy·∫øt

---

### L·ªói #6: Trino Port Confusion (Connection Refused)

**Ng√†y ph√°t hi·ªán:** 1 th√°ng 12, 2025

**Tri·ªáu ch·ª©ng:**

```
java.net.ConnectException: Failed to connect to localhost:8080
```

**Nguy√™n nh√¢n g·ªëc:**

- Trino internal port: **8081**
- Trino external port: **8085**
- Default `trino` CLI gi·∫£ ƒë·ªãnh port 8080

**Gi·∫£i ph√°p:**

Lu√¥n ch·ªâ ƒë·ªãnh port r√µ r√†ng:

```bash
# ‚úÖ B√™n trong Docker network:
docker exec trino trino --server localhost:8081

# ‚úÖ T·ª´ host machine:
trino --server localhost:8085

# ‚ùå Sai (m·∫∑c ƒë·ªãnh 8080):
docker exec trino trino
```

**C·∫•u h√¨nh Metabase:**

```yaml
Host: trino # Docker service name
Port: 8081 # Internal port
```

**Tr·∫°ng th√°i:** ‚úÖ ƒê√£ gi·∫£i quy·∫øt

---

### L·ªói #7: ML Training v·ªõi √≠t samples (~15-20)

**Ng√†y ph√°t hi·ªán:** 3 th√°ng 12, 2025

**Tri·ªáu ch·ª©ng:**

MLflow UI hi·ªÉn th·ªã:

- `train_samples: 14-17`
- `test_samples: 3-4`

User c√≥ 4000+ records trong Silver nh∆∞ng ch·ªâ 20 samples.

**Nguy√™n nh√¢n g·ªëc:**

**ƒê√ÇY KH√îNG PH·∫¢I L·ªñI!** H√†nh vi real-world fraud detection:

| Metric                | Gi√° tr·ªã     | Gi·∫£i th√≠ch             |
| --------------------- | ----------- | ---------------------- |
| T·ªïng records Silver   | ~4,200      | Sau v√†i ph√∫t streaming |
| Giao d·ªãch gian l·∫≠n    | ~10 (0.24%) | T·ªâ l·ªá th·ª±c t·∫ø: 0.5-1%  |
| Giao d·ªãch b√¨nh th∆∞·ªùng | ~4,190      | Majority class         |
| **Sau class balance** | 10+10=20    | Undersample t·ªâ l·ªá 1:1  |
| Train/Test (80/20)    | 16 + 4      | Dataset cu·ªëi c√πng      |

**Gi·∫£i ph√°p:**

**T√πy ch·ªçn 1: Bulk Load (Khuy·∫øn ngh·ªã)**

```bash
# Load 50K transactions ‚Üí ~250 fraud samples
docker exec data-producer python producer.py --bulk-load 50000
```

**T√πy ch·ªçn 2: ƒê·ª£i t·ª± nhi√™n**

- T·ªâ l·ªá fraud 0.5% ‚Üí 100 frauds c·∫ßn ~20K transactions
- Data producer streaming: ~5-10 transactions/gi√¢y
- ƒê·ª£i ~2-4 gi·ªù ƒë·ªÉ c√≥ ƒë·ªß d·ªØ li·ªáu

**T√πy ch·ªçn 3: TƒÉng t·ªëc streaming**

```python
# S·ª≠a services/data-producer/producer.py
time.sleep(0.5)  # Thay v√¨ time.sleep(5)
```

**Documentation ƒë√£ c·∫≠p nh·∫≠t:**

- `README.md` - Th√™m bulk load feature
- `docs/TROUBLESHOOTING.md` - Th√™m gi·∫£i th√≠ch Issue #7

**Tr·∫°ng th√°i:** ‚úÖ Kh√¥ng ph·∫£i l·ªói - Ho·∫°t ƒë·ªông ƒë√∫ng thi·∫øt k·∫ø

---

### L·ªói #8: MLflow Verification Task Failed

**Ng√†y ph√°t hi·ªán:** 3 th√°ng 12, 2025

**Tri·ªáu ch·ª©ng:**

Airflow task `verify_mlflow` failed v·ªõi "Models not found in registry"

**Nguy√™n nh√¢n g·ªëc:**

- Task ki·ªÉm tra MLflow **Model Registry** (registered models cho production)
- Nh∆∞ng training log v√†o **MLflow Tracking** (experiments/runs)
- Hai kh√°i ni·ªám kh√°c nhau trong MLflow!

**Gi·∫£i ph√°p:**

C·∫≠p nh·∫≠t verification task ki·ªÉm tra **Tracking** thay v√¨ **Registry**:

```python
# airflow/dags/model_retraining_taskflow.py

# ‚úÖ M·ªöI - Ki·ªÉm tra MLflow Tracking (experiments/runs)
response = requests.get(
    "http://mlflow:5000/api/2.0/mlflow/experiments/search"
)
experiments = response.json().get("experiments", [])

# T√¨m experiment
fraud_exp = next(
    (e for e in experiments if e["name"] == "fraud_detection_production"),
    None
)

# Ki·ªÉm tra runs
runs_response = requests.get(
    f"http://mlflow:5000/api/2.0/mlflow/runs/search",
    json={"experiment_ids": [fraud_exp["experiment_id"]]}
)
```

**C≈©ng ƒë√£ s·ª≠a:** ƒê·ªïi t·ª´ `curl` sang Python `requests` (mlflow container thi·∫øu curl)

**Tr·∫°ng th√°i:** ‚úÖ ƒê√£ gi·∫£i quy·∫øt

---

### L·ªói #9: FastAPI kh√¥ng load ƒë∆∞·ª£c model t·ª´ MLflow

**Ng√†y ph√°t hi·ªán:** 4 th√°ng 12, 2025

**Tri·ªáu ch·ª©ng:**

```
ModuleNotFoundError: No module named 'mlflow'
WARNING: Model not loaded, using rule-based prediction
```

**Nguy√™n nh√¢n g·ªëc:**

- FastAPI service ch∆∞a ƒë∆∞·ª£c deploy trong `docker-compose.yml`
- Code ƒë√£ c√≥ nh∆∞ng container kh√¥ng ch·∫°y
- L·ªói import ch·ªâ l√† IDE warning (kh√¥ng ph·∫£i l·ªói runtime)

**Gi·∫£i ph√°p:**

1. **Th√™m service v√†o docker-compose.yml:**

```yaml
fraud-detection-api:
  build: ./services/fraud-detection-api
  container_name: fraud-detection-api
  ports:
    - "8000:8000"
  environment:
    MLFLOW_TRACKING_URI: http://mlflow:5000
    AWS_ACCESS_KEY_ID: minio
    AWS_SECRET_ACCESS_KEY: minio123
    MLFLOW_S3_ENDPOINT_URL: http://minio:9000
    MODEL_NAME: fraud_detection_randomforest
    MODEL_STAGE: None
  depends_on:
    - mlflow
    - minio
  networks:
    - data_network
  restart: unless-stopped
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    interval: 30s
    timeout: 10s
    retries: 3
```

2. **Upgrade main.py v·ªõi MLflow integration:**

```python
def load_model_from_mlflow():
    """Load model t·ª´ MLflow Registry ho·∫∑c latest run"""
    try:
        # ∆Øu ti√™n: Model Registry
        model_uri = f"models:/{MODEL_NAME}/{MODEL_STAGE}"
        loaded_model = mlflow.pyfunc.load_model(model_uri)

    except Exception as e:
        # Fallback: Latest experiment run
        runs = client.search_runs(
            experiment_ids=[experiment.experiment_id],
            filter_string="tags.model_type='RandomForest'",
            order_by=["start_time DESC"],
            max_results=1
        )
        run = runs[0]
        model_uri = f"runs:/{run.info.run_id}/model"
        loaded_model = mlflow.pyfunc.load_model(model_uri)
```

3. **C·∫≠p nh·∫≠t Dockerfile v·ªõi curl:**

```dockerfile
RUN apt-get update && apt-get install -y curl && rm -rf /var/lib/apt/lists/*
```

**Ki·ªÉm tra:**

```bash
# Build v√† start service
docker compose up -d --build fraud-detection-api

# Test health
curl http://localhost:8000/health

# Test prediction
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{"amt": 850.50, "log_amount": 6.75, ...}'
```

**Tr·∫°ng th√°i:** ‚úÖ ƒê√£ gi·∫£i quy·∫øt

---

## ‚ùì C√¢u h·ªèi th∆∞·ªùng g·∫∑p (FAQ)

### Q1: Hive Metastore c√≥ vai tr√≤ g√¨?

**A:** Hive Metastore l√† **metadata cache** (KH√îNG ph·∫£i query engine):

- ‚úÖ TƒÉng t·ªëc `SHOW TABLES` (~100ms vs ~1-2s)
- ‚úÖ Compatibility v·ªõi BI tools c≈©
- ‚ùå KH√îNG d√πng ƒë·ªÉ query data
- ‚ùå KH√îNG b·∫Øt bu·ªôc (Delta t·ª± discover tables)

**M·∫´u truy v·∫•n:**

```sql
-- ‚úÖ ƒê√öNG - Query data
SELECT * FROM delta.gold.fact_transactions;

-- ‚úÖ OK - List metadata
SHOW TABLES FROM hive.gold;

-- ‚ùå SAI - Query qua Hive
-- SELECT * FROM hive.gold.fact_transactions; -- Error!
```

---

### Q2: T·∫°i sao kh√¥ng d√πng `hive.*` catalog ƒë·ªÉ query?

**A:** Hive connector kh√¥ng hi·ªÉu Delta format:

- Delta Lake s·ª≠ d·ª•ng `_delta_log/` transaction log
- Hive connector ch·ªâ ƒë·ªçc Parquet/ORC thu·∫ßn
- Trino's **Delta connector** ƒë·ªçc tr·ª±c ti·∫øp t·ª´ Delta format

**Khi n√†o d√πng Hive catalog?**

- SHOW TABLES (metadata discovery)
- Query non-Delta tables (Parquet/ORC thu·∫ßn)

**Khi n√†o d√πng Delta catalog?**

- Query Delta Lake tables (PH·∫¢I d√πng!)
- T·∫•t c·∫£ SELECT/INSERT/UPDATE operations

---

### Q3: Producer t·∫Øt r·ªìi b·∫≠t l·∫°i c√≥ b·ªã l·ªói kh√¥ng?

**A:** KH√îNG - nh·ªù checkpoint mechanism:

```python
# services/data-producer/producer.py

# 1. ƒê·ªçc checkpoint t·ª´ PostgreSQL
last_line = get_last_checkpoint()

# 2. Resume t·ª´ d√≤ng cu·ªëi c√πng
for i, row in enumerate(reader, start=last_line + 1):
    # Process...
    save_checkpoint(i, row['trans_num'])
```

**Checkpoint table:**

```sql
CREATE TABLE producer_checkpoint (
    id INT PRIMARY KEY,
    last_line_processed INT,
    last_trans_num VARCHAR(255),
    updated_at TIMESTAMP
);
```

**An to√†n:**

- ‚úÖ Kh√¥ng duplicate records
- ‚úÖ Resume ƒë√∫ng v·ªã tr√≠
- ‚úÖ Bulk load c≈©ng tu√¢n theo checkpoint

---

### Q4: Bulk load c√≥ conflict v·ªõi streaming kh√¥ng?

**A:** KH√îNG conflict:

**C∆° ch·∫ø:**

1. Bulk load insert v√†o PostgreSQL
2. Debezium capture INSERT events ‚Üí Kafka
3. Bronze streaming x·ª≠ l√Ω events
4. Silver/Gold batch process sau 5 ph√∫t
5. Producer ti·∫øp t·ª•c streaming t·ª´ d√≤ng ti·∫øp theo

**Checkpoint safe:**

- PostgreSQL SERIAL primary key (auto-increment)
- Debezium LSN (Log Sequence Number)
- Spark streaming checkpoint (exactly-once)

---

### Q5: L√†m sao bi·∫øt h·ªá th·ªëng ƒëang ch·∫°y t·ªët?

**A:** Ki·ªÉm tra c√°c ch·ªâ s·ªë sau:

**1. Container health:**

```bash
docker ps --format "table {{.Names}}\t{{.Status}}"
# T·∫•t c·∫£ containers: Up (healthy)
```

**2. Bronze streaming:**

```bash
docker logs bronze-streaming --tail 20
# ‚úÖ "Batch X written successfully"
```

**3. Airflow DAGs:**

- http://localhost:8081
- `lakehouse_pipeline_taskflow`: Success (xanh)
- Recent runs: < 5 ph√∫t tr∆∞·ªõc

**4. S·ªë l∆∞·ª£ng d·ªØ li·ªáu:**

```sql
-- Trino query
SELECT
  'bronze' as layer, COUNT(*) FROM delta.bronze.transactions
UNION ALL
SELECT 'silver', COUNT(*) FROM delta.silver.transactions
UNION ALL
SELECT 'gold', COUNT(*) FROM delta.gold.fact_transactions;

-- ‚úÖ S·ªë l∆∞·ª£ng t∆∞∆°ng ƒë∆∞∆°ng (bronze ‚âà silver ‚âà gold)
```

**5. CPU usage:**

```bash
docker stats --no-stream

# ‚úÖ B√¨nh th∆∞·ªùng:
# bronze-streaming: ~195% CPU (continuous)
# spark-master: ~50-100% CPU (when running jobs)
# airflow-*: ~10-30% CPU
```

---

### Q6: Khi n√†o n√™n restart services?

**A:** Ch·ªâ restart khi:

1. **High CPU (>600% total)**: Spark jobs stuck
2. **Out of memory**: Container crash loop
3. **No data flow**: Bronze/Silver/Gold kh√¥ng c·∫≠p nh·∫≠t
4. **Config changes**: Thay ƒë·ªïi docker-compose.yml

**L·ªánh restart:**

```bash
# Restart service c·ª• th·ªÉ
docker compose restart bronze-streaming

# Restart t·∫•t c·∫£ Spark services
docker compose restart spark-master spark-worker bronze-streaming

# Full restart (gi·ªØ data)
docker compose down
docker compose up -d

# Nuclear option (x√≥a T·∫§T C·∫¢ data)
docker compose down -v
docker compose up -d --build
```

---

### Q7: Data producer ch·∫°y bao l√¢u?

**A:** T√πy mode:

**Streaming mode (m·∫∑c ƒë·ªãnh):**

- Ch·∫°y v√¥ th·ªùi h·∫°n (container restart: always)
- Insert ~5-10 transactions/gi√¢y
- Fraud rate: 0.5-1%
- Dataset size: 1.8M records ‚Üí ~2-4 tu·∫ßn ƒë·ªÉ h·∫øt

**Bulk load mode:**

```bash
docker exec data-producer python producer.py --bulk-load 50000
# ‚Üí Ch·∫°y ~2-3 ph√∫t r·ªìi exit
# Producer t·ª± ƒë·ªông ti·∫øp t·ª•c streaming sau khi bulk load xong
```

---

### Q8: L√†m sao backup data?

**A:** Backup 3 th√†nh ph·∫ßn:

**1. MinIO (Data Lake):**

```bash
# Backup bucket
docker exec minio mc mirror lakehouse /backup/lakehouse-$(date +%Y%m%d)

# Restore
docker exec minio mc cp -r /backup/lakehouse-20241204 lakehouse/
```

**2. PostgreSQL (Source + Metastores):**

```bash
# Backup
docker exec postgres pg_dump -U postgres frauddb > backup/frauddb.sql
docker exec airflow-db pg_dump -U airflow airflow > backup/airflow.sql

# Restore
docker exec postgres psql -U postgres < backup/frauddb.sql
```

**3. MLflow artifacts:**

ƒê√£ c√≥ trong MinIO bucket (`s3a://lakehouse/models/`)

---

### Q9: Metabase kh√¥ng th·∫•y tables?

**A:** Ki·ªÉm tra c·∫•u h√¨nh connection:

**L·ªói th∆∞·ªùng g·∫∑p:**

```yaml
# ‚ùå Sai catalog
Catalog: hive # N√™n l√† "delta"

# ‚ùå Sai port
Port: 8085 # N√™n l√† 8081 (internal) n·∫øu Metabase trong Docker

# ‚ùå Sai host
Host: localhost # N√™n l√† "trino" n·∫øu Metabase trong Docker
```

**C·∫•u h√¨nh ƒë√∫ng:**

```yaml
Database Type: Trino
Host: trino # Docker service name
Port: 8081 # Internal port
Catalog: delta # ‚ö†Ô∏è PH·∫¢I d√πng delta
Database: gold # Ho·∫∑c silver/bronze
Username: (ƒë·ªÉ tr·ªëng)
Password: (ƒë·ªÉ tr·ªëng)
```

**Ki·ªÉm tra Trino ho·∫°t ƒë·ªông:**

```bash
docker exec trino trino --server localhost:8081 --execute "SHOW TABLES FROM delta.gold"
# ‚úÖ N√™n li·ªát k√™ 5 tables
```

---

### Q10: Model training qu√° l√¢u?

**A:** T·ªëi ∆∞u t√†i nguy√™n:

**Tr∆∞·ªõc khi training:**

```powershell
# Gi·∫£i ph√≥ng ~2GB RAM + 1-2 CPU cores
.\scripts\prepare-ml-training.ps1
```

**Spark config (ƒë√£ t·ªëi ∆∞u):**

```python
'--conf', 'spark.cores.max=2',
'--conf', 'spark.executor.cores=1',
'--conf', 'spark.executor.memory=1g',
'--conf', 'spark.driver.memory=1g',
```

**Th·ªùi gian mong ƒë·ª£i:**

- 50K records: ~2-3 ph√∫t
- 1M records: ~10-15 ph√∫t

**N·∫øu v·∫´n ch·∫≠m:**

- Gi·∫£m dataset: Ch·ªâ l·ªçc d·ªØ li·ªáu g·∫ßn ƒë√¢y
- TƒÉng resources: S·ª≠a `.wslconfig` (Windows)
- D√πng sampling: Train tr√™n 10% data ƒë·ªÉ test

---

### Q11: FastAPI tr·∫£ v·ªÅ "model not loaded"?

**A:** Ki·ªÉm tra c√°c b∆∞·ªõc sau:

**1. Service ƒëang ch·∫°y?**

```bash
docker ps | grep fraud-detection-api
# ‚úÖ N√™n th·∫•y: Up (healthy)
```

**2. Ki·ªÉm tra logs:**

```bash
docker logs fraud-detection-api --tail 50

# ‚úÖ Mong ƒë·ª£i:
# "‚úÖ Model loaded successfully from Model Registry"

# ‚ö†Ô∏è N·∫øu th·∫•y:
# "‚ùå Failed to load model from MLflow"
# ‚Üí MLflow ch∆∞a c√≥ model, ch·∫°y training tr∆∞·ªõc
```

**3. Training ƒë√£ ch·∫°y ch∆∞a?**

```bash
# Ki·ªÉm tra MLflow UI
open http://localhost:5000

# Ho·∫∑c trigger manual training
docker exec airflow-scheduler airflow dags trigger model_retraining_taskflow
```

**4. Reload model sau training:**

```bash
curl -X POST http://localhost:8000/model/reload

# ‚úÖ Response:
# {"status": "success", "model_version": "abc123"}
```

**5. Test prediction:**

```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "amt": 850.50,
    "log_amount": 6.75,
    "distance_km": 120.5,
    "age": 35,
    "hour": 23,
    ...
  }'

# ‚úÖ Response:
# {"is_fraud_predicted": 1, "fraud_probability": 0.85, "risk_level": "HIGH"}
```

---

## üîß Thao t√°c th∆∞·ªùng d√πng

### Reset Everything (Clean Slate)

```bash
# ‚ö†Ô∏è C·∫¢NH B√ÅO: X√≥a T·∫§T C·∫¢ d·ªØ li·ªáu!
docker compose down -v
docker compose up -d --build

# ƒê·ª£i ~5 ph√∫t ƒë·ªÉ kh·ªüi t·∫°o
docker logs -f bronze-streaming
```

### D·ª´ng/Kh·ªüi ƒë·ªông Services (Gi·ªØ Data)

```bash
# D·ª´ng (gi·ªØ volumes)
docker compose down

# Kh·ªüi ƒë·ªông
docker compose up -d

# Ki·ªÉm tra tr·∫°ng th√°i
docker compose ps
```

### Xem Logs

```bash
# Theo d√µi logs (Ctrl+C ƒë·ªÉ tho√°t)
docker logs -f bronze-streaming

# 50 d√≤ng cu·ªëi
docker logs bronze-streaming --tail 50

# L·ªçc theo t·ª´ kh√≥a
docker logs airflow-scheduler | grep "ERROR"

# Nhi·ªÅu services
docker logs bronze-streaming spark-master --tail 20
```

### D·ªçn d·∫πp Disk Space

```bash
# X√≥a images kh√¥ng d√πng
docker image prune -a

# X√≥a volumes kh√¥ng d√πng
docker volume prune

# X√≥a build cache
docker builder prune
```

---

## üìö T√†i nguy√™n b·ªï sung

### V·ªã tr√≠ Logs

- Container logs: `docker logs <service-name>`
- Airflow logs: Airflow UI ‚Üí DAGs ‚Üí Task ‚Üí Logs
- Spark logs: http://localhost:8080 ‚Üí Application ‚Üí stdout/stderr

### Metrics & Monitoring

- Spark jobs: http://localhost:8080
- Airflow: http://localhost:8081
- MLflow: http://localhost:5000
- Trino: http://localhost:8085
- MinIO: http://localhost:9001
- **FastAPI Docs: http://localhost:8000/docs**

### Files Documentation

- `README.md` - H∆∞·ªõng d·∫´n nhanh
- `PROJECT_SPECIFICATION.md` - ƒê·∫∑c t·∫£ k·ªπ thu·∫≠t
- `CHANGELOG.md` - File n√†y (issues, FAQ)

---

**Phi√™n b·∫£n t√†i li·ªáu:** 1.0  
**C·∫≠p nh·∫≠t l·∫ßn cu·ªëi:** 4 th√°ng 12, 2025  
**Duy tr√¨ b·ªüi:** Nh√≥m 6
